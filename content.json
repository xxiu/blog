{"pages":[{"title":"","text":"About一个程序员 blog 用来记录工作，尝试写公众号，非技术的内容，会同步到公众号。 如果有兴趣可以去微信搜索公众号 “北嗅” 关注我","link":"/about/index.html"}],"posts":[{"title":"tcp连接数","text":"Tcp 连接中，Server 监听固定端口，Client 发起主动连接。 经过三次握手后建立 Tcp 连接。 Clientclient 每次请求，如果不固定端口，系统会选用一个空闲端口。该端口是独占，不能和其他 Tcp 连接共享。本地端口最大数为 65536 ，0 有特殊含义（预留，系统指定动态生成的端口），其他的全作为 client 情况下 ，最大连接数为65535 。 除去系统服务使用的端口，能够使用的会比这个少一些，1024 一下的端口通常为保留端口。 #Server server 固定监听某个本地端口，等待 Client 连接。 实际 tcp 连接数 每个 socket 内存占用 Linux 中每个 TCP 连接最少占用多少内存？ 结论是 3.155KB socket 内存占用参数 linux 内核参数优化 Linux内核 TCP/IP、Socket参数调优 mem_max wmem_max 最大 socket 写 buffer rmem_max 最大 socket 读 buffer tcp_rmen tcp 读 buffer tcp_wmen tcp 写 buffer tcp_men 三个值，没有内存压力、进入内存压力阶段、拒绝分配 socket 每个 tcp 占用一个文件描述符 进程限制 一个进程最多只能打开 ulimit -n 个文件，默认为 1024 ，如果使用默认最多只有 1024 个并发 12# /etc/rc.local ulimit -SHN 1000000 #百万文件描述符 or 123# /etc/security/limits.conf* soft nofile 1000000* hard nofile 1000000 全局限制 /proc/sys/fs/file-nr ，分别是 已经分配的文件句柄、分配没使用的，最大文件句柄 12$ cat /proc/sys/fs/file-nr1440 0 784243 ​ 也就是这里的最大文件句柄是 784243 ​ 修改 /etc/sysctl.conf 123fs.file-max = 1000000 # 系统所有进程能够打开的数量限制 net.ipv4.ip_conntrack_max = 1000000net.ipv4.netfilter.ip_conntrack_max = 1000000","link":"/tcp连接数/"},{"title":"android,不折腾","text":"在最开始的时候是打算写 andoird 相关的内容 然而发现其实能够写的东西其实没那么多 对于 android 的一些玩机技巧，在 android 5 的时候 能够说的是什么呢，刷机 、supersu、 xposed 还有一些貌似黑科技的软件 当这些说完了后呢，只能推荐一些好用的软件是作为个人来讲，推荐软件是个很无趣的活 市面上有一大堆的软件推荐网站、公众号、APP 想要选一些小众的，最后会变成效率软件推荐集合 实际上对于刷机和root来说，热情过后就很无趣 在安全的角度来讲，所有需要手机授权root的软件都是流氓软件。 所有自称如何优化了官方软件，改进有多么大的，实际上都没有想象的那么大。 而以牺牲系统安全性为代价的特性，都是不值得的。 为什么你的安全软件需要root权限？ 因为如果没有root权限就什么也干不了。 那既然没有root权限什么都干不了 你还要安全软件干什么呢。 曾经就有安全软件利用系统漏洞来标榜自己是不需要root权限的安全软件。 当时觉得很牛逼。真的是牛逼，不需要root系统。小白用户也能很好的使用的安全软件。 只需要一键开启就 ok 了。 实际上呢，作为一个安全软件，不是应该想办法来修复系统漏洞吗。 利用系统漏洞和流氓软件有什么区别。 android 5 的时候，很多地方做的不够好 我们需要去下载一些优化软件，但是现在已经不需要了 你所需要的东西大部分系统厂商都能够提供 你所追求的所谓的效率，仔细统计一下，其实是一个低频的需求 能够花20分钟去打王者，却不能花几秒钟来多做一步操作 人有时候会高估自己的时间价值，有时候又会低估 对于自己手机系统安全性不在意，随意乱给root权限 当然国产软件随意乱索取权限 系统的权限管理已经能够满足我们绝大部分人的需求。 我们在玩机的时候，玩的是什么呢，玩的是折腾 折腾呢很多的Rom，玩了很多的app 最后发现不折腾却是最好的选择 在安卓圈里有魔改系统看不清厂商出品 原始安卓看不起魔改系统 实际上大部分的原生安卓都是魔改的 总是需要这样的或者那样的插件支持 找桌面，找辅助插件，找同步通讯录，找备份 反正就是要吧系统上所有的自带app都要替换掉。 最后还要吧google全家桶也装上 管他有没有用 管他速度怎么样，能不能连上 信仰嘛 我也用google的输入法，chrome 但是 google 只是个商业公司 并不是所有的东西都香 看看 google 关掉的产品 read 、chat、G+ 等待 还是不要折腾，适当替换部分软件 不要用root类的软件 不要乱授权通讯录、电话等权限 好好用厂商提供的版本 节约的时间 去读读书、谈谈朋友 岂不美哉 ps. 看到很多公众号一句话换一行，在这里尝试一下，这种风格很适合杠。想到哪，杠到哪 。","link":"/note/android不折腾/"},{"title":"北京小汽车摇号分析","text":"北京汽车摇号到底是怎么摇的，很多人只知道自己摇了多长时间，倍数是多少。这里我帮大家看了一下。 摇号规则 1、摇号基数序号分配方法：首先，将当期所有审核通过的编码按从小到大的顺序分配序号；然后，第二阶梯及以上的编码按从小到大的顺序，接在后面继续分配序号；再然后，第三阶梯及以上的编码按从小到大的顺序，接在后面继续分配序号；以此类推。因此，高阶梯的编码所对应的多个摇号基数序号是不连号的，当期摇号基数序号总数=第一阶梯人数+第二阶梯人数×2+第三阶梯人数×3…… 2、摇号方法：摇号程序从当期所有摇号基数序号中随机抽取中签者，高阶梯的编码对应多个摇号基数序号，于是享受了多倍的中签概率。摇号程序确保高阶梯编码的多个摇号基数序号最多只能摇中一个，当其中一个摇号基数序号中签，该编码即中签。 看到上面这一段会有点懵，大部分人会被阶梯和乘数搞混乱。 这里我来帮大家解释一下。 白话解释 首先，每个人在注册的时候会分配一个序号。 假设有 1000 个人摇号，每次摇 50 个人， 大概是像下面 如果按照每人一次的摇号规则，第一次摇号后，列表里会剩下950人。 这个时候又加入了100人。 如果前面950人的摇号倍数增加了一次。列表会变成如下（假设2号李四中号了）。 当然，表里的申请编码只是模拟的，和真实的申请编码无关。 如果所有人倍数都是一样，那么摇号表和上面是一致的。当有人有2倍的话，会把2 倍的人在摇号表追加一次。 真实的摇号表就成了如下： 这里，序号 1 和序号 1051 都是张三。 如果出现3倍，4倍，甚至是10倍，按照同样的方法往后面追加。这样就实现了一个人多倍摇号。 那么在摇号的时候，摇到 1 或者 1051 的话，张三都会中号。 也就是说序号1和1051 对应的申请编码都是 000001 。 这里会出现一个问题，如果在一次摇号中 1 和 1051 都中号了怎么办。在实际的摇号中不会出现这种情况。摇号程序会自动跳过。 这上面的排序方法就是每个月会提前放出来的小客车摇号的摇号池编码文件。 小客车的摇号数据文件可以在北京市小客车摇号网站里下载到 https://apply.bjhjyd.gov.cn/apply/user/person/ballotProgram.do?issueType=PERSON 程序的运行 摇号池编码文件发布后会公布文件 MD5 码 MD5 码是一种消息摘要算法，一种被广泛使用的密码散列函数，可以产生出一个128位 的散列，用于确保信息传输完整一致。 简单的说，MD5 码是利用文件的内容生成的一个值，如果文件中有一个字被修改了，计算出来的MD5 都是不一样的，确保了文件不会被修改。 到这里已经提供了一份不会被修改的列表。接下来就是随机数出场了。每次摇号的时候由公证人员使用随机的方式提供一个6位的随机种子。 这个随机种子是如何工作的呢。 摇号程序是使用 .net 开发的程序,使用了 .Net中Random类生成的随机数。 具体可以查看 ： https://docs.microsoft.com/en-us/dotnet/api/system.random?view=netframework-4.8 。 如果是.net 的开发者都会知道 random 生成的随机数是伪随机数，就是说在随机种子固定的情况下，生成的随机数是一致的，这也是有同样的摇号池编码文件和一个随机数后，就能够确定所有中号人员名单的原因。 如果对摇号程序有兴趣可以去下载后，使用 ILSpy 来做反编译查看其中的代码。 https://github.com/icsharpcode/ILSpy 摇号程序的逻辑基本就是以上的内容。在计算摇号的时候也完全的按照以上的内容来做的。 这里不贴代码，只说一下代码逻辑。 首先，程序确定了总的摇号数 N ,真实的个人摇号编码是一个13位的数。程序中初始化的时候会实例化一大块内存。长度为 N*13 个字节 。 然后按顺序将所有人的编码写到这块内存中。 类似于如下(忽略空格） 。 000000000001 000000000002 ..... 00000012345 随机数的生成区间是 1 到 N ,随机种子固定，如果摇到的号位 m ，那么对应的摇号编码的位置是 m*13 开始的字节后的 13 位，就是对应的中号编码。 然后将这中号编码加入到一个字典中。如果这个中号编码在字典中存在，说明已经中号，直接跳过。 程序中几个有意思的地方 包含有三个入口文件 FromMain.cs FromMainB.cs FromMainC.cs 见证了不同时期不同版本。 FromMain.cs 是没有加倍的入口文件，FromMainC 是现在正在使用的，FromMainB 是中间某个时间的。 能够容纳的最大抽签人数是 4294967296L ，这个数字大概与全亚洲人口差不多。 按照程序内存初始化，1G 内存能支持900W人。 如何保证公正 摇号程序是简单粗暴，直接有效。 利用了计算机的随机数是伪随机的方式，来保证了任何计算机在得到数据文件和随机种子后得到的结果都是一致的。 有时候可能会听到找黄牛可以帮摇到号的，有没有这个可能。从数据角度来说是可能的，从实际的角度来说是不可能的。 从前面的内容我们可以知道摇号结果是有两个因素决定的。 摇号文件 随机种子 摇号文件提供了一个摇号列表，列表记录了每个人的位置，和摇号总人数。这份文件是提前开放下载。按照顺序排序，位置是由数据库的位置决定，人工无法干预。 那剩下一个变量随机种子，那么我给你一个机会让你自己去填一个随机种子。你赶紧用摇号软件去暴力循环，找到了能够让你自己中号的那个随机种子。这个是可能的。 但是现在要去找随机种子，让100个人都中号，这个是不可能的，随机区间太大，而你并不能控制随机数。 也就是说，即使给你自定义随机种子的权利，你也只能帮个位数的人来摇到号，而无法批量中号。 倍数的欺骗 在所有人都开始摇号后，不管有没有需求，反正先去那里站个坑。当到达一定的时候，倍数会变成一个毫无意义的数字。 如果每个人都开始有倍数后，这个倍数就没有意义了。如果5个人摇号，每个人有1个号，过几个月好这5个人变成了每个人3个号，和拿一个号没有任何的区别。在真实的摇号场景下，每个月都会多出几万甚至是几十万的摇号编码，而真实的中签概率是 2000 分之一。即使是增加了倍数，我们摇到号的概率依然在不断的下降。所以在摇号的绝大部分人都只是陪衬。像笔者这种重来没有中过奖的人，基本与之无缘。 如果我们按每次中号的几率为 2000 分之一，中签率是怎么样的呢？ 下图是使用每次中签率 1/2000 计算，按每年 6 次，计算 300 次(50年)绘制的中签概率图： x轴(年) y轴(概率) 可以看到 5 年中号的概率是 0.06 ，10年 是 0.11 ，20年 0.21 。 但是实际上每个人会存在倍数，所以看看 2 倍，5 倍和 10 倍的概率图是什么样的。年头太长也没有什么意义，下面只统计 10 年 2倍 2倍10年中号概率0.25左右 5倍 5倍10年中号概率 0.5 左右 10 倍 10倍10年中号概率0.7 所以在摇号难度不上升的情况下(怎么可能), 希望还是很大的。","link":"/note/北京小汽车摇号分析/"},{"title":"Docker私有源配置","text":"基本命令123sudo docker run -d -p 5000:5000 --restart=always --name registry \\ -v /home/docker/registry:/var/lib/registry registry:2 # 加入restart=always 跟随docker启动时自启动# 本地启动后镜像服务器的地址为 localhost:5000 运行后会自动到docker hub 上拉取 123sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEregistry 2 2e2f252f3c88 4 weeks ago 33.3MB Docker 镜像命名规则完整的image名称：registry.domain.com/name/base:latest registry.domain.com image 所在服务器地址 name 命名空间 base 具体名字 latest 版本号 验证本地私有镜像 本地存在一个ubuntu:16.04 的镜像 1234root@pprt-s1:/home/docker# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEregistry 2 2e2f252f3c88 4 weeks ago 33.3MBubuntu 16.04 b9e15a5d1e1a 5 weeks ago 115MB 添加一个新的taglocalhost:5000/ubuntu:16.04 123456root@pprt-s1:/home/docker# docker tag ubuntu:16.04 localhost:5000/ubuntu:16.04root@pprt-s1:/home/docker# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEregistry 2 2e2f252f3c88 4 weeks ago 33.3MBubuntu 16.04 b9e15a5d1e1a 5 weeks ago 115MBlocalhost:5000/ubuntu 16.04 b9e15a5d1e1a 5 weeks ago 115MB 将镜像localhost:5000/ubuntu:16.04 push 到私有源 12345678root@pprt-s1:/home/docker# docker push localhost:5000/ubuntu:16.04The push refers to repository [localhost:5000/ubuntu]75b79e19929c: Pushed4775b2f378bb: Pushed883eafdbe580: Pushed19d043c86cbc: Pushed8823818c4748: Pushed16.04: digest: sha256:9b47044b1e79b965a8e1653e7f9c04b5f63e00b9161bedd5baef69bb8b4c4834 size: 1357 删除本地 localhost:5000/ubuntu:16.04 1234567root@pprt-s1:/home/docker# docker image rm localhost:5000/ubuntu:16.04Untagged: localhost:5000/ubuntu:16.04Untagged: localhost:5000/ubuntu@sha256:9b47044b1e79b965a8e1653e7f9c04b5f63e00b9161bedd5baef69bb8b4c4834root@pprt-s1:/home/docker# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEregistry 2 2e2f252f3c88 4 weeks ago 33.3MBubuntu 16.04 b9e15a5d1e1a 5 weeks ago 115MB 从私有源拉取镜像，成功得到镜像 123456789root@pprt-s1:/home/docker# docker pull localhost:5000/ubuntu:16.0416.04: Pulling from ubuntuDigest: sha256:9b47044b1e79b965a8e1653e7f9c04b5f63e00b9161bedd5baef69bb8b4c4834Status: Downloaded newer image for localhost:5000/ubuntu:16.04root@pprt-s1:/home/docker# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEregistry 2 2e2f252f3c88 4 weeks ago 33.3MBubuntu 16.04 b9e15a5d1e1a 5 weeks ago 115MBlocalhost:5000/ubuntu 16.04 b9e15a5d1e1a 5 weeks ago 115MB 身份验证1234$ mkdir auth$ docker run \\ --entrypoint htpasswd \\ registry:2 -Bbn testuser testpassword &gt; auth/htpasswd 为外网提供服务12345678910sudo docker run -d -p 5000:5000 \\ --restart=always \\ --name registry \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/home/docker/cert_file/os.pprt.net/pprt.net.cer \\ -e REGISTRY_HTTP_TLS_KEY=/home/docker/cert_file/os.pprt.net/pprt.net.key \\ -v /home/docker/registry:/var/lib/registry \\ -v `pwd`/auth:/auth \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ registry:2 nginx代理https://github.com/docker/docker-registry/blob/master/contrib/nginx/nginx.conf nginx.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# For nginx &lt; 1.3.9# FYI: Chunking requires nginx-extras package on Debian Wheezy and some Ubuntu versions# See chunking http://wiki.nginx.org/HttpChunkinModule# Replace with appropriate values where necessaryupstream docker-registry { server localhost:5000;}# uncomment if you want a 301 redirect for users attempting to connect# on port 80# NOTE: docker client will still fail. This is just for convenience# server {# listen *:80;# server_name my.docker.registry.com;# return 301 https://$server_name$request_uri;# }server { listen 443; server_name docker.os.pprt.net; ssl on; ssl_certificate /home/docker/cert_file/fullchain.cer; ssl_certificate_key /home/docker/cert_file/pprt.net.key; client_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486) chunkin on; error_page 411 = @my_411_error; location @my_411_error { chunkin_resume; } location / { auth_basic \"Restricted\"; auth_basic_user_file docker-registry.htpasswd; include docker-registry.conf; } location /_ping { auth_basic off; include docker-registry.conf; } location /v1/_ping { auth_basic off; include docker-registry.conf; }} docker-registry.conf 12345proxy_pass http://docker-registry;proxy_set_header Host $http_host; # required for docker client's sakeproxy_set_header X-Real-IP $remote_addr; # pass on real client's IPproxy_set_header Authorization \"\"; # see https://github.com/dotcloud/docker-registry/issues/170proxy_read_timeout 900; 查询12$ curl http://domain.com/v2/_catalog{\"repositories\":[\"ubuntu\"]} 停止并删除容器1docker container stop registry &amp;&amp; docker container rm -v registry","link":"/dev/Docker私有源配置/"},{"title":"Dockerfile编写实践","text":"Docker 镜像是由 Layers 组成，Dockerfile 中每一条指令都会创建一个层，层数最多 127 层。 选择更小的基础镜像 通常我们使用的镜像有 Ubuntu 、CentOs、 debian 、Alpine 。其中推荐使用 Alpine ，Alpine 的基础镜像只有 4.4M 左右， 1.1 scratch 镜像 ​ scratch 镜像是空镜像。如果要运行一个包含所有依赖的二进制文件，可以直接使用 scratch 作为基础镜像。 1.2 busybox 镜像 ​ scratch是个空镜像，如果希望镜像里可以包含一些常用的Linux工具，busybox镜像是个不错选择，镜像本身只有1.16M，非常便于构建小镜像。 减少镜像层数 将多条指令写成一条。如果有太多的 RUN,会导致镜像臃肿，使用时通过 &amp;&amp; 和 / 来实现。每个 RUN执行完后记得清理不需要的缓存文件等。降低镜像体积。 1rm -rf /var/lib/apt/lists/* 多阶段构建 Dockerfile中每条指令都会为镜像增加一个镜像层，并且你需要在移动到下一个镜像层之前清理不需要的组件。实际上，有一个Dockerfile用于开发（其中包含构建应用程序所需的所有内容）以及一个用于生产的瘦客户端，它只包含你的应用程序以及运行它所需的内容。这被称为“建造者模式”。Docker 17.05.0-ce 版本以后支持多阶段构建。使用多阶段构建，你可以在Dockerfile中使用多个FROM语句，每条FROM指令可以使用不同的基础镜像，这样您可以选择性地将服务组件从一个阶段COPY到另一个阶段，在最终镜像中只保留需要的内容。 下面是一个使用COPY --from 和 FROM ... AS ... 的Dockerfile： 123456789101112131415# CompileFROM golang:1.9.0 AS builderWORKDIR /go/src/v9.git...com/.../k8s-monitorCOPY . .WORKDIR /go/src/v9.git...com/.../k8s-monitorRUN make buildRUN mv k8s-monitor /root# Package# Use scratch imageFROM scratchWORKDIR /root/COPY --from=builder /root .EXPOSE 8080CMD [&quot;/root/k8s-monitor&quot;] 构建镜像，你会发现生成的镜像只有上面COPY 指令指定的内容，镜像大小只有2M。这样在以前使用两个Dockerfile，现在使用多阶段构建就可以搞定。 将不变的内容尽量提前 如果层没有变化，Docker build 的时候会利用缓存，在开发的时候，比如 npm 、pip 之类的包管理工具，可以将配置文件提前到代码之前，然后 copy 代码后将缓存目录软连到目录，避免每次代码都需要npm 。 在实际处理过程中，可能会出现一些依赖无法处理，导致不能够提前。build 时可以采用多阶段构建。 或者直接 -v 本地代码目录，直接使用缓存。 提前配置国内源 由于网络原因。系统源，代码包管理源速度都会慢，或者无法访问。需要将源信息提前配置到 dockerfile 中。 阿里源 https://opsx.alibaba.com/mirror 腾讯源 https://mirrors.cloud.tencent.com/ 清华大学源 https://mirrors.tuna.tsinghua.edu.cn/ 设置时区 不同的镜像默认的时区可能不同，导致运行时获取的时间不一致。 1RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime","link":"/dev/Dockerfile编写实践/"},{"title":"Docker配置TLS认证开启远程访问","text":"默认情况下，Docker 通过监听本地的 unix socket 运行，同时还可以通过 TCP 进行通信，方便对 Docker 集群 管理。Docker 官方提供了通过 TLS 加密，来保证只有信任的客户端才能远程访问 Docker 服务。 采用私有 CA 签名证书。客户端只能够连接到该 CA 签名的证书和服务器。 使用 OpenSSL 创建 CA 、server 和 client使用过程中涉及到三个方面 创建私有 CA 证书 使用 CA 证书对 Server 证书签名 使用 CA 证书对 client 证书签名 生成 CA 公钥和私钥。12345// 1. 生成 ca key 需要设置一个密码# openssl genrsa -aes256 -out ca-key.pem 4096// 2. 创建 ca 公钥# openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem 创建 Server 端证书1234567891011121314//创建 server key# openssl genrsa -out server-key.pem 4096//证书签名请求文件 /CN后的填使用正式的ip或者域名# openssl req -subj \"/CN=192.168.1.123\" -sha256 -new -key server-key.pem -out server.csr//使用 CA 证书签名文件// extfile.cnf IP:XXXX 允许访问的ip# echo subjectAltName = DNS:$HOST,IP:10.10.10.20,IP:127.0.0.1 &gt;&gt; extfile.cnf# echo extendedKeyUsage = serverAuth &gt;&gt; extfile.cnf//生成签名证书# openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf 创建 client 证书1234567// 客户端证书# openssl genrsa -out key.pem 4096// 客户端签名请求文件# openssl req -subj '/CN=client' -new -key key.pem -out client.csr# echo extendedKeyUsage = clientAuth &gt; extfile-client.cnf// CA 签名# openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf server 端配置123456789101112131415161718//1. 停止 docker 服务$ service docker stop//2.1 修改文件/etc/default/docker 添加变量 Docker_OPTS# Use DOCKER_OPTS to modify the daemon startup options.#DOCKER_OPTS=\"\"DOCKER_OPTS=\" -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --tls --tlscacert /root/.docker/openssl/ca.pem --tlscert /root/.docker/openssl/server-cert.pem --tlskey /root/.docker/openssl/server-key.pem\"// 2.2 修改文件: /lib/systemd/system/docker.service 添加变量 $DOCKER_OPTS## Add EnviromentFile + add \"$DOCKER_OPTS\" at end of ExecStart## After change exec \"systemctl daemon-reload\"EnvironmentFile=/etc/default/dockerExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS// 重启 docker 服务$ service docker start Client 端访问$HOST 为服务端地址 12$ docker --tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem \\ -H=$HOST:2375 version portainer 添加 Endpoint 官方文档https://docs.docker.com/engine/security/https/","link":"/dev/Docker配置TLS认证开启远程访问/"},{"title":"Harbor配置-DockerRegistry私有镜像仓库","text":"Harbor简介Harbor是VMware公司开源的企业级Docker Registry项目，其目标是帮助用户迅速搭建一个企业级的Docker registry`服务。 它以Docker公司开源的registry为基础，提供了管理UI，基于角色的访问控制(Role Based AccessControl)，AD/LDAP集成、以及审计日志(Auditlogging) 等企业用户需求的功能，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源 Docker Distribution。 环境准备 Ubuntu 14.04 Docker and Docker Compose Docker Compose 安装123456sudo curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose$ docker-compose --versiondocker-compose version 1.22.0, build f46880fe python Harbor下载下载Harbour版本的二进制文件 https://github.com/vmware/harbor/releases 12wget https://storage.googleapis.com/harbor-releases/release-1.6.0/harbor-offline-installer-v1.6.0.tgztar -zxvf harbor-offline-installer-v1.6.0.tgz 官方的链接正常无法下载，感谢戴亚同学帮忙。 下载解压后进入解压的文件夹 Harbor配置解压后目录下生成 harbor.conf 文件，根据需要修改配置文件。 Harbor启动1./install.sh 12345678910/home/zhangwei# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES136373bd5377 goharbor/harbor-jobservice:v1.6.0 \"/harbor/start.sh\" 2 days ago Up 2 days harbor-jobservicee593d2a724de goharbor/nginx-photon:v1.6.0 \"nginx -g 'daemon of…\" 2 days ago Up 2 days (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:1080-&gt;80/tcp nginxf29c2341d237 goharbor/harbor-ui:v1.6.0 \"/harbor/start.sh\" 2 days ago Up 2 days (healthy) harbor-uia03a1bab796f goharbor/harbor-adminserver:v1.6.0 \"/harbor/start.sh\" 2 days ago Up 2 days (healthy) harbor-adminserverc11b5b108311 goharbor/redis-photon:v1.6.0 \"docker-entrypoint.s…\" 2 days ago Up 2 days 6379/tcp redis6d95f6c0aaf3 goharbor/harbor-db:v1.6.0 \"/entrypoint.sh post…\" 2 days ago Up 2 days (healthy) 5432/tcp harbor-db2fa99bb3d06c goharbor/registry-photon:v2.6.2-v1.6.0 \"/entrypoint.sh /etc…\" 2 days ago Up 2 days (healthy) 5000/tcp registryda21879d2dd1 goharbor/harbor-log:v1.6.0 \"/bin/sh -c /usr/loc…\" 2 days ago Up 2 days (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-log 根据需要修改docker-compose.yml 文件的端口 123456789101112proxy: image: goharbor/nginx-photon:v1.6.0 container_name: nginx restart: always volumes: - ./common/config/nginx:/etc/nginx:z networks: - harbor ports: - 1080:80 - 443:443 - 4443:4443","link":"/dev/Harbor配置-DockerRegistry私有镜像仓库/"},{"title":"Jenkins-docker-ansible自动化上线","text":"从11月开始到现在，项目开始从 php 转换到 nodejs + 后端服务，相应的部署环境转换到docker。 在使用纯php环境，上线比较简单，主要是代码拉取，拷贝和软链切换。 运行环境使用docker后相应的需要引入一个打包的过程，所以前面所依靠的上线部署系统无法满足当前的需求。 目前来说，Jenkins 作为自动化的流水线工具，几乎是不二的选择。 将 Jenkins 作为流水线工具，在打包阶段依赖 docker-compose。 上线阶段依赖 ansible 。 jenkins 流程 Git 拉取代码 Jenkins 根据不同的上线配置不同的脚本 docker-compose 打包代码 指定-f 参数来决定不同的镜像 项目下: ​ docker-compose.yml 对应正式环境 ，Jenkins 绑定到 master 分支 ​ docker-compose.dev.yml 对应测试环境， Jenkins 绑定到 develop 分支 ​ 由于打包过程使用的 dockerfile 都一样。 在自行的测试环境上可以自行编辑个人的docker-compose文件来作为开发环境。在 .gitignore 里面讲个人配置文件过滤掉。 docker-compose 上传代码 同打包代码一样，使用 -f 参数来指定 docker-compose 文件来上传。 执行ansible 的 playbook 来完成上线 整个过程相对来说比较简单，总结下来：1）jenkins负责docker打包和发布，2）ansible 负责线上部署脚本。 整个部署过程全部都是脚本化 jenkins 脚本由 Jenkinsfile 负责 docker 编译发布 由 docker-compose.yml 负责 线上发布有 ansible playbook.yml 负责。 所有的脚本全部都放下项目目录的deploy里面。涉及到私密信息，保存在 Jenkins 或者 ansible 的配置中，脚本中使用变量来替换。 ansible 流程ansible 在整个流程中负责与线上的机器交互，交互过程脚本话，全部都在一个名为 playbook.yml 的脚本中 在 Jenkins 中调用只需要一行 12345ansiblePlaybook credentialsId: '2495416d-79bb-4c43-9759-76102a166b3d', playbook: 'playbook.yml', extraVars: [ workspace: \"${WORKSPACE}\" ] playbook 负责 copy 配置文件 根据配置文件，拉取docker image 更新现有配置文件 重新启动docker 重新加载 nginx 配置 docker 打包部署上线，涉及到nodejs项目需要使用 npm install 或者 php 项目需要使用 compose install的，按照正常的项目配置，每次在 docker build 的过程，由于源码变化，导致了每次执行初始化的过程都需要重新安装。 我希望在自动化部署上线中，docker build 的过程时间越短越好。所以就需要利用到 docker images 的缓存。如果当前层没有发生变化，那么 docker 在打包过程中会自动使用缓存。 所以，将 nodejs 项目中的 package.json 或者 php 项目中的 composer.json 实现负责到指定文件夹做初始化。这样就可以让这段耗时 install 过程在第一次编译完成后充分的利用 docker cache。 比如： 常规流程 1234ADD . /var/www/sourceWORKDIR /var/www/sourceRUN npm install \\ &amp;&amp; npm run build 修改后流程 12345678ADD ./package*.json /var/www/node/WORKDIR /var/www/nodeRUN npm installADD . /var/www/sourceWORKDIR /var/www/sourceRUN ln -sf ../node/node_modules ./node_modules \\ &amp;&amp; npm run build","link":"/dev/Jenkins-docker-ansible自动化上线/"},{"title":"Tomcat and solr 配置","text":"tomcat 安装下载安装tomcat8.0 http://tomcat.apache.org/download-80.cgi wget http://apache.dataguru.cn/tomcat/tomcat-8/v8.0.15/bin/apache-tomcat-8.0.15.tar.gz tar -zxvf apache-tomcat-8.0.15.tar.gz cp -f apache-tomcat-8.0.15 /usr/local/tomcat 编译安装jsvc cd /usr/local/tomcat/bin tar -zxf commons-daemon-native.tar.gz cd commons-daemon-1.0.x-native-src/unix ./configure --with-java=/home/vagrant/jdk1.8.0_25/ #--with-java 参数为java的根目录 make cp jsvc ../.. cd ../../bin ./startup.sh 这个时候tomcat应该可以访问了 solr配置vagrant@aegir:~/solr-4.10.1/example/webapps$ sudo cp solr.war /usr/local/tomcat/webapps/ vagrant@aegir:~/solr-4.10.1/example/webapps$ cd ../lib/ext/ vagrant@aegir:~/solr-4.10.1/example/lib/ext$ sudo cp *.jar /usr/local/tomcat/lib/ vagrant@aegir:~/solr-4.10.1/example/lib/ext$ cd .. vagrant@aegir:~/solr-4.10.1/example/lib$ cd ../resources/ vagrant@aegir:~/solr-4.10.1/example/resources$ sudo cp log4j.properties /usr/local/tomcat/lib/ 在 /usr/local/tomcat/conf/Catalina/localhost/创建文件solr.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;Context docBase=&quot;webapps/solr.war&quot; debug=&quot;0&quot; crossContext=&quot;true&quot;&gt; &lt;Environment name=&quot;solr/home&quot; type=&quot;java.lang.String&quot; value=&quot;/usr/local/tomcat/solr&quot; override=&quot;true&quot;/&gt; &lt;/Context&gt; 配置好后重启tomcat,访问localhost:8080/solr .","link":"/dev/Tomcat_and_solr配置/"},{"title":"Kong 随记","text":"端口默认情况下 kong 监控一下端口 :8000 监听来自客户端的传入HTTP流量，并将其转发到上游服务。 :8443监听来自客户端的HTTPS流量。此端口具有与端口类似的行为:8000，但它仅需要HTTPS流量。可以通过配置文件禁用此端口。 8000 和 8443 作为提供服务的端口,在正常的线上服务是需要修改成 80 和 443 端口。使用 docker来启动会很方便做端口映射，而不需要对配置做修改 :8001 管理端口，提供管理 Admin API :8444 Admin API 的 HTTPS 端口 在最开始的看 Kong 相关的时候，一下看到 4 个服务端口会有些焦虑，实际上只是只是两种服务提供了 HTTP 和 HTTPS 的访问，8000和8443提供服务，8001和8443 用来管理。 配置文件Kong 启动的时候，会根据配置文件和模板生成 nginx.conf 和 nginx_kong.conf .这两个文件的生成模板可以在 https：//github.com/kong/kong/tree/master/kong/templates 中找到 ，在kong start 的时候 ，Kong 会将模板中的nginx.lua和nginx_kong.lua 使用模板引擎拷贝到 /usr/local/kong 。 在这个地方我吃了些亏，在没有仔细看文档的情况下手动修改 nginx_kong.conf 中的内容，启动后不生效。 实际是重启又被还原了。 当不能够通过配置文件来修改时，可以在 kong 中使用 --nginx-conf 来自定义模板 。 Servicekong 的设计是一个 service 对应多个 routes 。 在service 和 routes 之间做了拦截，这个拦截的过程就是插件所做的工作 。 Routeskong 中的 route 和有些理解上有些类似 nginx 中的 location 。在匹配上又有些不同 。 paths 作为与路由匹配的路径，paths 支持正则表达式。path 中的路径是采用的是前缀匹配。 比如我们配置了 /api 和 /api/ 的区别是 : /api 可以匹配 /apiabc 和 /apiabc /api/ 只能匹配到 /api/abc 为了防止二义性，最好以 /来结尾 strip_path 从上游请求删除匹配前缀,默认为true。 默认打开的情况下 ,删除匹配前缀,如果前缀为 /api/ 访问/api/abc,service收到的请求: 开启状态时，为 &lt;host&gt;/abc 关闭状态时, 为 &lt;host&gt;/api/abc Certificatekong 中的证书有有几个地方，Admin API 管理的证书是我们最关心的证书。 使用 API 管理证书，使用的是 Let's Encrypt的证书，使用 acme.sh 生成，在测试环境上线后，服务的 API 请求无法到达，提示证书无法通过验证，具体表现是在 Android 上服务不可用，在 Mac 和 IOS 可用。 怀疑证书的问题，使用openresty直接配置可访问，排除 怀疑 kong 的问题，手动配置后也没有问题，排除 怀疑 API 的问题，使用其他厂商的证书无问题，排除 最后问题还是落到了怀疑某个配置上，使用证书服务检查提示缺少证书链 浏览器的处理现代的浏览器都有证书自动下载的功能，但很多浏览器在安装后是使用系统内置的证书库，如果你缺失的那张CA证书，在系统的内置证书库中不存在的话，用户第一次访问网站时会显示如下情况 问题落在了证书链上, Kong 中的证书链深度 lua_ssl_verify_depth 默认为1 ，将参数修改为 2 。 这确实是个很坑的事情，生成证书后，加上 ca 证书，证书链深度为2 ,和默认不匹配。 如果数据库使用的是 PG 的话在使用 Docker 启动可以使用 PG_SSL_VERIFY=2 的全局变量来配置证书链深度。 DashboardKong 官方的 Dashboard 属于企业版，这里推荐使用 Konga 这个开源的 Kong API 管理工具。 资源https://docs.konghq.com/0.11.x/configuration/ https://www.pocketdigi.com/book/kong/ https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2018/09/29/kong-usage-problem-and-solution.html","link":"/dev/Kong随记/"},{"title":"centos下php编译安装","text":"安装前准备123456789yum -y install gcc automake autoconf libtool makeyum -y install gcc gcc-c++ glibcyum -y install libmcrypt-devel mhash-devel libxslt-devel \\libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel \\zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel \\ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel \\krb5 krb5-devel libidn libidn-devel openssl openssl-devel 安装1234567891011tar zvxf php-5.4.7.tar.gzcd php-5.4.7./configure --prefix=/usr/local/php --enable-fpm --with-mcrypt \\--enable-mbstring --disable-pdo --with-curl --disable-debug --disable-rpath \\--enable-inline-optimization --with-bz2 --with-zlib --enable-sockets \\--enable-sysvsem --enable-sysvshm --enable-pcntl --enable-mbregex \\--with-mhash --enable-zip --with-pcre-regex --with-mysql --with-mysqli \\--with-gd --with-jpeg-dirmakemake install no package libmcrypt availablecentos官方默认不对mcrypt模块支持,需要安装epel扩展 .在http://mirrors.sohu.com/fedora-epel/6/i386/ 使用 Ctrl+F 搜索关键词“epel-release” 找到当前最新的扩展包发现最新版本为：epel-release-6-7.noarch.rpm 12wget http://mirrors.sohu.com/fedora-epel/6/i386/epel-release-6-7.noarch.rpmrpm -ivh epel-release-6-7.noarch.rpm 然后就可以安装1yum install libmcrypt-devle 安装后如果运行yum报错vim /etc/yum.repos.d/epel.repo将12#baseurlmirrorlist 改成12baseurl#mirrorlist","link":"/dev/centos-install-php/"},{"title":"ci配置smarty手记","text":"需要用ci来写一个后台配置smarty,在网络上能够找到一些相关的文章.但是都是比较旧的内容,大部分是smary2.*的配置方法.按照这个配置后会出现一些错误.其实配置看smary官方会比较简单. ###基础 在php中使用smarty的用法 12require_once(&apos;Smarty.class.php&apos;);$smarty = new Smarty(); 这里就可以使用对象$smarty的assign和display对象来解析模板.在ci里面使用时为了在controller里面来使用这两个函数. ###配置 smarty里面有4个需要配置的项1234$smarty-&gt;setTemplateDir( ...);$smarty-&gt;setCompileDir(... );$smarty-&gt;setConfigDir( ...);$smarty-&gt;setCacheDir(... ); 那么我们在ci的config里面创建一个smarty.php的文件,并加入4个变量.其中APPPATH的值为application目录.创建’templates_c’,其他三个文件夹ci里面都存在.123456&lt;?php if ( ! defined(&apos;BASEPATH&apos;)) exit(&apos;No direct script access allowed&apos;);$config[&apos;template_dir&apos;] = APPPATH . &apos;views&apos;;$config[&apos;compile_dir&apos;] = APPPATH . &apos;templates_c&apos;;$config[&apos;cache_dir&apos;] = APPPATH . &apos;cache&apos;;$config[&apos;config_dir&apos;] = APPPATH . &apos;config&apos;; ###类库首先将smarty的lib目录复制到ci的libraries目录,并改名为smarty.在libraries里面创建一个Ci_smarty.php(文件名首字母大写，类名和文件名一样)的文件.这里主要是加载配置文件等.12345678910111213141516171819&lt;?phpif(!defined(&apos;BASEPATH&apos;)) EXIT(&apos;No direct script asscess allowed&apos;);require_once( APPPATH . &apos;libraries/smarty/Smarty.class.php&apos; );class Ci_smarty extends Smarty { protected $ci; public function __construct(){ parent::__construct(); $this-&gt;ci = &amp; get_instance(); $this-&gt;ci-&gt;load-&gt;config(&apos;smarty&apos;);//加载smarty的配置文件 //获取相关的配置项 // $this-&gt;template_dir= .. ;这是2.*的方法,3.1之后修改为 getXXX setXXX $this-&gt;setTemplateDir($this-&gt;ci-&gt;config-&gt;item(&apos;template_dir&apos;)); $this-&gt;setCompileDir($this-&gt;ci-&gt;config-&gt;item(&apos;compile_dir&apos;)); $this-&gt;setCacheDir($this-&gt;ci-&gt;config-&gt;item(&apos;cache_dir&apos;)); $this-&gt;setConfigDir($this-&gt;ci-&gt;config-&gt;item(&apos;config_dir&apos;)); }} 然后在config/autoload.php里面设置自动加载Ci_smarty 1$autoload[&apos;libraries&apos;] = array(&apos;Ci_smarty&apos;); ##自定义控制器在core文件夹添加一个My_Controller.php的自定义控制器.将smarty的assign和display两个函数添加进入.12345678910111213141516&lt;?php if(!defined(&apos;BASEPATH&apos;)) EXIT(&apos;No direct script asscess allowed&apos;);class MY_Controller extends CI_Controller { public function __construct() { parent::__construct(); } public function assign($key,$val) { $this-&gt;ci_smarty-&gt;assign($key,$val); } public function display($html) { $this-&gt;ci_smarty-&gt;display($html); }} 将控制器继承自My_Controller就可以使用这两个函数了. ##实例 控制器继承需要修改为My_Controller 12345678910111213141516&lt;?php if ( ! defined(&apos;BASEPATH&apos;)) exit(&apos;No direct script access allowed&apos;);class Welcome extends My_Controller { public function index() { //$this-&gt;load-&gt;view(&apos;welcome_message&apos;); $data[&quot;title&quot;]=&quot;标题&quot;; $data[&quot;num&quot;]=&quot;123123&quot;; $this-&gt;assign(&apos;data&apos;,$data); $this-&gt;display(&quot;index.html&quot;); }} view文件夹中的index.html文件12345678910&lt;html&gt;&lt;head&gt; &lt;title&gt;{$data.title}&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;{$data.num}&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;","link":"/dev/ci-smarty-config/"},{"title":"docker build openwrt 遇到的小问题","text":"在使用 docker 编译 openwrt 遇到了两个坑，问题不复杂，却折腾了一天。 环境如下 Dockerfile 定义编译 openwrt 的编译环境 docker-compose.yml 定义变化 openwrt 的运行配置，主要是将代码目录 data 使用 -v 映射到容器中 /data 代码存放目录 开发流程就两步： docker-compose build 编译镜像 docker-compose run openwrt bash 进入编译环境 在前一天执行第二步出错后，发现需要添加新的 lib 库，于是更新 Dockerfile ，在 Dockerfile 中添加了缺失的 lib。然后执行docker-compose build,但是执行这一步却卡死在命令行里。重试了 N 次也没有任何效果。 于是使用 原生的 docker build 来编译镜像 docker build -t &lt;image name&gt; . 放弃 docker-compose ，使用 docker build 来编译，出现了Sending build context to Docker daemon 的提示。 提示是将编译的上下文发送给守护进程。昨天将 openwrt 的编译相关的源码全部都放到了当前目录的 data 目录下。openwrt 的源码加上 dl 目录的文件，整体超过了 10 个 G。 命令行的提示在发送需要编译的内容给 docker 的守护进程，这里突然意识到 docker 在将这 10 几个 G 的文件发送给 docker 守护进程。这样不死才怪。 查看docker官方文档，其中有关的说明, docker 会将 build 指定的目录下的文件都发送给 docker 的守护进程。 docker 提供了 .dockerignore 文件用来过滤，避免不必要的大文件，敏感文件发送到守护进程。 .dockerignore 文件的用法和 .gitignore 文件用法类似。 官方提示最好每个 docker build 的目录最好都提供一个 .dockerignore 文件。 创建一个 .dockerignore 并将 data 目录添加到文件里。然后重新执行 docker-compose build。 解决了这个问题。 在编译镜像过程中，还出现了另外的一个问题。由于 mac 上的空间不够，在 docker 的设置里执行 Move disk image 将 Docker.raw文件挪到扩展卡上，我记得扩展卡剩余空间有 70G 左右。但是在执行的时候总是失败。尝试了几次之后。开始怀疑 docker 在 mac 上有问题。询问同事，都表示没出现过。 用 ncdu 查看扩展卡的硬盘容量占用。 发现硬盘容量剩余 1G 左右。文件夹.Spotlight-V100 占用 48G。文件夹是 Spotlight的索引文件。 这个可能是昨天在编译 openwrt 时，mac 创建索引出现了一些问题导致的。 删除这个索引目录后，硬盘空间又回来了。从新’Move disk image’，执行成功。 在编译 openwrt 时，硬盘占用确实是个大问题，以前使用 vagrant 虚拟机作为编译开发环境，一个 box 编译多次后体积可能会超过30G ，虚拟机的硬盘扩容还麻烦，使用 docker 作为开发编译环境，随用随起，方便不少。","link":"/dev/docker-build/"},{"title":"docker-compose 中的 external-links","text":"在使用服务时使用同一台服务器的 mysql 的 docker 服务。想到 external-links 这个配置。 按照文档的配置添加了如下配置 12external_links: - mysql_mysql_1:mysql 但是配置无论如何都不成功。 https://docs.docker.com/compose/compose-file/#external_links 官方文档相关的描述， If you’re using the version 2 or above file format, the externally-created containers must be connected to at least one of the same networks as the service that is linking to them. 也就是说想要使用 external_links 是需要两个服务在同一个网络段，这里只提了一句，对第一次写配置文件的开发者非常的不友好。 创建网络 12$ docker network create -d bridge my_defaultcb1e8d02683ad3dc0bff2881dccb2d3b05cab855b1d463d2cb45685a97dd196b 给mysql 服务添加网络 123456789version: '2'services: mysql: image: mysql:5.7 networks: - my_defaultnetworks: my_default: external: true 主要是最后一段 给服务添加网络 12345678910services: project: networks: - my_default external_links: - mysql_mysql_1:mysql - redis_redis_1:redisnetworks: my_default: external: true 以上的配置中删除了和网络配置不相关的配置。","link":"/dev/docker-compose-external-links/"},{"title":"docker_sysctl","text":"Sysctl 用于配置运行时的内核参数，正常情况下修改 /etc/sysctl.conf . 然后执行 sysctl -p 就能够设置成功。 当使用 Docker 启动服务时，使用 sysctl -p 来配置 docker 的参数时会出现问题。比如 12sysctl: cannot stat /proc/sys/net/core/rmem_max: No such file or directorysysctl: cannot stat /proc/sys/net/core/wmem_max: No such file or directory 由于是配置的内核参数，如果容器修改了内核的参数，还是需要宿主机支持才行，直接修改是不支持的。 Docker的base image做的很精简，甚至都没有init进程，原本在OS启动时执行生效系统变量的过程(sysctl -p)也给省略了，导致这些系统变量依旧保留着kernel默认值。 在 Docker 中对 sysctl 做了一些支持。 但是也只是有限的支持。在 Docker 启动容器时，指定 –sysctl 参数可以设置 sysctl 的参数 。 在 docker-compose.yml 文件中也提供了 sysctls 的参数 。 123sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 但是，Docker 中 sysctl 的可调参数要比宿主机少得多。 未被 docker 限制的参数 Docker通过一个 ValidateSysctl函数来限制 sysctl参数可以传入的项。 12345678910111213141516// docker/opts/opts.gofunc ValidateSysctl(val string) (string, error) { validSysctlMap := map[string]bool{ \"kernel.msgmax\": true, \"kernel.msgmnb\": true, \"kernel.msgmni\": true, \"kernel.sem\": true, \"kernel.shmall\": true, \"kernel.shmmax\": true, \"kernel.shmmni\": true, \"kernel.shm_rmid_forced\": true, } validSysctlPrefixes := []string{ \"net.\", \"fs.mqueue.\", } 容器中可以看见该配置项 可以使用 sysctl -a 看到 配置项可以 namespace 化 配置项无法 namespace化，指的是该配置项是全局的，无法为每个 namespace生成独立的配置。 一般来说，宿主机和容器是一对多的关系，一个无法 namespace化的配置项，调整之后会影响所有其他容器以及宿主机本身，这是我们无法接受的。 以上内容参考https://segmentfault.com/a/1190000009225126 在设置 socket 的一些网络参数时 。对内核参数进行修改 123456789net.core.wmem_max=12582912net.core.rmem_max=12582912net.ipv4.tcp_rmem= 10240 87380 12582912net.ipv4.tcp_wmem= 10240 87380 12582912net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_timestamps = 1net.ipv4.tcp_sack = 1net.ipv4.tcp_no_metrics_save = 1net.core.netdev_max_backlog = 5000 直接修改 docker 的 sysctl 是不能成功的。 在 docker-compose 中可以通过使用 network_mode: “host” ，修改宿主机的参数来实现。 缺点时集群模式无法使用，并且容器和宿主机共享网络空间，无法做到隔离。 使用 –privileged 默认情况下 docker 容器是无特权的。容器不允许访问任何的设备。授予 –privileged 后，Docker 启动对宿主机的所有设备的访问。以允许容器对主机的访问几乎与在主机上容器外部运行的进程相同。","link":"/dev/docker-sysctl/"},{"title":"FastDFS配置nginx以及fastdfs-nginx-model","text":"###下载zlib http://zlib.net/ 123456wget http://zlib.net/zlib-1.2.8.tar.gztar -zxvf zlib-1.2.8.tar.gzcd zlib-1.2.8./configuremakemake install ###下载安装pcre http://pcre.org/123456wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.36.tar.gztar -zxvf pcre-8.34.tar.gzcd pcre-8.34./configuremakemake install ###nginx安装 nginx编译安装需要添加fastdfs-nginx-module的模块,路径为fastdfs-nginx-module/src的路径. 123456sudo ./configure --with-pcre=../pcre-8.36 \\--with-zlib=../zlib-1.2.8 \\--add-module=../fastdfs-nginx-module/src make make install ###mod_fastdfs.conf 配置 mod_fastdfs.conf配置基本和storage的配置差不多,主要是一下几条记录的配置 123456tracker_server=192.168.10.11:22122 #tracker服务器storage_server_port=23000 #storage服务器端口group_name=group1 #这台服务器的group名store_path0=/home/vagrant/fastdfs_data/storage/ #store_path路径,和storage.conf相同 ###nginx配置 首先在storage/data中建立软链1ln -s //home/vagrant/fastdfs_data/storage/data /home/vagrant/fastdfs_data/storage/data/M00 然后在nginx中配置如下 12345678910server { listen 8090; server_name 192.168.11.10; access_log logs/fastdfs.log; location /M00{ root /home/vagrant/fastdfs_data/storage/data; ngx_fastdfs_module; }} 如果有多个store_path值,可以继续上面的步骤. 假设store_path有多个值,可以为这几个值创建不同的软链.在nginx继续添加location配置,例如12345678910111213141516171819 ln -s //home/vagrant/fastdfs_data/storage/data /home/vagrant/fastdfs_data/storage/data/M00 ln -s //home/vagrant/fastdfs_data/storage1/data /home/vagrant/fastdfs_data/storage1/data/M01server { listen 8090; server_name 192.168.11.10; access_log logs/fastdfs.log; location /M00{ root /home/vagrant/fastdfs_data/storage/data; ngx_fastdfs_module; } location /M01{ root /home/vagrant/fastdfs_data/storage1/data; ngx_fastdfs_module; }}","link":"/dev/fastDFS-and-fast-nginx-model/"},{"title":"fastDFS扩容","text":"###添加tracker tracker非常容易扩展,直接增加tracker机器即可.集群中的tracker都是对等的,所有的tracker都接受stroage心跳信息.每个tracker是对等的.由客户端来选择使用哪个tracker. 如果新增加一台tracker server，storage server连接该tracker server，发现该tracker server返回的本组storage server列表比本机记录的要少，就会将该tracker server上没有的storage server同步给该tracker server。 ###添加group 文件上传时,tracker会分配一个group给client.group直接配置好后启动group中的storage即可.而添加group也是集群扩容的方式. 配置好group后,启动新的group,tracker接受新的stroage心跳信息,来完成添加. ###group添加storage fastDFS同group内的storage数据是同步的.storage中由专门的线程根据binlog进行文件同步. 当新添加一台storage,会由已有的一台storage将所有数据同步给新的服务器. 新加入的storage server主动连接tracker server，tracker server发现有新的storage server加入，就会将该组内所有的storage server返回给新加入的storage server，并重新将该组的storage server列表返回给该组内的其他storage server； storage server有7个状态，如下： FDFS_STORAGE_STATUS_INIT :初始化，尚未得到同步已有数据的源服务器 FDFS_STORAGE_STATUS_WAIT_SYNC :等待同步，已得到同步已有数据的源服务器 FDFS_STORAGE_STATUS_SYNCING :同步中 FDFS_STORAGE_STATUS_DELETED :已删除，该服务器从本组中摘除（注：本状态的功能尚未实现） FDFS_STORAGE_STATUS_OFFLINE :离线 FDFS_STORAGE_STATUS_ONLINE :在线，尚不能提供服务 FDFS_STORAGE_STATUS_ACTIVE :在线，可以提供服务 ###storage添加空间 在storage添加硬盘,然后添加store_path,一个group中各台storage的store_path的数量和配置必须一致.添加完成后重启服务,会自动在新添加的目录创建文件夹. ###数据迁移 如果新旧IP地址一一对应，而且是一样的，那非常简单，直接将data目录拷贝过去即可。 IP不一样的话，会比较麻烦一些。如果使用了V4的自定义server ID特性，那么比较容易，直接将tracker上的IP和ID映射文件storage_ids.conf修改好即可。storage_ids文件可以再源码目录的conf里面找到示例. 如果是用IP地址作为服务器标识，那么需要修改tracker和storage的data目录下的几个数据文件，将旧IP调整为新IP。注意storage的data目录下有一个.打头的隐藏文件也需要修改。另外，需要将后缀为mark的IP地址和端口命名的同步位置记录文件名改名。文件全部调整完成后才能启动集群服务。 tracker server上需要调整的文件列表：data/storage_groups_new.datdata/storage_servers_new.datdata/storage_sync_timestamp.dat storage server需要调整的文件列表：data/.data_init_flagdata/sync/${ip_addr}_${port}.mark：此类文件，需要将文件名中的IP地址调整过来","link":"/dev/fastDFS扩容/"},{"title":"fastDFS架构","text":"####FastDFS是什么 FastDFS是开源的轻量级分布式文件系统,解决海量存储的问题,适合中小文件存储.由三部分组成 tracker server (跟踪服务器) storage server (存储服务器) client (客户端) ####storage server storage 以组为单位组织,一个group内可以包含多台storage机器,数据互为备份. group内每台storage依赖本地文件服务器.storage可配置多个数据存储目录. storgae写文件时会根据配置好的规则选择一个目录来存储.为避免单个目录文件数太多.storage第一次启动时,会创建2级子目录.每级256个共65536个目录. 新添加文件会以hash的方式被路由到一个子目录,然后文件数据直接作为本地文件存储到改目录. ####Tracker server Trancker是负责管理所有的storage 和group,每个storage在启动后会连接tracker,将自己所属的group等信息告知tracker.并保持心跳. tracker根据storage心跳建立group-&gt;storage list的映射表. tracker需要管理的元信息很少,全部都放在内存中. tracker元信息都是由storage汇报的信息生产,所以tracker非常容易扩展,直接增加tracker机器即可.集群中的tracker都是对等的,所有的tracker都接受stroage心跳信息. ###client FastDFS提供基本的文件访问接口,以客户端库的方式提供.比如php ,java,.net等. 上传文件 client 向Tracker发起上传请求 Tracker 查询可用的storage,并返回storage的ip和端口 client根据返回的storage上传文件. storage生成file_id,并将文件写入磁盘. storage返回file_id(路径信息和文件名)给client. client存储文件信息到数据库等 ####选择tracker当集群中有多个tracker,客户端可任意选择一个. ####选择group tracker接收上传请求,会分配一个group,group支持如下规则: Round robin，所有的group间轮询 Specified group，指定某一个确定的group Load balance，剩余存储空间多多group优先 ####选择storage 选择group后,tracker会在group内选择一个storage,支持如下规则 Round robin，在group内的所有storage间轮询 First server ordered by ip，按ip排序 First server ordered by priority，按优先级排序（优先级在storage上配置） 生成的文件名当文件存储到某个子目录后,会为文件生成一个文件名,文件名有group,存储目录,两级子目录,fileid,文件后缀拼接而成 12[组名] [磁盘] [目录] [文件名]group1/ M00 /00/0d/ wKgKC1SQBbeABu7vAAAAEGqzRP4043.txt ###download file 客户端上传完成后,会到达file_id信息. client请求Tracke,tracke返回storage的ip和端口 client请求根据返回的storage请求file_id. storage返回file_content ####HTTP访问支持 FastDFS的tracker和storage都内置了http协议的支持，客户端可以通过http协议来下载文件，tracker在接收到请求 时，通过http的redirect机制将请求重定向至文件所在的storage上；新版本已经移除了内置的HTTP协议，FastDFS提供了通过apache或nginx扩展模块下载文件的支持.","link":"/dev/fastDFS架构/"},{"title":"fastDFS安装配置","text":"[FastDFS FQA])(http://bbs.chinaunix.net/thread-1920470-1-1.html) 讨论论坛 安装libeventhttp://libevent.org/12345tar -zxvf libevent-2.0.21-stable.tar.gzcd libevent-2.0.21-stable/./configure --prefix=/usrmakesudo make install 注意:–prefix参数需要时/usr,否则需要做符号链接 fastdfs5.x里面需要安装,而不需要安装libevent1https://github.com/happyfish100/libfastcommon.git ###安装FastDFS1234tar zvxf FastDFS_v4.06.tar.gzcd FastDFs./make.sh./make.sh install 注意上面两步，检查是否出错，如果出错，则说明上面的libfastcommon ~libevent~没有安装好安装成功后，FastDFS 安装在/usr/local/bin中。配置文件在/etc/fdfs中 ###conf配置 FastDFS 配置文件详解 修订版1 ###必要配置 ####配置及启动Tracker Server1234567mkdir /home/fastdfs #创建存储目录vi /etc/fdfs/tracker.confbase_path=/home/fastdfs #修改为上面创建的目录/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf #启动trackerd,参数为配置文件路径 检查tracker是否启动成功，可以查看如下文件/home/fastdfs/logs/trackerd.log ####配置及启动Storage Server 12345678910111213mkdir /home/fastdfs/fdfs_storage #创建文件存储目录cd /etc/fdfsvim /etc/fdfs/storage.confbase_path=/home/fastdfs/fdfs_storage #修改为上面创建的目录store_path0=/home/fastdfs/fdfs_storage #修改为文件存储目录tracker_server=10.201.20.237:22122 #设置tracker server的信息/usr/bin/fdfs_storaged /etc/fdfs/storage.conf #启动服务器,参数为配置文件路径 接下来会出现很多mkdir data path，这是系统在创建数据目录。 ###目录结构 ####tracker server目录及文件结构：123456${base_path} |__data | |__storage_groups.dat：存储分组信息 | |__storage_servers.dat：存储服务器列表 |__logs |__sync：tracker server日志文件 ####storage server 目录及文件结构12345678910111213${base_path} |__data | |__.data_init_flag：当前storage server初始化信息 | |__storage_stat.dat：当前storage server统计信息 | |__sync：存放数据同步相关文件 | | |__binlog.index：当前的binlog（更新操作日志）文件索引号 | | |__binlog.###：存放更新操作记录（日志） | | |__${ip_addr}_${port}.mark：存放向目标服务器同步的完成情况 | | | |__一级目录：256个存放数据文件的目录，目录名为十六进制字符，如：00, 1F | |__二级目录：256个存放数据文件的目录，目录名为十六进制字符，如：0A, CF |__logs |__storaged.log：storage server日志文件 让server进程退出运行直接kill即可让server进程正常退出，可以使用killall命令，例如：12killall fdfs_trackerdkillall fdfs_storaged 也可以使用FastDFS自带的stop.sh脚本，如：1/usr/local/bin/stop.sh /usr/local/bin/fdfs_storaged /etc/fdfs/storage.conf stop.sh只会停止命令行（包括参数）完全相同的进程。千万不要使用-9参数强杀，否则可能会导致binlog数据丢失的问题。","link":"/dev/fastDFS安装配置/"},{"title":"frp反向代理web应用","text":"frp 是什么frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。 github: https://github.com/fatedier/frp 类似的 localtunnel https://github.com/localtunnel/localtunnel ngrok https://ngrok.com/ frp 能干什么将处于内网的服务通过外网机器映射出去，公司内网有些服务需要能够在外网访问，这个应该是成本最低的解决方法，比如nas，内网路由器管理映射等等。 frp 的文档已经很详细了。由于是使用go开发的，所以，整个安装只有四个文件 frps 服务端 frs.ini 服务端配置 frpc 客户端 frpc.ini 客户端配置 相比其他的程序来说，frp 使用起来简单方便。 123456789#服务端启动./frps -c frps.ini#服务端启动 后台执行# nohup ./frps -c frps.ini &amp; ​# 客户端启动./frpc -c frpc.ini#客户端端启动 后台执行# nohup ./frps -c frps.ini &amp; frps.ini 配置1234[common]bind_port = 7000 # 监听端口vhost_http_port = 6001 # http 端口， client 端的http会被转到这个端口subdomain_host = xxxx.com # 绑定的主域名 ，在dns里面设置好绑定，与 frpc.ini 中的 subdomain 一起使用 frpc.ini 配置123456789101112[common]server_addr = 192.168.1.110 # server 端域名server_port = 7000 # server 端监听端口​​[web01] # http 服务穿透 必须以 web 开头type = http # 服务类型 http or httpslocal_port = 8080 # web服务本机端口subdomain = web # 服务的域名，与server端的subdomain_host配和使用,访问域名为 www.xxxx.com​#http_user =admin # 访问需要登录，测试发现认证失败，问题原因不详。#http_pwd =admin 作为反向代理来说，最常用的就是代理 web 服务或者是 ssh。frp 配置起来非常简单。 作者在发布的 build 的 zip 文件中提供了全量配置选项。","link":"/dev/frp反向代理web应用/"},{"title":"firefox 安装 copper 插件","text":"Firefox 升级后开始不支持三方协议的插件，调试 coap 协议使用的 Copper 插件在Firefox上不起作用。 需要做调试的时候，发现在关闭了 Firefox 更新的情况下 Copper 插件依然不起作用，于是想着重新安装，在插件商店里面居然已经找不到 Copper 这个插件了。官方已经将这个插件从 Firefox 的商店下架了。 解决方法 Copper Github地址 https://github.com/mkovatsc/Copper Firefox RES 版 https://ftp.mozilla.org/pub/firefox/releases/52.0.2esr/ 使用RES版是因为正常的版本从源码加载插件插件依然无法使用 安装方式安装方式参考github的安装说明 从github拉取 Copper 源码 clone git://github.com/mkovatsc/Copper.git 创建一个名为 copper@vs.inf.ethz.ch 的文件 到firefox的 extensions 目录，路径如下 Windows: C:\\Users\\\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\xxxxxxxx.default\\extensions\\ Linux: ~/.mozilla/firefox/xxxxxxxx.default/extensions/ MacOS: ~/Library/Application Support/Firefox/Profiles/xxxxxxxx.default/extensions/ 将第一步中clone的源码路径添加到第二步创建的文件中 在 firefox 中打开 about:config 设置 xpinstall.signatures.required 为 false5.. 重启 firefox 。","link":"/dev/firefox-install-copper/"},{"title":"github 下载指定文件夹或文件","text":"工具 svn 复制指定目录，比如：https://github.com/googlevr/gvr-unity-sdk/tree/master/GoogleVR 将 tree/master 替换为 trunk https://github.com/googlevr/gvr-unity-sdk/trunk/GoogleVR 命令 1svn checkout https://github.com/googlevr/gvr-unity-sdk/trunk/GoogleVR","link":"/dev/github-checkout-file-or-dir/"},{"title":"gitolite-install","text":"安装脚本1234567891011121314151617181920212223242526272829303132333435#!/bin/bashuseradd -m git#安装git-coreapt-get install git-core#创建gitolite安装路径shell_dir=/gitolite=/home/git/gitolitemkdir -p $gitolite&lt;!--more--&gt;#cp key# 安装时替换为管理员秘钥echo &apos;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDEtBEpIoVQ6ZxFgkQR/Df23lsX+1m5IzKI9VbWDu2KSJhWsuFqc1KGvwYI7DOtcRBUKQFTkf9ExxqbfqUWlyn2cnkpMsfH24Mb28HmHmEBqjtTfurOSH9SNQSgDhxG2K3A0EqiSKFMgFuuGkcyMfIjtTWFhbg5TAWZ69WRnr71QR2pCTuc46bRwRyeZR1s2Ohtss6pF28HphCI58HVg7+VVHTpqz5XLmX6TwOrQtjrTAp0lu12d7Ul5K0IKadF62MOSJJMh0YcC3VZIq1ixqAooUfry+/yS9NBGjJZpKJFeew25tRq9iZJwTp zhangwei@zhangwei&apos; &gt;$gitolite/admin.pub#cp admin.pub $gitolitecd $gitolitegit clone git://github.com/sitaramc/gitolite srcecho &apos;开始安装gitolite ......&apos;${gitolite}/src/install -to $gitoliteecho &quot;安装到了$gitolite&quot;rm -rf ${gitolite}/srcchown -R git:git $gitolite#将管理员key放到同级目录命名为admin.pub 并启动su - git -c &quot;${gitolite}/gitolite setup -pk ${gitolite}/admin.pub &quot; 安装钩子gitolite 钩子目录为 /home/git/.gitolite/hooks/common 钩子脚本拷贝到这个目录，比如post-receive。执行以下脚本刷新钩子1/home/git/gitolite/gitolite setup --hooks-only","link":"/dev/gitolite-install/"},{"title":"让homebrew 走代理","text":"命令1$ http_proxy=http://IP:PORT https_proxy=http://IP:PORT brew install PACKAGE 1$ ALL_PROXY=socks5://IP:PORT brew nstall PACKAGE 例子1$ http_proxy=http://127.0.0.1:1080 https_proxy=http://127.0.0.1:1080 brew install PACKAGE 1$ ALL_PROXY=socks5://127.0.0.1:1080 brew nstall PACKAGE","link":"/dev/homebrew-proxy/"},{"title":"hugo试用","text":"前段时间发现了 go 写的博客生成程序，看到官网相关的介绍，最吸引人的一条就是只需要下载一个文件就可以运行了。 我一直用的是 hexo,就想着换 hugo 试试,最后感觉还是有些缺点，在所有准备工作都做完，为此还修改了一个python的转换程序出来，然后放弃了转换到 hugo 的想法首先，hugo的优点是单个文件，然后生成快，比hexo还要快，具体没有过多的比较。hugo作为新的blog生成程序，出现了几个不太和谐的问题。 对于中文支持，有些地方还有点问题。比如在我选的某款模板在category列表分页的地方的url编码。 生成的文件是用 title 字段生成的。对一些特殊字符处理的不够好，比如 . # 也许还有别的字符。 摘要部分的 markdown 内容没有进行格式化，导致列表页很乱。 我并没有去研究是 hugo 还是模板的问题。当然，总体来说 hugo 还是一款值得尝试的 github blog 程序。我还会关注，等到时机成熟时转换过去。 今天终于平滑的把’hexo’转换到了’hugo’,最终确认是模板的问题。转换的原因是由于我的blog文件是放置到OneDrive里面，而nodejs在初始化的时候会生存一个node_modules的文件夹来放必须的hexo的插件。在昨天更新的时候有个插件目录过深，导致OneDrive错误退出。于是下决心转换到hugo，文件夹清爽多了","link":"/dev/hugo试用/"},{"title":"ikvm.net简介","text":"ikvm.net是什么http://www.ikvm.net/ ikvm.net是能够运行在mono和.net framework的java虚拟机。它包括了 在.net中实现的一个java虚拟机 java类库的.net实现 java和.net的互操作工具 它可以再.net中使用java库。它包括一个ikvmc可以讲java字节码转换为.net IL。如果你在.net application中引用java library 。 ikvm现在最大化的实现了与jdk1.4的兼容。 引用java api 到.net 中首先需要添加IKVM.Runtime.dll ， OpenJDK.ClassLibrary.dll到项目中，使用ikvmc将jar转换为.net dll。 1ikvmc -target:library mylib.jar 例如我需要使用pdfbox在.net中调用，使用以下命令生成了PDFBox-0.7.3.dll 1ikvmc -target:library PDFBox-0.7.3.jar ikvm.net的组成IKVM.Runtime.dll: VM运行时和所有支持代码。它包括以下的功能： Byte Code JIT 编译器和验证器: 使用JIT将Java Byte Code编译为IL。 对象模式映射结构: 将.NET中的System.Object，System.String，System.Exception映射为java代码中的java.lang.Object， java.lang.String，java.lang.Throwable。 管理本地方法（在Classpath中）的.NET重新实现。 *IKVM.GNU.Classpath.dll: 被编译的GNU Classpath版本，它是由自由软件基金会实现的java类库和一些IKVM.NET附加代码组成的。注意：这里的GNU Classpath不是IKVM.NET的一部分，但是前者被用在IK.VM.NET中。 IKVM.JNI.[Mono|CLR-Win32].dll: 通过实现JNI接口管理C++汇编。作为一个可选部分，只在程序使用自己的本地库时才被用到。而对于纯java程序来讲是不会被用到的。 ikvm.exe: 与java.exe很类似的启动执行程序（动态模式）。 ikvmc.exe: 静态编译器，被用来编译java类和jar使其成为.NET汇编（静态模式）。 ikvmstub.exe: 一个从.NET汇编生成存根类的工具，就如javap一样反编译.NET汇编。IKVM.NET了解如何存根并用实际的.NET类型引用替换对存根的引用。 IKVM.AWT.WinForms.dll: 非常有限的零散AWT实现。 IKVM.NET 8.02015年的1月份ikvm发布了新的版本,支持java8.类库使用OpenJDK 8 . IKVM提供了两种模式。在动态模式下，它就像其他任何虚拟机那样直接运行Java应用。在静态模式下，Java字节码被重新编译为.NET库和可执行程序。 在使用意在运行在IKVM 上的Java代码时，通过在命名空间前面加上“cli.”，可以把.NET类导进来。为满足Java编译器的需求，还需要使用ikvmstub工具生成相应的Java存根文件。 可以使用nuget直接安装使用,在vs里使用nuget搜索ikvm下载安装即可.1install-package IKVM http://weblog.ikvm.net/2015/01/12/IKVMNET80ReleaseCandidate1.aspx https://www.nuget.org/packages/IKVM","link":"/dev/ikvm-net-introduction/"},{"title":"jenkins git 配置大意错误","text":"Jenkins新建项目中源码管理使用Git时遇到如下问题： Failed to connect to repository : Error performing command: git ls-remote -h git@192.168.199.89:zhangwei/jenkins-test.git HEAD 解决： 1.Jenkins服务器上查看git是否已安装及安装位置 git version whereis git 打开Jenkins的 主页面 &gt; 系统管理 &gt; Global Tool Configuration 可以看到错误提示：There’s no such executable git in PATH: /sbin, /usr/sbin, /bin, /usr/bin. 在出错的地方填入系统上git的地址： 一般默认填入 git 问题是很简单的问题，由于git的路径没有配置正确，导致了错误，而jenkins并没有将命令执行的错误消息给出来，只是给出了 git ls-remote -h git@192.168.199.89:zhangwei/jenkins-test.git HEAD 执行错误，误导了问题源。","link":"/dev/jenkins git 配置大意错误/"},{"title":"kettle 数据导入乱码.md","text":"在使用 kettle 处理数据，导入到 mysql 时出现了中文乱码的问题，有两种情况： mysql 编码问题，这个直接修改表编码即可 kettle 导入的编码问题，需要在【数据库连接】-【选项】里面添加一条 1characterEncoding:utf8","link":"/dev/kettle-characterEncoding/"},{"title":"jenkins钉钉机器人","text":"dingding-notificationshttps://plugins.jenkins.io/dingding-notificationsjenkinsfile 代码123456789post { success{ dingTalk( accessToken: &apos;7d7acde934be6177bb813100396a34e35ec2a84&apos;, imageUrl: &apos;http://xxxx.oss-cn-beijing.aliyuncs.com/success.png&apos;, jenkinsUrl: &apos;http://192.168.0.89:8080&apos;, message: &quot;${currentBuild.fullDisplayName} \\nJOB NAME: ${env.JOB_NAME} \\nBUILD NUMBER ${env.BUILD_NUMBER}\\nDeploy SUCCESS&quot;, notifyPeople: &apos;&apos;) } } 钉钉推送结果 从作者的源码里发现 title 和massege 混到一起没分开，在钉钉里面体验不够好，好处在于简单方便。 HTTP Request Pluginhttps://wiki.jenkins.io/display/JENKINS/HTTP+Request+Plugin钉钉文档https://open-doc.dingtalk.com/docs/doc.htm?spm=a219a.7629140.0.0.karFPe&amp;treeId=257&amp;articleId=105735&amp;docType=1 按照钉钉文档发送http请求，很容易定制出自己需要的通知效果。123456789101112131415161718httpRequest contentType: &apos;APPLICATION_JSON_UTF8&apos;, httpMode: &apos;POST&apos;, requestBody: &quot;&quot;&quot; { &quot;msgtype&quot;: &quot;markdown&quot;, &quot;markdown&quot;: {&quot;title&quot;:&quot;${currentBuild.fullDisplayName} 上线成功&quot;, &quot;text&quot;:&quot;![ok](http://xxx.oss-cn-beijing.aliyuncs.com/ok.png) ${currentBuild.fullDisplayName} 上线成功 \\n JOB NAME: ${env.JOB_NAME} \\n BUILD NUMBER ${env.BUILD_NUMBER}\\n ### Deploy SUCCESS \\n @xxxxxx \\n\\n #### [去 Jenkins 查看](http://192.1.1.89:8080/job/${env.JOB_NAME}/)&quot;, }, &quot;at&quot;: { &quot;atMobiles&quot;: [ &quot;xxxxx&quot; ], &quot;isAtAll&quot;: false } } &quot;&quot;&quot;, responseHandle: &apos;NONE&apos;, url: &apos;https://oapi.dingtalk.com/robot/send?access_token=xxxx&apos; 钉钉通知 使用HTTP Request Plugin 插件能够更好的满足部署上线个性化通知，满足自己的需要。","link":"/dev/jenkins钉钉机器人/"},{"title":"jenkins 配置说明","text":"这是给团队写的 jenkins 会出现问题的说明，简单的描述了 jenkins 使用中会出现的问题，以及应对方法。当然最好的应对方法是没有问题。 jenkins 是什么jenkins 自动化的命令运行引擎，pipeline （流水线） 是按照既定的逻辑来执行 jenkinsfile 中的代码。 也就是说它只是一个流程执行引擎，所有的命令都需要依赖其它软件包。 比如 npm ，maven, ansiable 等等。当使用 docker 来启动 jenkins 时，需要在 dockerfile 里面加上打包需要的依赖。 jenkins 结构参看提交到 github 的 docker 配置文件中的 jenkins 的配置。 这里可以直接使用 docker-compose 来启动和停止 jenkins， 文件说明 123456789101112131415161718# jenkins dockerfile 文件，留意几个文件映射version: &apos;3&apos;services: jenkins: user: root restart: always build: . image: dbj/jenkinsci volumes: - ./home:/root - ./data:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock - ./ansible:/etc/ansible ports: - &quot;8080:8080&quot; - &quot;50000:50000&quot; - &quot;10100:10100&quot; jenkins 文件说明 123456789101112root@pprt-s2:~/jenkins# tree -L 2.├── ansible│ └── hosts hosts 为 ansible 服务器组配置，新增服务器配置├── data data 映射为 jenkins 文件目录│ └── workspace workspace 为项目工作空间，每个pipeline 在这里都会又多文件夹├── docker-compose.yml ├── Dockerfile├── home 映射到 /root 主目录，保存 ssh key 和一些用户缓存│ ├── docker-entrypoint.sh│ ├── goproxy-shell│ └── maven Jenkins 错误jenkins 中所报的错误，都不是 jenkins 的错误，而是配置的错误，网络的错误，组件的错误。 如果出现错误参看 jenksin 日志，基本能够定位到错误问题。 配置的错误 配置发生错误，需要检测 jenkinsfile 语法是否正确。 所取的配置文件是否加载正确，传入的配置参数是否正确。 网络的错误 网络发生错误。主要是在访问三方依赖服务的时候会发生。 一是在拉取代码，推送打好的 docker 包时发生。 特别是在 github 上拉去代码，当拉去文件庞大时，访问国外网络资源，会导致 jenkins 命令执行超时的问题。 手工解决办法： 在 workspace 里面有和项目名称一致的目录开头的文件夹 123- &lt;project&gt; # 项目执行 pipeline 目录- &lt;project&gt;@script # 项目代码目录- &lt;project&gt;@tmp # 项目其他文件目录 比如jenkins 里面的 zeus 项目，会有 zeus zeus@script zeus@tmp 三个文件夹，如果有其他 git 代码地址一样的，可以直接将执行成功过的内容拷贝到需要执行的项目，我们的子项目都在一个仓库里，所以很容易找得到其他成功过的项目。 组件的错误 需要在 jenkins 里面正确的配置依赖组件，比如 npm 、ansible 、maven 等等。 涉及到的组件错误，参看 jenkins 日志都能够很好的解决。 jenkins 缓存在上线过程中，不管是 node 、PHP、maven 都会拉对应源文件，为了提交打包速度，会将指定的目录缓存在本地硬盘。 缓存的问题在于，可能会出现缓存中包冲突，如果对此有疑问，可以直接检查于项目同名的项目目录，这里编译后的代码和上线运行的代码一致 。 ansiable 上线问题我们上线 ansiable 主要是用来远程执行 docker-compose 命令，具体可以参考项目 depoly-config 分支下的 &lt;Project&gt;/file/bookplay.xml . 如果 docker 内部出现冲突，执行docker-compose 时命令执行未成功，输出的是警告，而不是错误。导致无法捕获。产生的原因基本上和docker natwork 有关， docker-compose down 无法关掉网络，导致执行失败。 这里需要手动到服务器去处理。网络问题是由于前期有容器做了网络共享，导致网络关闭失败。 产生这个问题的体现为: Jenkins 上线成功，但是线上代码还是旧版本。","link":"/dev/jenkins-deploy-question-md/"},{"title":"kettle随记","text":"注意排序，作为流处理，排序很重要，想要体会排序，可以理解一下普通排重和 hash 排重的区别。 普通排重，是需要排序后线性处理，排重的时候只需要比较相邻的就行了，不许要保存状态 hash排重，对需要排重的字段做 hash，比较 hash 值，所以不需要排序。 kettle 作为一个流处理工具，还是排序后做处理好一点，很多时候会避免掉很多的莫名其妙，其实是自己理解补充的问题。 kettle行转列 kettle 中提供了行转列，列转行的控价，但是适用性不高，需要特定的不是特别好用，在 kettle 的例子里面有一个用来处理的 JavaScript 的脚本 JavaScript - create new rows.ktr，可以用来坐行转列。 ‘’’ if (groupsField!=null) { var groups = groupsField.split(“,”); for (i=0;i&lt;groups.length;i++) { newRow = createRowCopy(getOutputRowMeta().size()); var rowIndex = getInputRowMeta().size(); newRow[rowIndex++] = trim( groups[i] ); newRow[rowIndex++] = &quot;N&quot;; putRow(newRow); } } var subgroup = “”; var ignore = “Y”; ‘’’ 这里把需要转为列的字段，创建新的列和一个标志位，然后按照标志位来过滤掉旧的数据，得到一个行转列的列表。 列转行 列转行和行转列一样，kettle 提供的都是一个分组的功能，时候，需要将多列转换成一行。 首先将需要的多个字段连接为一个字段，然后利用分组的功能将这一个字段使用连接符连接在一起，形成需要的一行数据。 更新和补充数据更新和补充，其实是一回事，比如有两个数据源，新的和旧的。需要用新的数据安装指定的 key 替换掉旧的数据，或者对旧数据做补充。首先对两列数据都按 key 排序 然后合并记录可以得到需要的数据。 join Row 在转换中，kettle似乎不能定义全局变量（也行是我没有找到），很多输入后需要定制输出文件，没有全局变量的话，需要在流程中把输出路径一直保持在流中。 这里其实可以在最初的步骤中将路径计算出来后，在需要的时候使用字段选择来选择出路径，然后用join Row将路径连接到尾部。（没有在列表中找到 join row 这个控件，可以到 Join Rows - adding fields to a stream.ktr 里面拷贝一个）","link":"/dev/kettle随记/"},{"title":"用 git 来 对letsencrypt 证书管理","text":"在做 https 的时候，使用了 letsencrypt 的证书，letsencrypt 的证书 90 天更新一次，正常情况下，只有一个域名就很方便处理，写一个 crontab 就搞定了。 letsencrypt 出了泛域名后，泛域名只支持 1 级的匹配。当有多个子域名使用的时候，管理起来就很难受。 所幸的是，acme.sh 支持 deploy_hook 来自行调用一个 shell 脚本。所以用不到10行的代码，做了一个利用 git 来管理域名证书。 首先在acme.sh里面配置一个 git 的地址。 申请泛域名时同时申请泛域名的主域名 hook 提交按照域名提交到不同的分支 线上对应的域名只需要配置一个 crontab 来定时拉 git ，重启服务。 思路虽然简单粗暴，这样做的好处是可以很清楚的在git上面看到证书的更新，同时更新也可以利用 gitlab 的 hook 通知到 钉钉。 12345678910111213141516171819202122232425262728293031323334353637383940git_deploy() { _cdomain=\"$1\" _ckey=\"$2\" _ccert=\"$3\" _cca=\"$4\" _cfullchain=\"$5\" _debug _cdomain \"$_cdomain\" _debug _ckey \"$_ckey\" _debug _ccert \"$_ccert\" _debug _cca \"$_cca\" _debug _cfullchain \"$_cfullchain\" GIT_REMOTE=\"${GIT_REMOTE:-$(_readaccountconf_mutable GIT_REMOTE)}\" if [ -z $GIT_REMOTE ];then _err \"Git remote is enpty! please add GIT_REMOTE to the account.conf file\" fi cd $DOMAIN_PATH # 判断是否有git源 if [ ! -f \".git\" ];then git init git remote add origin $GIT_REMOTE git add --all fi export GIT_DIR=$DOMAIN_PATH/.git DATE=$(date +%Y%m%d)# echo \"$DATE $_cdomain update time\" &gt;&gt;$DOMAIN_PATH/update_data.txt git add --all git commit -m \"update $DATE\" git push --force origin master:$_cdomain unset GIT_DIR# _err \"Not implemented yet\" return 1}","link":"/dev/letsencrypt证书同步管理/"},{"title":"mac .net core尝试","text":"以下是对.net core 发布的尝试，内容完全来自微软官方的文档粘贴，主要是用来记录尝试的过程。 下载https://www.microsoft.com/net/download .NET Core SDK = Develop apps with .NET Core and the SDK+CLI (Software Development Kit/Command Line Interface) tools.NET Core = Run apps with the .NET Core runtime 安装 SDKmacOS 10.11 (El Capitan) or higher is required. There are known issues with OpenSSL 0.9.8 and oh-my-zsh https://github.com/dotnet/core/blob/master/cli/known-issues.md 安装完后提示12$dotnet --helpzsh: command not found: dotnet 创建链接1ln -s /usr/local/share/dotnet/dotnet /usr/local/bin 再次执行123$dotnet --help.NET Command Line Tools (1.0.0-preview2-003121)...... 具体的步骤参考 https://www.microsoft.com/net/core#macos 创建项目123mkdir hwappcd hwappdotnet new 运行项目 12dotnet restoredotnet run 创建 website 入门https://docs.asp.net/en/latest/getting-started.html Install .NET CoreCreate a new .NET Core project:123mkdir aspnetcoreappcd aspnetcoreappdotnet new Update the project.json file to add the Kestrel HTTP server package as a dependency:12345678910111213141516{ &quot;version&quot;: &quot;1.0.0-*&quot;, &quot;buildOptions&quot;: { &quot;emitEntryPoint&quot;: true }, &quot;dependencies&quot;: { &quot;Microsoft.NETCore.App&quot;: { &quot;type&quot;: &quot;platform&quot;, &quot;version&quot;: &quot;1.0.0&quot; }, &quot;Microsoft.AspNetCore.Server.Kestrel&quot;: &quot;1.0.0&quot; }, &quot;frameworks&quot;: { &quot;netcoreapp1.0&quot;: { } }} Restore the packages:1dotnet restore Add a Startup.cs file that defines the request handling logic:123456789101112131415161718using System;using Microsoft.AspNetCore.Builder;using Microsoft.AspNetCore.Hosting;using Microsoft.AspNetCore.Http;namespace aspnetcoreapp{ public class Startup { public void Configure(IApplicationBuilder app) { app.Run(context =&gt; { return context.Response.WriteAsync(&quot;Hello from ASP.NET Core!&quot;); }); } }} Update the code in Program.cs to setup and start the Web host:123456789101112131415161718using System;using Microsoft.AspNetCore.Hosting;namespace aspnetcoreapp{ public class Program { public static void Main(string[] args) { var host = new WebHostBuilder() .UseKestrel() .UseStartup&lt;Startup&gt;() .Build(); host.Run(); } }} Run the app (the dotnet run command will build the app when it’s out of date):1dotnet run ###Browse to http://localhost:5000:","link":"/dev/mac-net-core-尝试/"},{"title":"Mac上制作Ubuntu USB启动盘","text":"前几天，小伙伴终于把测试服务器玩坏了，openssl 尝试了很久始终都不能正常的工作，不管是编译还是apt-get。最后还是决定重新装一下系统。记录一下其中做 ubuntu 启动盘的命令,一下部分内容来自网络， 一、下载ubuntu iso镜像去官网下，下载原版的，不要下xxx麒麟 二、将iso转换为img文件生成一个ubuntu_server.img的磁盘镜像文件，mac osx会默认追加一个.dmg 1$ hdiutil convert -format UDRW -o./ubuntu_server ./ubuntu-14.04.4-server-amd64.iso 三、查看USB的盘符查看当前系统上挂载的磁盘，其中/dev/disk3是我的USB磁盘。不同的系统disk后的数字可能不一样，但一般都是diskN的模式 12345678910111213141516171819202122$ diskutil list/dev/disk0 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *121.3 GB disk0 1: EFI EFI 209.7 MB disk0s1 2: Apple_CoreStorage Macintosh HD 120.5 GB disk0s2 3: Apple_Boot Recovery HD 650.0 MB disk0s3/dev/disk1 (internal, virtual): #: TYPE NAME SIZE IDENTIFIER 0: Apple_HFS Macintosh HD +120.1 GB disk1 Logical Volume on disk0s2 3F4EC944-D436-4957-A043-B0C1B211389F Unencrypted/dev/disk2 (disk image): #: TYPE NAME SIZE IDENTIFIER 0: Apple_partition_scheme +17.9 MB disk2 1: Apple_partition_map 32.3 KB disk2s1 2: Apple_HFS Flash Player 17.9 MB disk2s2/dev/disk3 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *7.8 GB disk3 1: DOS_FAT_32 EXPO 7.8 GB disk3s1 四、卸载USB磁盘使用diskutil unmountDisk卸载USB磁盘，注意卸载（umount）与弹出(eject)的区别 12$ diskutil unmountDisk /dev/disk3Unmount of all volumes on disk3 was successful 五、将镜像写入USB将第二步生成的img文件写入到USB磁盘/dev/rdisk3,写入的时候稍微等待一会 1234567$ sudo dd if=ubuntu_server.dmg of=/dev/rdisk3 bs=1mPassword:Sorry, try again.Password:579+0 records in579+0 records out607125504 bytes transferred in 123.920251 secs (4899324 bytes/sec) 六、弹出USB1$ diskutil eject /dev/disk1","link":"/dev/making-ubuntu-usb-boot-disk-on-mac/"},{"title":"mac重新安装java","text":"这是个很悲伤的故事，需要接一个java程序。然后当然是开始调试，然后部署到 tomcat 上面。但是不管怎么样编译，放到 tomcat 都会报错。于是开始各种怀疑。 首先这个程序是在 windows 的 JDK1.7 的版本编译，在 linux 的 tomcat 上面运行没有什么问题。拿到我这了后，在 mac 下 JDK1.8 的版本下运行，总是有个地方运行不对。于是开始怀疑是 JDK 的问题。就用命令行删除了和 JDK 有关的文件 。然后到 oracle 官网下载了对于的版本，发现安装到最后会出错，同事说 mac 下面不在支持 JDK1.7，于是接着下载 JDK1.8 来安装。但是在最后还是出了问题。死活都安装不上。此时心里真是有一万头草泥马呼啸而过。 最后搜索了很久，偶然的得到了一个地址 1https://support.apple.com/downloads/java 啥也不说，热泪一行。","link":"/dev/mac重新安装java/"},{"title":"minikube","text":"​​ Minikube 安装minikube 123curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 &amp;&amp; \\ chmod +x minikube &amp;&amp; \\ sudo mv minikube /usr/local/bin/ 阿里修改版的 1curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.30.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 安装 xhyve（已经被弃用） 123brew install docker-machine-driver-xhyvesudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyvesudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve 安装 hyperkit 12345curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \\&amp;&amp; chmod +x docker-machine-driver-hyperkit \\&amp;&amp; sudo mv docker-machine-driver-hyperkit /usr/local/bin/ \\&amp;&amp; sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \\&amp;&amp; sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit 参考： https://github.com/kubernetes/minikube/blob/master/docs/drivers.md#hyperkit-driver ​ https://blog.arkey.fr/2018/06/18/minikube-with-hyperkit/ 下载 kubectl 1brew install kubectl 启动 1minikube start --vm-driver=hyperkit --registry-mirror=https://registry.docker-cn.com --alsologtostderr --v 10 配置有问题可以 rm -rf ~/.minikube 后重新启动 ，修改驱动会导致失败 验证配置 1kubectl cluster-info 使用mimikube相同的Docker 主机镜像 12eval $(minikube docker-env) #设置eval $(minikube docker-env -u) #撤销设置 123kubectl config get-contexts #获取k8s环境kubectl config use-context docker-for-desktop # 切换到系统kubectl config use-context minikube #切换到minikube 打开控制面板 1minikube dashboard","link":"/dev/minikube/"},{"title":"nginx在proxy_pass里使用变量","text":"nginx server_name 配置文档： http://nginx.org/en/docs/http/server_names.html 在做 nginx 正则表达式 proxy_pass,nginx 反向代理不过去。 比如 12345678server { listen 80; server_name ~^(?&lt;user&gt;.+)\\.domain\\.com$; location / { proxy_pass &lt;http://$user.domain1.com&gt;; }} 会报出如下错误 no resolver defined to resolve xxx.xxx web端返回http 502 错误。 从 google 搜索得到的结果是 ：在Ngnix中如果用变量作为反向代理的地址时，容易出现“no resolver defined to resolve xxx.xxx”的问题 在 Nginx 0.6.18 后启用了 resolver 指令,在使用变量来构造某个server地址的时候一定要用resolver指令来指定DNS服务器的地址 所以在nginx的配置文件中的http{}部分添加一行resolver 8.8.8.8; resolver 8.8.8.8; 1234567891011121314server { listen 80; server_name ~^(?&lt;user&gt;.+)\\.pay\\.iov-smart\\.net$; location / { # set $subdomain \"\"; # if ($host ~* \"^(.+)\\.pay\\.iov-smart\\.net$\"){ # set $subdomain $1; # } resolver 8.8.8.8; proxy_pass http://$user.pay.iov-smart.net:6001; }}","link":"/dev/nginx-server-name-proxy-pass/"},{"title":"memcacheq Berkeleydb 备份恢复","text":"这是很久以前碰到的问题，一直躺在笔记里，没有整理出来。在 memecacheq 运行一段时间后会出现以下的问题,原因是 Berkeleydb 存储出问题了1[memcacheq] [Wed Jun 3 17:02:11 2015] &quot;BDB1546 unable to join the environment&quot; 日志清理有两种方法可以清理memcachedb的日志 利用memcache协议清理日志 1 echo db_archive|nc 127.0.0.1 21201 利用berkeleydb的命令行清理日志 1 /usr/local/BerkeleyDB.xxx/bin/db_archive -d -h home 备份及恢复 备份 1/usr/local/BerkeleyDB.xxx/bin/db_hotbackup [-c] -h home -b backup_dir 正常恢复 1/usr/local/BerkeleyDB.xxx/bin/db_recover -f -h home 灾难性恢复，新增一个c参数1/usr/local/BerkeleyDB.xxx/bin/db_recover -cf -h home 监控 使用memcache协议的stats, stats bdb, stats rep指令，例如1 echo stats|nc 127.0.0.1 21201 标准的bdb工具db_stat 故障恢复 master-slave结构的memcachedb,当master当机的时，可采用以下步骤恢复 停止slave 将slave作为master跑起来 1 memcachedb -p21201 -d -r -u root -H ./mdb_11211_m -N -R 127.0.0.1:31201 -M 有充裕的时间恢复原master","link":"/dev/memcacheq-berkeleydb-备份恢复/"},{"title":"nginx中gzip和cache配置","text":"gzip12345678910gzip on; #开启 gzip gzip_min_length 1k; # 最小压缩字节gzip_buffers 16 64k; # 16 *64k 内存gzip_http_version 1.1; #识别协议版本gzip_comp_level 6; # 压缩比率gzip_types text/plain application/javascript text/css application/xml; # 指定压缩的头gzip_vary on; # 给代理服务器使用 ​gzip_proxied any; #代理结果压缩​ cache1234567891011121314151617181920212223242526proxy_cache_path /dev/shm/proxy_cache_dir levels=1:2 keys_zone=mycache:200m inactive=1d max_size=1g;# proxy_cache_path 缓存文件路径# levels 设置缓存文件目录层次，1:2 表示两级# key_zone 缓存名字和共享内存大小# inactive 指定时间没人访问删除 # max_size 最大缓存空间，缓存空间满后覆盖缓存时间最长的proxy_temp_path /data/nginx_cache/proxy_cache/proxy_temp_dir;# 使用temp存储， 如果不使用 可以在 max_size 后关闭 use_temp_path=off;proxy_cache_valid 200 302 10m ; # 指定状态码，缓存有效时间 针对不同的response code设定不同的缓存时间，如果不设置code，默认为200,301,302,也可以用any指定所有codeproxy_cache mycache;# 设置开启对后端响应的缓存 关闭为 proxy_cache off ，参数值为名称proxy_cache_key $scheme$proxy_host$request_uri;# 给缓存设定keyproxy_cache_bypass $http\\_pragma $http_authorization;# 指定哪些响应在某些值不为空或不为0的情况下不走缓存proxy_cache_min_uses 1;# 指定在多少次请求之后才缓存响应内容 默认为1proxy_cache_use_stale off;# 指定在后端服务器在返回什么状态码的情况下可以使用过期的缓存，比如proxy_cache_use_stale error timeout invalid_header http_500 http_502 http_503 http_504;proxy_cache_lock off;# 默认不开启，开启的话则每次只能有一个请求更新相同的缓存，其他请求要么等待缓存有数据要么限时等待锁释放;nginx 1.1.12才开始有proxy_cache_lock_timeout 5s;# 等待缓存锁超时之后将直接请求后端，结果不会被缓存 ; nginx 1.1.12才开始有add_header Nginx-Cache &quot;$upstream_cache_status&quot;;# http head 返回 HIT 为命中 MISS 为未命中 EXPIRED 过期 cache example123456789101112131415161718192021http { # we set this to be on the same filesystem as proxy_cache_path proxy_temp_path /usr/local/nginx/proxy_temp; # good security practice dictates that this directory is owned by the # same user as the user directive (under which the workers run) proxy_cache_path /usr/local/nginx/proxy_temp keys_zone=CACHE:10m levels=1:2 inactive=6h max_size=1g;​ server { location / { # using include to bring in a file with commonly-used settings include proxy.conf; # referencing the shared memory zone defined above proxy_cache CACHE; proxy_cache_valid any 1d; proxy_cache_bypass $http_pragma $http_authorization; proxy_cache_min_uses 3; proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504; proxy_pass http://upstream; } }}","link":"/dev/nginx中gzip和cache配置/"},{"title":"nsq 随录","text":"nsqnsq 官网nsqgo-nsq 组件nsqlookupd是守护进程负责管理拓扑信息。客户端通过查询 nsqlookupd 来发现指定话题（topic）的生产者，并且 nsqd 节点广播话题（topic）和通道（channel）信息。 nsqd负责接收、排队、投递消息，可以独立运行，也可以接入到 nsqlookupd 的集群 限定内存占用nsqd 提供一个 –mem-queue-size 配置选项，这将决定一个队列保存在内存中的消息数量。如果队列深度超过此阈值，消息将透明地写入磁盘。nsqd 进程的内存占用被限定于 –mem-queue-size * #of_channels_and_topics： nsqadminWEB UI，用来汇集集群的实时统计，并执行不同的管理任务。 设计publishing –&gt; Topics -&gt; Channels –&gt; Consumers publishing 投递一个消息到 Topics Topics 将消息传递给 Channels ，这里是一对多， 每个 Channels 可连接多个 Consumer,但是只能被一个 Consumer 消费掉 docker-compose12345678910111213141516171819202122232425262728293031version: &apos;2&apos;services: nsqlookup: image: nsqio/nsq hostname: nsqlookup ports: - &quot;4160:4160&quot; - &quot;4161:4161&quot; command: /nsqlookupd nsq: image: nsqio/nsq hostname: nsq volumes: - ./data:/data ports: - &quot;4150:4150&quot; - &quot;4151:4151&quot; links: - nsqlookup:nsqlookup command: /nsqd --broadcast-address nsq --lookupd-tcp-address=nsqlookup:4160 nsqadmin: image: nsqio/nsq hostname: nsqadmin links: - nsqlookup:nsqlookup ports: - &quot;4171:4171&quot; command: /nsqadmin --lookupd-http-address=nsqlookup:4161 go-nsq example12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package mainimport ( &quot;github.com/nsqio/go-nsq&quot; &quot;flag&quot; &quot;log&quot; &quot;time&quot;)var url string func init(){ flag.StringVar(&amp;url,&quot;url&quot;,&quot;127.0.0.1:4150&quot;,&quot;nsqd&quot;) flag.Parse()}// 生产者func startProducer(){ config := nsq.NewConfig() producer,err := nsq.NewProducer(url,config) if err != nil { log.Fatal(err) } for{ if err := producer.Publish(&quot;test&quot;,[]byte(&quot;text message&quot;)); err!=nil { log.Fatal(&quot; publish error:&quot;+err.Error()) } log.Println(&quot;publish: test&quot; ) time.Sleep(1*time.Second) }}// 消费者func startConsumer(c string){ cfg := nsq.NewConfig() consumer, err := nsq.NewConsumer(&quot;test&quot;,c,cfg) if err != nil { log.Fatal(err) } consumer.AddHandler(nsq.HandlerFunc(func(message *nsq.Message) error { log.Println(&quot;Consumer:&quot;+ c+&quot; channel #&quot; + string(message.Body)) return nil })) if err := consumer.ConnectToNSQD(url); err != nil { log.Fatal(err) } &lt;-consumer.StopChan}func main() { go startConsumer(&quot;c1&quot;) go startConsumer(&quot;c2&quot;) go startConsumer(&quot;c3&quot;) startProducer() } 相关连接nsq wiki 从nsq搭建到go-nsq的使用","link":"/dev/nsq/"},{"title":"使用magick.net将pdf转换为图片","text":"现在手上有个需求是要将pdf转换为一页一页的image.最开始找到的是pdfbox来处理pdf的.在pdfbox.apache.org的官网首页写了一句’convert you pdfs to image files’.所以最开始就使用pdfbox来作为转换库.但是在后面却发现出现了一个问题.由于暂时解决不了,发现了magick这个东西.可以用来处理100多种图片格式,并且提供了各种语言的api. ###下载magick.net ImageMagick http://www.imagemagick.org/ .net上官网提供了两个类库 magick.net 和imagemagickapp.我选择了magick.net这个库.下载地址 http://magick.codeplex.com/releases/view/137513下载 Magick.NET-7.0.0.0007-Q16-AnyCPU-net40-client.zip 其中下载的时候最好选择AnyCPU的库,在vs编译的时候不需要考虑平台性的问题.Q8 Q16 q16-HDRI分别表示8位 16位和32位的.可以根据需要选择不同版本的库. ###下载安装ghostscript 根据自己的需要在ghostscript.com选择下载32位或64位的库.下载地址如下: http://ghostscript.com/download/gsdnld.html 如果不安装ghostscript,magick.net在执行Read函数读取pdf时会报如下错误. 1iisexpress.exe: FailedToExecuteCommand `&quot;gswin32c.exe&quot; -q -dQUIET -dSAFER -dBATCH -dNOPAUSE -dNOPROMPT -dMaxBitmap=500000000 -dAlignToPixels=0 -dGridFitTT=2 &quot;-sDEVICE=pngalpha&quot; -dTextAlphaBits=4 -dGraphicsAlphaBits=4 &quot;-r300x300&quot; &quot;-sOutputFile=C:/Users/ZHANGW~1/AppData/Local/Temp/magick-10612rmwPHf2ITKBB%d&quot; &quot;-fC:/Users/ZHANGW~1/AppData/Local/Temp/magick-10612_fb2bvFOsIGG&quot; &quot;-fC:/Users/ZHANGW~1/AppData/Local/Temp/magick-10612Pg1c2Td4S-bk&quot;&apos; (ϵͳ�Ҳ���ָ�����ļ��� ) @ error/delegate.c/ExternalDelegateCommand/459 下载安装Visual C++ Redistributable for Visual Studio .NET 4.0: Visual C++ Redistributable for Visual Studio 2012 .NET 2.0: Visual C++ Redistributable for Visual Studio 2008 x64 ro x86 由于开发是在vs的环境,所以不需要安装这步.在部署的时候忘了这步,导致程序一直报错找不到magick-xxx.dll.以为是环境的原因,错误的在编译环境上折腾了很久. 生成image在工程中引用Magick.net.当然也可以使用nuget来引入.转换代码如下,filepath为pdf的路径. 123456789101112131415161718192021222324MagickReadSettings settings = new MagickReadSettings();settings.Density = new MagickGeometry(72, 72); //设置格式using (MagickImageCollection images = new MagickImageCollection()){ images.Read(filepath, settings); int pageCount = images.Count(); for (int i = 0; i &lt; page;i++ ) { MagickImage image = images[i]; image.Format = MagickFormat.Jpeg; string path = string.Format(&quot;{0}/{1}.jpg&quot;, dir, i);//相对路径 string filename = &quot;d:/img/&quot; + path; image.Write(filename); }} 这里转换就完成了. ###IKVM和pdfbox生成的问题 最开始我是使用pdfbox来生成的.代码如下: 1234567891011121314151617PDDocument doc = PDDocument.load(this._path);int page= doc.getDocumentCatalog().getAllPages().size();//获取总的页数for (int i = 0; i &lt; page; i++){ PDPage pdfPage = allpages.get(i) as PDPage; java.awt.image.BufferedImage image = pdfPage.convertToImage(); string path = string.Format(&quot;{0}/{1}.jpg&quot;, dir, i);//相对路径 string filename =&quot;d:\\img&quot; + path; ImageIO.write(image, &quot;jpg&quot;, new java.io.File(filename));} 运行的时候在下面这行代码中出错了. java.awt.image.BufferedImage image = pdfPage.convertToImage(); 原因是IKVM.GNU.Classpath对awt和Swing实现不完整.导致了错误.当然如果不需要使用awt和Swing的库.使用.net来调用java库IKVM还是能够解决一些问题的. ###其他 PDFsharp http://pdfsharp.net/wiki/PDFsharpFAQ.ashx itextsharp http://itextsharp.sourceforge.net/ ImageMagick http://www.imagemagick.org/http://magick.codeplex.com/http://magick.codeplex.com/releases/view/137513 imageMagick 的另一个.net api实现 http://sourceforge.net/projects/imagemagickapp/","link":"/dev/pdf-convert-to-image-for-magick-net/"},{"title":"openwrt","text":"openwrt 框架结构 1234567891011121314151617181920212223242526272829303132tools和toolchain目录：包含了一些通用命令, 用来生成固件, 编译器, 和C库.build_dir/host目录：是一个临时目录, 用来储存不依赖于目标平台的工具.build_dir/toolchain-目录：用来储存依赖于指定平台的编译链. 只是编译文件存放目录无需修改.build_dir/target-目录：用来储存依赖于指定平台的软件包的编译文件, 其中包括linux内核, u-boot, packages,只是编译文件存放目录无需修改.staging_dir目录：是编译目标的最终安装位置, 其中包括rootfs, package, toolchain.package目录：软件包的下载编译规则, 在OpenWrt固件中, 几乎所有东西都是.ipk, 这样就可以很方便的安装和卸载.target目录：目标系统指嵌入式设备, 针对不同的平台有不同的特性, 针对这些特性, &quot;target/linux&quot;目录下按照平台进行目录划分, 里面包括了针对标准内核的补丁, 特殊配置等.bin目录：编译完OpenWrt的二进制文件生成目录, 其中包括sdk, uImage, u-boot, dts, rootfs构建一个嵌入式系统完整的二进制文件.config目录：存放着整个系统的的配置文件.docs目录：里面包含了整个宿主机的文件源码的介绍, 里面还有Makefile为目标系统生成docs.include目录：里面包括了整个系统的编译需要的头文件, 但是是以Make进行连接的.feeds目录：扩展软件包索引目录.scripts目录：组织编译整个OpenWrt的规则.tmp目录：编译文件夹, 一般情况为空.dl目录：所有软件的下载目录, 包括u-boot, kernel.logs目录：如果编译出错, 可以在这里找到编译出错的log. jshn : 一个 josn 对象转换库ubus : 一个系统总线","link":"/dev/openwrt/"},{"title":"petl函数表","text":"petl文档地址： https://petl.readthedocs.io/en/latest/index.html Extract/Load可以读取 csv pickle text xml html json database excel array hdfs 等文件 Transfrom 转换Basicpetl.transform.basics. head(table, n=5) 取开始的行 tail(table, n=5) 取尾部的行 rowslice(table, *sliceargs) 对行做切片 cut(table, *args, **kwargs) 取指定列 cutout(table, *args, **kwargs) 排除指定列 movefield(table, field, index) 移动列到新的位置 cat(*tables, **kwargs) 连接两个表 stack(*tables, **kwargs) 连接两个表，不匹配表头 skipcomments(table, prefix) 跳过包含指定前缀的行 addfield(table, field, value=None, index=None, missing=None) 添加一个新的字段 addcolumn(table, field, col, index=None, missing=None) 添加新的数据列(和addfield有些相似) addrownumbers(table, start=1, step=1, field=’row’) 添加行计数 addfieldusingcontext(table, field, query) 添加字段，query是一个回调，把相邻三行作为三个参数传入 annex(*tables, **kwargs) join 两个或多个表，按行连接 Headerpetl.transform.headers rename(table, *args, **kwargs) 修改名称 setheader(table, header) 设置表头 extendheader(table, fields) 扩展表头（可以用来补全表头信息） pushheader(table, header, *args) 设置表头（将第一行当中数据，而不是表头) prefixheader(table, prefix) 为表头添加前缀 suffixheader(table, suffix) 为表头添加后缀 sortheader(table, reverse=False, missing=None) 排序 skip(table, n) 跳过行的项不为 n 的行 Convertingpetl.transform.conversions convert(table, *args, **kwargs) 转换（包括数据类型，格式，替换，lambda 等) convertall(table, *args, **kwargs) 同convert convertnumbers(table, strict=False, **kwargs) 转换为数值 replace(table, field, a, b, **kwargs) 替换 replaceall(table, a, b, **kwargs) 替换全部 format(table, field, fmt, **kwargs) 格式化 formatall(table, fmt, **kwargs) 格式化全部 interpolate(table, field, fmt, **kwargs) 插入 interpolateall(table, fmt, **kwargs) update(table, field, value, **kwargs) 更新 Selecting rowspetl.transform.selects select(table, *args, kwargs) 根据条件选择行 （扩展出各种 select 的快捷方法 selectusingcontext(table, query) 基于上一行和下一行来选择数据 rowlenselect(table, n, complement=False) 选择长度为n的行 facet(table, key) 按指定的key分组，将返回值映射到字典 biselect(table, *args, **kwargs) 拆分成两个表，一个为选择的行，一个为抛弃的行 Regular expressionspetl.transform.regex search(table, *args, **kwargs) 查询返回匹配行 searchcomplement(table, *args, **kwargs) 查询返回不匹配的行 sub(table, field, pattern, repl, count=0, flags=0) 替换 split(table, field, pattern, newfields=None, include_original=False, maxsplit=0, flags=0) 分割指定fields capture(table, field, pattern, newfields=None, include_original=False, flags=0, fill=None) 分组，通过表达式分组来分割fields Unpacking compound valuespetl.transform.unpacks unpack(table, field, newfields=None, include_original=False, missing=None) 拆分 list 或 tuples unpackdict(table, field, keys=None, includeoriginal=False, samplesize=1000, missing=None) 拆分 dictionary Transforming rowspetl.transform.maps fieldmap(table, mappings=None, failonerror=False, errorvalue=None) 转换表，在输入与输出之间映射 rowmap(table, rowmapper, header, failonerror=False) 转换表，函数 rowmapmany(table, rowgenerator, header, failonerror=False) 转换表，将行映射到任意行 rowgroupmap(table, key, mapper, header=None, presorted=False, buffersize=None, tempdir=None, cache=True) Sortingpetl.transform.sorts sort 排序 mergesort 多个表排序，输出一个表 issorted 判断是否为排序表 Joinspetl.transform.joins join 根据指定的键连接表 leftjoin 左连接 lookupjoin 左连接，如果右表多，任意选择行 rightjoin 右连接 outerjoin 完全连接，保留所有，为空填充None crossjoin 笛卡尔连接 antijoin 返回左表中没有在右表出现的行 unjoin 将一个表拆成两个表 hashjoin hashleftjoin hashlookupjoin hashrightjoin hashantijoin Set operationspetl.transform.setops complement 返回a中不在b中的行 diff 查找两个表之间的差异，返回两个表，分别为b表不在a中的，a表不在b中的 recordcomplement 查找不在b中的记录 recorddiff 查找两个表之间的差异 intersection 返回a也在b中的数据 hashcomplement hashintersection Deduplicating rowspetl.transform.dedup duplicates 选出给定键具有重复的行 unique 选出给定键唯一行 conflicts 选出相同键值，但其他行不同的 distinct 返回表中不同行 isunique 判断是否唯一 Reducing rows (aggregation)petl.transform.reductions aggregate 聚集，对指定键做聚合 rowreduce 对行根据指定键做reduce mergeduplicates 合并给定键下的重复行 merge 根据指定键合并表 fold python reduce 操作 groupcountdistinctvalues groupselectfirst ‘’’还有很多，不想写了。。。‘’’","link":"/dev/petl函数表/"},{"title":"python安装PIL","text":"安装依赖123sudo apt-get install libjpeg-devsudo apt-get install libfreetype6-devsudo apt-get install zlib-devel 安装 PIL1sudo pip install -U PIL --allow-external PIL --allow-unverified PIL macmac 安装 PIL 的时候出现了两个错误，主要是库的问题. 错误12345_imagingft.c:73:10: fatal error: &apos;freetype/fterrors.h&apos; file not found#include &lt;freetype/fterrors.h&gt; ^1 error generated.error: command &apos;cc&apos; failed with exit status 1 方法1ln -s /usr/local/include/freetype2 /usr/local/include/freetype 错误1234567/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.9.sdk/usr/include/tk.h:78:11: fatal error: &apos;X11/Xlib.h&apos; file not found# include &lt;X11/Xlib.h&gt; ^1 error generated.error: command &apos;cc&apos; failed with exit status 1 方法1ln -s /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.9.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers/X11 /usr/local/include/X11 ubuntu 上安装的错误错误123456789imaging.c:75:20: fatal error: Python.h: No such file or directory #include &quot;Python.h&quot; ^compilation terminated.error: command &apos;x86_64-linux-gnu-gcc&apos; failed with exit status 1 方法123上面是 python 库的问题，可能是默认的 python 路径的问题，可以重新安装来解决sudo apt-get install python-dev 错误1IOError: decoder zip not available 方法有可能是 zlib 的问题12345678910sudo apt-get install python-dev libjpeg-dev libfreetype6-dev zlib1g-dev# create these links, if already exists, remove it and re-link itln -s /usr/lib/x86_64-linux-gnu/libjpeg.so /usr/libln -s /usr/lib/x86_64-linux-gnu/libfreetype.so /usr/libln -s /usr/lib/x86_64-linux-gnu/libz.so /usr/lib# reinstall PILpip uninstall PILpip install PIL 错误1python PIL bug: NoneType object has no attribute bands 方法在使用 python PIL 获取分量的时候出现了错误1r,g,b,a = image.split() 1&apos;NoneType&apos; object has no attribute &apos;bands&apos; 原因是图片遇到 mode 为 RGBA 的时候会出现这个问题。 123456789101112131415vim /usr/local/lib/python2.7/dist-packages/PIL/Image.py定位到1501 行1494 def split(self):1495 &quot;Split image into bands&quot;14961497 self.load() #1498 if self.im.bands == 1:1499 ims = [self.copy()]1500 else:1501 #self.load()1502 ims = []1503 for i in range(self.im.bands):1504 ims.append(self._new(self.im.getband(i)))1505 return tuple(ims) 将else中得self.load() 放到 if 外面来 参考http://stackoverflow.com/questions/20325473/error-installing-python-image-library-using-pip-on-mac-os-x-10-9 http://stackoverflow.com/questions/19532125/cant-install-pil-after-mac-os-x-10-9 http://stackoverflow.com/questions/3544155/about-the-pil-error-ioerror-decoder-zip-not-available","link":"/dev/python-install-pil/"},{"title":"php和nginx使用x-accel-redirect做文件下载","text":"1234567891011$file_url= \"g1/\".$file_path;$file_name=$title.'.'.$file_ext;header(\"Content-Type:application/octet-stream;charset=utf-8\");header('Content-Disposition: attachment; filename='.$file_name);header('X-Accel-Redirect: /filedfs/'.$file_url);header(\"X-Accel-Buffering: yes\");header(\"X-Accel-Limit-Rate :102400\"); //速度限制 Byte/sheader(\"Accept-Ranges: none\");//单线程 限制多线程header(\"X-Accel-Charset: utf-8\"); 1234location /filedfs/ { internal; proxy_pass http://10.8.8.100:8808/;} 首先在 php 里面设置了 X-Accel-Redirect 值为 /filedfs/ + fileurl .然后 nginx 里面将 /filedfs/ 转发到固定的url .并设置连接为内部的 internal . 遇到的问题 : 在配置 location 的时候. location 后面加了一个~ ,变成了正则表达式匹配.导致 proxy_pass 后面不能够添加/. 对于nginx的internal的理解不足. 参考 http://wiki.nginx.org/XSendfile http://kovyrin.net/2010/07/24/nginx-fu-x-accel-redirect-remote/ http://blog.csdn.net/h70614959/article/details/37766697 http://blog.csdn.net/guichenglin/article/details/7737790","link":"/dev/php-and-nginx-x-accel-redirect/"},{"title":"RocketMQ小记","text":"Rocketmqhttps://rocketmq.apache.org/docs/quick-start/ github： https://github.com/apache/rocketmq Apache RocketMQ 架构 Name Server :是一个无状态节点，可集群部署，在消息队列 MQ 中提供命名服务，更新和发现 Broker 服务 Broker ： 消息中转角色，负责存储、转发消息。 分为 Master Broker 和 Slave Broker ，一个 Master Broker 可以对应多个 Slave Broker 。Broker 启动后将自己注册到 Name Server 。随后每30秒定期向 Name Server 上报 Topic 路由信息。 生产者： 与 Name Server 集群中的其中一个节点（随机）建立长连 。 定期从Name Server 读取 Topic 路由。并向提供 Topic 服务的 Master Broker 建立长链接 。 定时向Master Broker 发送心跳 消费者： 与 Name Server 集群中的其中一个节点建立长链接。定期中Name Server 拉去Topic 路由信息。 并向提供 Topic 的Master Broker、slave Broker 建立长连。且定时向 Master Broker 、Slave Broker 发送心跳。 消费者即可以从 Master Broker 订阅消息，也可以从 Slave Broker 订阅消息，订阅规则由 Broker 决定。 功能发布/订阅消息传递模型 财务级交易消息 各种跨语言客户端，例如Java，C / C ++，Python，Go 可插拔的传输协议，例如TCP，SSL，AIO 内置的消息跟踪功能，还支持开放式跟踪 多功能的大数据和流生态系统集成 按时间或偏移量追溯消息 可靠的FIFO和严格的有序消息传递在同一队列中 高效的推拉消费模型 单个队列中的百万级消息累积容量 多种消息传递协议，例如JMS和OpenMessaging 灵活的分布式横向扩展部署架构 快如闪电的批量消息交换系统 各种消息过滤器机制，例如SQL和Tag 用于隔离测试和云隔离群集的Docker映像 功能丰富的管理仪表板，用于配置，指标和监视 认证与授权 核心模块rocketmq-broker：接受生产者发来的消息并存储（通过调用rocketmq-store），消费者从这里取得消息 rocketmq-client：提供发送、接受消息的客户端API。 rocketmq-namesrv：NameServer，类似于Zookeeper，这里保存着消息的TopicName，队列等运行时的元信息。 rocketmq-common：通用的一些类，方法，数据结构等。 rocketmq-remoting：基于Netty4的client/server + fastjson序列化 + 自定义二进制协议。 rocketmq-store：消息、索引存储等。 rocketmq-filtersrv：消息过滤器Server，需要注意的是，要实现这种过滤，需要上传代码到MQ！（一般而言，我们利用Tag足以满足大部分的过滤需求，如果更灵活更复杂的过滤需求，可以考虑filtersrv组件）。 rocketmq-tools：命令行工具。 消息模型 MessageMessage（消息）就是要传输的信息。 一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。 一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题。 TopicTopic（主题）可以看做消息的规类，它是消息的第一级类型。比如一个电商系统可以分为：交易消息、物流消息等，一条消息必须有一个 Topic 。 Topic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。 一个 Topic 也可以被 0个、1个、多个消费者订阅。 TagTag（标签）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。 标签有助于保持您的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。 Group分组，一个组可以订阅多个Topic。 分为ProducerGroup，ConsumerGroup，代表某一类的生产者和消费者，一般来说同一个服务可以作为Group，同一个Group一般来说发送和消费的消息都是一样的 Queue在Kafka中叫Partition，每个Queue内部是有序的，在RocketMQ中分为读和写两种队列，一般来说读写队列数量一致，如果不一致就会出现很多问题。 Message QueueMessage Queue（消息队列），主题被划分为一个或多个子主题，即消息队列。 一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。 消息的物理管理单位。一个Topic下可以有多个Queue，Queue的引入使得消息的存储可以分布式集群化，具有了水平扩展能力。 Offset在RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用Offset 来访问，Offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限。 也可以认为 Message Queue 是一个长度无限的数组，Offset 就是下标。 消息消费模式消息消费模式有两种：Clustering（集群消费）和Broadcasting（广播消费）。 默认情况下就是集群消费，该模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。 而广播消费消息会发给消费者组中的每一个消费者进行消费。 Message OrderMessage Order（消息顺序）有两种：Orderly（顺序消费）和Concurrently（并行消费）。 顺序消费表示消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列。 并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。","link":"/dev/rocketmq/"},{"title":"saltstack-cp模块","text":"git_file1sudo salt &apos;*&apos; cp.get_file salt://files/1.txt /srv/1.txt [template=jinja] [gzip=5] 其中salt: 所指定的位置为 /srv/salt ,可以指定模板 和 压缩等级 get_dircp.get_dir可以从master下载整个目录，语法如下：1# salt &apos;*&apos; cp.get_dir salt://etc/apache2 /etc cp.get_dir也支持模板和压缩：1# salt &apos;*&apos; cp.get_dir salt://etc/{{pillar.webserver}} /etc gzip=5 template=jinja get_urlcp.get_url可以从一个URL地址下载文件，URL可以是msater上的路径（salt://），也可以是http网址。 12# salt &apos;*&apos; cp.get_url salt://my/file /tmp/mine# salt &apos;*&apos; cp.get_url http://www.abc.com /tmp/index.html get_templatecp.get_template可以在文件下载之前用模板引擎处理。 1# salt &apos;*&apos; cp.get_template salt://path/to/template /minion/dest pushcp.push可以从客户端传文件到master上，处于很明显的安全考虑，默认没有启用此功能","link":"/dev/saltstack-cp模块/"},{"title":"saltstack-event","text":"示例salt_event.py123456import salt.utils.event__opts__ = salt.config.client_config(&apos;/etc/salt/master&apos;)event = salt.utils.event.MasterEvent(__opts__[&apos;sock_dir&apos;])for eachevent in event.iter_events(full=True): print eachevent print &apos;---------------&apos; event 是在 master 端执行1event = salt.utils.event.MasterEvent(__opts__[&apos;sock_dir&apos;]) 运行event1python salt_event.py 利用python命令行运行这个文件后，用来捕获执行的事件，例如执行命令 salt ‘*’ test.ping","link":"/dev/saltstack-event/"},{"title":"saltstack-grains","text":"用来匹配minion的grains，是指那些关于minion主机的静态信息，比如OS，软件版本，虚拟化，CPU，内存等等。 查看grains12salt ‘*’ grains.items #打印grainssalt ‘*’ grains.item [keyname] #打印指定的值 在minion 中配置grains/etc/salt/grains 自定义 grains/etc/salt/minion 自定义 grains1234567grains: roles: - webserver - memcache deployment: datacenter4 cabinet: 13 cab_u: 14-15 在/srv/salt/_grains配置grains创建 grains 目录1mkir /srv/salt/_grains &amp;&amp; cd /srv/salt/_grains test.py123def foo(): s={’name’:’zhang’,’age’:10} return s 在top file中使用grains1234567{% set node_type = salt[&apos;grains.get&apos;](&apos;node_type&apos;, &apos;&apos;) %}{% if node_type %} &apos;node_type:{{ self }}&apos;: - match: grain - {{ self }}{% endif %} 同步后查看grains值12salt ‘*’ saltutil.sync_all //同步到客户端 saltutil.sync_grains saltutil.sync_all state.highstatesalt ‘*’ grains.item name //输出name的名称","link":"/dev/saltstack-grains/"},{"title":"saltstack-returners","text":"return 是在 minion 端运行的一个回调 创建return目录1mkdir /srv/salt/_returners 创建一个local_return/srv/salt/_returners/local_return.py 实现123456789101112#coding=utf8import jsondef __virtual__(): return &apos;local_return&apos;def returner(ret): &apos;&apos;&apos; Return data to the local file &apos;&apos;&apos; result_file = &apos;/var/local/salt/returner&apos; result = file(result_file,&apos;a+&apos;) result.write(str(json.dumps(ret.values()))[1:-1]+&apos;\\n&apos;) result.close() virtual() 用来定义返回器的名称 returner(ret) 为具体返回器执行的值 同步1sudo salt &apos;*&apos; saltutil.sync_all 执行minion 端在执行完 test.ping 后会回调 local_runturn1sudo salt &apos;*’ test.ping --return local_return return reids 示例1234567891011121314151617181920212223242526try: import redis HAS_REDIS = Trueexcept ImportError: HAS_REDIS = False__virtualname__ = &apos;redis&apos;def __virtual__(): if not HAS_REDIS: return False return __virtualname__def returner(ret): &apos;&apos;&apos; Return information to a redis server &apos;&apos;&apos; # Get a redis connection serv = redis.Redis( host=&apos;redis-serv.example.com&apos;, port=6379, db=&apos;0&apos;) serv.sadd(&quot;%(id)s:jobs&quot; % ret, ret[&apos;jid&apos;]) serv.set(&quot;%(jid)s:%(id)s&quot; % ret, json.dumps(ret[&apos;return&apos;])) serv.sadd(&apos;jobs&apos;, ret[&apos;jid&apos;]) serv.sadd(ret[&apos;jid&apos;], ret[&apos;id&apos;])","link":"/dev/saltstack-returners/"},{"title":"saltstack-pillar","text":"Pillar是Salt用来分发全局变量到所有minions的一个接口。Pillar data的管理类似于Salt State Tree。 创建 pillar 目录和 top.sls 文件1mkdir /srv/pillar/ &amp;&amp; vim /srv/pillar/top.sls 内容为123base: ‘*&apos; - cache 1234567{% if grains[&apos;os&apos;] == &apos;RedHat&apos; %}apache: httpdgit: git{% elif grains[&apos;os&apos;] == &apos;Debian&apos; %}apache: apache2git: git-core{% endif %} 查看 pillar1sudo salt &apos;*&apos; pillar.items 在State SLS files中使用123git: pkg.installed: - name: {{ pillar[&apos;git&apos;] }} 参考http://docs.saltstack.cn/zh_CN/latest/topics/pillar/index.html","link":"/dev/saltstack-pillar/"},{"title":"saltstack-states","text":"这里用 ubuntu 中安装 redis 作为示例 创建 top.sls/srv/salt/top.sls123base: &apos;server-01&apos;: - roles.redis server-01 是需要安装redis的一台机器 创建redis states 树/srv/salt/roles/redis/init.sls12345678910111213141516171819redis-server: pkg: - installed #使用 pkg install 安装 redis-server 包 service: - name: redis-server - running #运行 - require: - pkg: redis-server - watch: - pkg: redis-server - file: /etc/redis/redis.conf/etc/redis/redis.conf: file.managed: - source: salt://roles/redis/redis.conf - template: jinja - user: root - group: root - mode: 644 参考http://docs.saltstack.cn/zh_CN/latest/topics/tutorials/starting_states.html","link":"/dev/saltstack-states/"},{"title":"saltstack-runners","text":"创建目录mkdir -p /srv/salt/_runner ; cd /srv/salt/_runner 修改配置文件runner_dirs: [‘/srv/salt/_runner’] 编写runner123456789#!/usr/bin/evn python#coding=utf-8import sysimport salt.clientdef push(): client = salt.client.LocalClient(__opts__[&apos;conf_file&apos;]) minions = client.cmd(&apos;*&apos;, &apos;test.ping&apos;, timeout=1) for minion in sorted(minions): print minion 调用sudo salt-run public.push","link":"/dev/saltstack-runners/"},{"title":"saltstack-安装","text":"导入saltstack PPA keyUbuntu下最新版本的包发布在saltstack PPA。如果你有 add-apt-repository 工具，你可以一键添加软件源仓库并导入PPA的key。1sudo add-apt-repository ppa:saltstack/salt 如果提示以下错误1add-apt-repository: command not found? 需要安装下面两个东西12sudo apt-get install python-software-propertiessudo apt-get install software-properties-common 添加源和key12echo deb http://ppa.launchpad.net/saltstack/salt/ubuntu `lsb_release -sc` main | sudo tee /etc/apt/sources.list.d/saltstack.listwget -q -O- &quot;http://keyserver.ubuntu.com:11371/pks/lookup?op=get&amp;search=0x4759FA960E27C0A6&quot; | sudo apt-key add - 更新sudo apt-get update 安装软件包安装使用apt-get命令从源仓库安装Salt master, minoin, 或者syndic。 12345sudo apt-get install salt-mastersudo apt-get install salt-minionsudo apt-get install salt-syndic minion配置12master: 192.168.1.10 # master 的ip地址id: web-01 #当前 minion 机器的名字 重启minion1# sudo service salt-minion restart master配置1interface: 192.168.1.10 重启master1# sudo service salt-master restart 注意安装的时候要注意安装的 master 和 minion 端版本要一致 key 管理12345salt-key -L #查看所有key的状态salt-key -a keyname #接受名字为keyname的机器salt-key -A #批量接受salt-key -d keyname #删除名字为keyname的机器salt-key -D #批量删除 参考http://docs.saltstack.cn/zh_CN/latest/ref/cli/salt-key.html","link":"/dev/saltstack-安装/"},{"title":"saltstack-自定义模块","text":"创建 modules 目录1mkdir /srv/salt/_modules &amp;&amp; cd /srv/salt/_modules 在 _modules 目录创建一个模块 test.py 代码如下12def foo(): return &apos;foo&apos; 同步到客户端salt ‘*’ state.highstate salt ‘*’ saltutil.sync_returners salt ‘*’ saltutil.sync_all 执行模块salt ‘*’ test.foo","link":"/dev/saltstack-自定义模块/"},{"title":"搜狗词库转换为txt文件","text":"在solr分词的时候需要一些词库,在搜狗细胞词库可以下载到.scel的词库.但是需要转成成我所需要的.在网络找到一份python的处理代码.可以将词库提取出来.稍微修改了一下main里读取目录文件的部分和输出格式.就得到了我所需要的词库文件.代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190#!/usr/bin/python# -*- coding: utf-8 -*-import structimport sysimport binasciiimport pdbimport os#搜狗的scel词库就是保存的文本的unicode编码，每两个字节一个字符（中文汉字或者英文字母）#找出其每部分的偏移位置即可#主要两部分#1.全局拼音表，貌似是所有的拼音组合，字典序# 格式为(index,len,pinyin)的列表# index: 两个字节的整数 代表这个拼音的索引# len: 两个字节的整数 拼音的字节长度# pinyin: 当前的拼音，每个字符两个字节，总长len##2.汉语词组表# 格式为(same,py_table_len,py_table,{word_len,word,ext_len,ext})的一个列表# same: 两个字节 整数 同音词数量# py_table_len: 两个字节 整数# py_table: 整数列表，每个整数两个字节,每个整数代表一个拼音的索引## word_len:两个字节 整数 代表中文词组字节数长度# word: 中文词组,每个中文汉字两个字节，总长度word_len# ext_len: 两个字节 整数 代表扩展信息的长度，好像都是10# ext: 扩展信息 前两个字节是一个整数(不知道是不是词频) 后八个字节全是0## {word_len,word,ext_len,ext} 一共重复same次 同音词 相同拼音表#拼音表偏移，startPy = 0x1540;#汉语词组表偏移startChinese = 0x2628;#全局拼音表GPy_Table ={}#解析结果#元组(词频,拼音,中文词组)的列表GTable = []def byte2str(data): &apos;&apos;&apos;将原始字节码转为字符串&apos;&apos;&apos; i = 0; length = len(data) ret = u&apos;&apos; while i &lt; length: x = data[i] + data[i+1] t = unichr(struct.unpack(&apos;H&apos;,x)[0]) if t == u&apos;\\r&apos;: ret += u&apos;\\n&apos; elif t != u&apos; &apos;: ret += t i += 2 return ret#获取拼音表def getPyTable(data): if data[0:4] != &quot;\\x9D\\x01\\x00\\x00&quot;: return None data = data[4:] pos = 0 length = len(data) while pos &lt; length: index = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #print index, pos += 2 l = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #print l, pos += 2 py = byte2str(data[pos:pos+l]) #print py GPy_Table[index]=py pos += l#获取一个词组的拼音def getWordPy(data): pos = 0 length = len(data) ret = u&apos;&apos; while pos &lt; length: index = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] ret += GPy_Table[index] pos += 2 return ret#获取一个词组def getWord(data): pos = 0 length = len(data) ret = u&apos;&apos; while pos &lt; length: index = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] ret += GPy_Table[index] pos += 2 return ret#读取中文表def getChinese(data): #import pdb #pdb.set_trace() pos = 0 length = len(data) while pos &lt; length: #同音词数量 same = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #print &apos;[same]:&apos;,same, #拼音索引表长度 pos += 2 py_table_len = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #拼音索引表 pos += 2 py = getWordPy(data[pos: pos+py_table_len]) #中文词组 pos += py_table_len for i in xrange(same): #中文词组长度 c_len = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #中文词组 pos += 2 word = byte2str(data[pos: pos + c_len]) #扩展数据长度 pos += c_len ext_len = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #词频 pos += 2 count = struct.unpack(&apos;H&apos;,data[pos]+data[pos+1])[0] #保存 GTable.append((count,py,word)) #到下个词的偏移位置 pos += ext_lendef deal(file_name): print &apos;-&apos;*60 f = open(file_name,&apos;rb&apos;) data = f.read() f.close() if data[0:12] !=&quot;\\x40\\x15\\x00\\x00\\x44\\x43\\x53\\x01\\x01\\x00\\x00\\x00&quot;: print &quot;确认你选择的是搜狗(.scel)词库?&quot; sys.exit(0) #pdb.set_trace() print &quot;词库名：&quot; ,byte2str(data[0x130:0x338])#.encode(&apos;GB18030&apos;) print &quot;词库类型：&quot; ,byte2str(data[0x338:0x540])#.encode(&apos;GB18030&apos;) print &quot;描述信息：&quot; ,byte2str(data[0x540:0xd40])#.encode(&apos;GB18030&apos;) print &quot;词库示例：&quot;,byte2str(data[0xd40:startPy])#.encode(&apos;GB18030&apos;) getPyTable(data[startPy:startChinese]) getChinese(data[startChinese:])if __name__ == &apos;__main__&apos;: #将要转换的词库添加在这里就可以了 #o = [&apos;1.scel&apos;, &apos;2.scel&apos; ] o=os.listdir(&apos;D:\\work\\pythonwork&apos;) #scel文件夹目录 for f in o: name, ext = os.path.splitext(f) if ext==&apos;.scel&apos;: deal(f) # for f in o: # deal(f) #保存结果 f = open(&apos;sougou.txt&apos;,&apos;w&apos;) for count,py,word in GTable: #GTable保存着结果，是一个列表，每个元素是一个元组(词频,拼音,中文词组)，有需要的话可以保存成自己需要个格式 #我没排序，所以结果是按照上面输入文件的顺序 #f.write( unicode(&apos;{%(count)s}&apos; %{&apos;count&apos;:count}+py+&apos; &apos;+ word).encode(&apos;utf-8&apos;) )#最终保存文件的编码，可以自给改 f.write( unicode(word).encode(&apos;utf-8&apos;) )#最终保存文件的编码，可以自给改 f.write(&apos;\\n&apos;) f.close()","link":"/dev/sogou-words-deal-python-code/"},{"title":"solr中数据导入中多值处理","text":"在数据索引的时候会碰到一个field有多个值的情况,在field的属性中提供了一个multiValued=&quot;true&quot;的属性.可以做多值索引. 做多值索引可以使用copyField或者是直接使用dataimport.这两种情况可以解决遇到的大多数多值索引的需求. ###使用copyField做多值索引 设置一个field的multiValued值为true. 并设置copyField值到这个field.这个field将以多值来存储.schema.xml文件配置如下 123&lt;field name=&quot;text&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&gt;&lt;copyField source=&quot;filename&quot; dest=&quot;text&quot;/&gt;&lt;copyField source=&quot;ext&quot; dest=&quot;text&quot;/&gt; 索引后搜索结果如下,其中text的值为filename和ext两个值组成的多值索引. 1234567891011121314&quot;docs&quot;: [ { &quot;ext&quot;: &quot;docx&quot;, &quot;text&quot;: [ &quot;docx&quot;, &quot;??????????100?????.docx&quot; ], &quot;filename&quot;: &quot;??????????100?????.docx&quot;, &quot;count&quot;: 1, &quot;id&quot;: &quot;12&quot;, &quot;url&quot;: &quot;file/61777234d3d949f2a95b61035088fdd4.docx&quot;, &quot;_version_&quot;: 1488260579550298000 } ] ##dataimport多值索引 dataimport方式是很实用的多值索引方式,比如主附表需要索引,主表一条记录对应了附表多条记录.就可以使用这种方式来索引. 我有file和filelist两个表,其中filelist的fid是file的外键. 1234567891011121314151617CREATE TABLE `file` ( `id` int(11) NOT NULL AUTO_INCREMENT, `filename` varchar(255) DEFAULT NULL, `ext` varchar(20) DEFAULT NULL, `count` int(11) DEFAULT NULL, `url` varchar(255) DEFAULT NULL, `is_exc` int(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=latin1;CREATE TABLE `filelist` ( `id` int(11) NOT NULL AUTO_INCREMENT, `fid` int(11) DEFAULT NULL, `url` varchar(255) DEFAULT NULL, `pagenum` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=latin1; schema.xml配置中添加一条urllist的field,multiValued=”true”.1&lt;field name=&quot;urllist&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&gt; data-config.xml中配置urllist的实体,将查询的url绑定到urllist.1234&lt;entity name =&quot;filelist&quot; query=&quot;select fid ,url from filelist where fid =&apos;${file.id}&apos; order by pagenum asc &quot;&gt; &lt;field column =&quot;url&quot; name=&quot;urllist&quot; /&gt; &lt;/entity&gt; 最后查询出来的urllist的结果为sql查询的多值. 123456&quot;urllist&quot;: [ &quot;179/daf/5205d53046f6089c50b2d43509775f/0.jpg&quot;, &quot;179/daf/5205d53046f6089c50b2d43509775f/1.jpg&quot;, &quot;179/daf/5205d53046f6089c50b2d43509775f/2.jpg&quot;, &quot;179/daf/5205d53046f6089c50b2d43509775f/3.jpg&quot; ], ###全部配置 #####schema.xml 123456789101112&lt;field name=&quot;id&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; required=&quot;true&quot; multiValued=&quot;false&quot; /&gt;&lt;field name=&quot;filename&quot; type=&quot;text_question&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;ext&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;count&quot; type=&quot;int&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;url&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;urllist&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&gt;&lt;field name=&quot;text&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot; /&gt;&lt;uniqueKey&gt;id&lt;/uniqueKey&gt;&lt;copyField source=&quot;filename&quot; dest=&quot;text&quot;/&gt;&lt;copyField source=&quot;ext&quot; dest=&quot;text&quot;/&gt; #####data-config.xml 1234567891011121314151617 &lt;document name=&quot;filetest&quot; dataSource=&quot;test&quot;&gt; &lt;entity name=&apos;file&apos; query=&quot;select id,filename,ext,count,url from file &quot; &gt; &lt;field column=&quot;id&quot; name=&quot;id&quot; /&gt; &lt;field column=&quot;filename&quot; name=&quot;filename&quot; /&gt; &lt;field column=&quot;ext&quot; name=&quot;ext&quot; /&gt; &lt;field column=&quot;count&quot; name=&quot;count&quot; /&gt; &lt;field column=&quot;url&quot; name=&quot;url&quot; /&gt; &lt;entity name =&quot;filelist&quot; query=&quot;select fid ,url from filelist where fid =&apos;${file.id}&apos; order by pagenum asc &quot;&gt; &lt;field column =&quot;url&quot; name=&quot;urllist&quot; /&gt; &lt;/entity&gt; &lt;/entity&gt;&lt;/document&gt; ###后记在配置多值索引的时候,由于配置的时候有个地方配置错了,索引文件出不来,在stackoverflow上查询到使用group_concat做查询连接,然后使用’splitBy = “,”‘做分割把我带到坑了.虽然逻辑上没有问题.但是却不符合常规的思维方式,只能说是奇淫技巧.后面好好检查了两个配置.重新做索引得到了想要的结果.","link":"/dev/solr-dataimporthandler-fields-multivalued/"},{"title":"solr多核配置","text":"假设已经配置好了一个单core的solr服务器. ###solr.xml配置文件单核和多核主要在solr.xml配置不同.在solr/example中已经有一个名称为multicore的文件夹里面给我们配置好了一个两个核心的配置,分别是core1和core2.但是我只是想在我已经配置好的solr服务器上添加一个核心.所以需要拷贝solr.xml配置到tomcat/solr文件夹1/solr-4.10.1/example/multicore$ sudo cp solr.xml /usr/local/tomcat/solr/ 在solr.xml里面和单核心不同在于节点cores下面,1234&lt;core name=&quot;core0&quot; instanceDir=&quot;core0&quot; /&gt;&lt;core name=&quot;core1&quot; instanceDir=&quot;core1&quot; /&gt;.....&lt;core name=&quot;coren&quot; instanceDir=&quot;coren&quot;/&gt; name为core名称,instanceDir为core的目录.将名称和目录修改成对应的名称和目录即可.名字和目录名称可以使随意的. ###创建core1的目录 比如我第一个core的名字和目录都为collection1,第二个打算为core1.那么solr.xml中的配置就是12&lt;core name=&quot;collection1&quot; instanceDir=&quot;collection1&quot; /&gt;&lt;core name=&quot;core1&quot; instanceDir=&quot;core1&quot; /&gt; 这里我们需要一个core1的目录,每个core目录里面都必须包含一个data目录,用来存储索引文件.包括一个conf目录,用来存储配置文件. conf目录里面需要包括schema.xml和solrconfig.xml两个基本的配置文件.conf目录可以直接拷贝一个基本的配置来自己修改. 进入/tomcat/solr目录,执行以下命令 1234sudo mkdir -p core1/datacd core1sudo mkdir confsudo cp -rf ../collection1/conf/* conf/ #这里我直接拷贝的collection1的配置 ###在gui界面添加 点击Core Admin-&gt;Add Core,在new_core和instanceDir里填写core1,其他不变,点击按钮Add Core,加载后,就可以再界面上管理添加的core了. 在回到刚刚创建的core1目录.这里已经创建了一个core.properties的文件.内容为: 123456#Written by CorePropertiesLocator#Tue Dec 02 06:21:36 UTC 2014name=core1config=solrconfig.xmlschema=schema.xmldataDir=data 重启tomcat.如果发现一下错误.是由于solr.xml文件中没有添加指定的core节点 There exists no core with name “core1” 关于solr多核的详情wikiQuick Review: What are Multiple Cores?","link":"/dev/solr-multiple-cores-config/"},{"title":"solr集成mmseg4j分词","text":"mmseg4jhttps://code.google.com/p/mmseg4j/ https://github.com/chenlb/mmseg4j-solr mmseg4j 用 Chih-Hao Tsai 的 MMSeg 算法(http://technology.chtsai.org/mmseg/ )实现的中文分词器，并实现 lucene 的 analyzer 和 solr 的TokenizerFactory 以方便在Lucene和Solr中使用。 MMSeg 算法有两种分词方法：Simple和Complex，都是基于正向最大匹配。Complex 加了四个规则过虑。官方说：词语的正确识别率达到了 98.41%。mmseg4j 已经实现了这两种分词算法。 mmseg4j-solr2.2.0里面有两个jar包，分别是mmseg4j-core-1.10.0.jar，mmseg4j-solr-2.2.0.jar.将两个jar拷贝到/WEB-INF/lib里面。 配置12345&lt;fieldType name=&quot;text_mmseg&quot; class=&quot;solr.TextField&quot; positionIncrementGap=&quot;100&quot;&gt; &lt;analyzer&gt; &lt;tokenizer class=&quot;com.chenlb.mmseg4j.solr.MMSegTokenizerFactory&quot; mode=&quot;complex&quot; dicPath=&quot;dic&quot;/&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 这时候在定义field是就可以使用text_mmseg的fieldType。 tokenizer参数mmseg4j 在 solr 中主要支持两个参数：mode、dicPath。mode 表示是什么模式分词（有效值：simplex、complex、max-word，如果输入了无效的默认用 max-word。）。dicPath 是词库目录可以是绝对目录，也可以是相对目录（是相对 solr.home 目录下的，dic 就会在 solr.home/dic 目录下找词库文件），如果不指定就是默认在 CWD/data 目录（程序运行当前目录的data子目录）下找。 这个地方所说的solr.home我理解的时候，总是觉得是tomcat/solr 这个目录。也就是solr核心的根目录，测试了很久，这里所说的solr.home是指定core的目录，默认也就是/tomcat/solr/collection1这个目录。 dicPath支持相对路径和绝对路径，上面配置的dic,所以需要在/tomcat/solr/这个目录创建一个名为dic的目录。然后将词库文件放到这个目录下。并且词库文件名必须以words开头.dic结尾。词库强制使用utf-8.由于 utf-8 文件有带与不带 BOM 之分，建议词库第一行为空行或为无 BOM 格式的 utf-8 文件。 搜狗词库http://www.sogou.com/labs/dl/r.html 需要转换才能给mmseg4j来使用","link":"/dev/solr-集成mmseg4j分词/"},{"title":"solr导入mysql数据内存不足","text":"在solr使用dataImport时,在测试机上由于内存太小,创建索引时不成功. ###开启batchSize 在data-config.xml文件的dataSource中加入batchSize=”-1”的配置.参考http://wiki.apache.org/solr/DataImportHandlerFaq I’m using DataImportHandler with a MySQL database. My table is huge and DataImportHandler is going out of memory. Why does DataImportHandler bring everything to memory? DataImportHandler is designed to stream row one-by-one. It passes a fetch size value (default: 500) to Statement#setFetchSize which some drivers do not honor. For MySQL, add batchSize property to dataSource configuration with value -1. This will pass Integer.MIN_VALUE to the driver as the fetch size and keep it from going out of memory for large tables. Should look like:1&lt;dataSource type=&quot;JdbcDataSource&quot; name=&quot;ds-2&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost:8889/mysqldatabase&quot; batchSize=&quot;-1&quot; user=&quot;root&quot; password=&quot;root&quot;/&gt; ###分批处理 在data-config.xml的query节点使用limit来分批处理数据,比如1query=&quot;select * from tb_content limit ${dataimporter.request.begin},50000 &quot; 使用了一个begin的参数来每次分批处理50000条记录. 然后访问的连接参数,第一次clean=true清理旧的索引,后面的clean=false不清理索引.访问连接如下1234http://localhost:8080/solr/core1/dataimport?wt=json&amp;commit=true&amp;clean=true&amp;command=full-import&amp;begin=0http://localhost:8080/solr/core1/dataimport?wt=json&amp;commit=true&amp;clean=false&amp;command=full-import&amp;begin=50000http://localhost:8080/solr/core1/dataimport?wt=json&amp;commit=true&amp;clean=false&amp;command=full-import&amp;begin=100000.... 分批建立索引php脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?php/** * 用于分批创建索引 */set_time_limit(0);$http_url=&quot;http://localhost:8080/solr/core1/dataimport&quot;;$arr = array();$arr[&apos;wt&apos;] =&apos;json&apos;;$arr[&apos;commit&apos;] =&apos;true&apos;;$arr[&apos;clean&apos;] =&apos;true&apos;;$arr[&apos;command&apos;] =&apos;full-import&apos;;$arr[&apos;begin&apos;] =0;$url = $http_url.&apos;?&apos;.http_build_query($arr);file_get_contents($url);echo $url;$arr[&apos;clean&apos;] = &apos;false&apos;;while( 1 ) { $url = $http_url.&apos;?&apos;.&apos;command=status&amp;wt=json&apos;; $content = file_get_contents($url); $content = json_decode($content); if( $content-&gt;status == &apos;busy&apos; ){//执行 var_dump( http_build_query( $arr ) ); var_dump( $content-&gt;statusMessages ); } if( $content-&gt;status == &apos;idle&apos; ){//完成 $arr[&apos;begin&apos;] = intval($arr[&apos;begin&apos;]) + 50000; if( $arr[&apos;begin&apos;] &gt; 500000 ){ break; } $url = $http_url.&apos;?&apos;.http_build_query( $arr ); file_get_contents( $url ); echo $url ; sleep( 5 ); } sleep( 1 );}","link":"/dev/solr-mysql-import-out-of-memory/"},{"title":"solrphp and solr searcher使用","text":"前面我配置好了solr，并且数据库建立索引也完成了。 为php添加搜索首先下载solrphp http://wiki.apache.org/solr/SolPHP 在solrphp里面包括了一个/Apache/solr的文件夹。将solr这个文件夹拷贝到项目中并引用。1require_once(&apos;Solr/Service.php&apos;); 现在可以开始使用进行搜索了,一下三段代码建立个一个简单的搜索。 12345require_once(&apos;Solr/Service.php&apos;);$solr= new Apache_Solr_Service(&apos;192.168.10.11&apos;,&apos;8080&apos;,&apos;solr/&apos;);$query= $solr-&gt;search($_GET[&apos;q&apos;], 0, 10); //查询q Apache_Solr_Service实例化了一个连接到solr的服务，$solr-&gt;search表示查询传入的参数q，并查询0开始的10条数据 。$query是一个Apache_Solr_Response对象，这个对象是Solr数据返回的对象。主要包括了5个函数 。 123456789public function getHttpStatus()public function getHttpStatusMessage()public function getType()public function getEncoding()public function getRawResponse() 使用上面的函数来获取需要的数据 12345678910111213141516171819if ($query-&gt;getHttpStatus()==200){ $raw=$query-&gt;getRawResponse(); $rawobj=json_decode($raw); $response=$rawobj-&gt;response ; echo &quot;All:&quot;.$response-&gt;numFound.&quot;&lt;/br&gt;&quot;; echo &quot;start:&quot;.$response-&gt;start.&quot;&lt;/br&gt;&quot;; foreach ($response-&gt;docs as $value) { //$value为在solr的schema.xml文件里配置的 field ...... }} Lucene中的IndexSearcher在Lucene中搜索最终是调用了IndexSearcher的search方法，同时传入了一个Query的实例。其中Lucene内置的Query类型包括了一下几个： TermQuery 通过项进行搜索 比如返回域content里包含hello的文档 1Query query=new TermQuery(new Term(&quot;content&quot;,&quot;hello&quot;)); TermRangeQuery 指定范围搜索 比如搜索title里面从a到d范围内，包含a不包含d的文档 1Query query=new TermRangeQuery(&quot;title&quot;,&quot;a&quot;,&quot;d&quot;,ture,false); NumericRangeQuery 指定数字范围 搜索201401到201405范围内的文档 1Query query=NumericRangeQuery.newIntRange(&quot;month&quot;,&quot;201401&quot;,&quot;201405&quot;,ture,ture); PrefixQuery 通过字符串搜索 搜索content中以hello开头的文档 1Query query=new PrefixQuery(new Term(&quot;content&quot;,&quot;hello&quot;)); BooleanQuery 组合搜索 1234BooleanQuery query=new BooleanQuery();query.add(iquery,BooleanQuery.Occur.MUST);query.add(...);...... 其中BooleanQuery的add方法传入的一个Query对象和一个Occur的枚举。Occur枚举包括了MUST(and),SHOULD(no),MUST_NOT(not).来和query之间做逻辑的组合。 PhraseQuery 短语搜索，用来查询 WildcardQuery 通配符查询 FuzzQuery 类似项搜索。使用Levenshtein算法，算法详情可以查看 http://www.cnblogs.com/ac1985482/p/Levenshtein.html MatchAllDocsQuery 匹配所有文档 解析表达式当使用Lucene来做开发的时候我们可以使用以上的对象做，当使用solr的时候，就不能使用上面的对象来做了，这个时候解析表达式就发挥作用了 检索运算符 “:” 指定字段查指定值，如返回所有值: “?” 表示单个任意字符的通配 “” 表示多个任意字符的通配（不能在检索的项开始使用或者?符号） “~” 表示模糊检索，如检索拼写类似于”roam”的项这样写：roam~将找到形如foam和roams的单词；roam~0.8，检索返回相似度在0.8以上的记录。 邻近检索，如检索相隔10个单词的”apache”和”jakarta”，”jakarta apache”~10“” 控制相关度检索，如检索jakarta apache，同时希望去让”jakarta”的相关度更加好，那么在其后加上””符号和增量值，即jakarta4 apache 布尔操作符AND、|| 布尔操作符OR、&amp;&amp; 布尔操作符NOT、!、-（排除操作符不能单独与项使用构成查询） “+” 存在操作符，要求符号”+”后的项必须在文档相应的域中存在 () 用于构成子查询 [] 包含范围检索，如检索某时间段记录，包含头尾，date:[200707 TO 200710] {}不包含范围检索，如检索某时间段记录，不包含头尾，date:{200707 TO 200710} “ 转义操作符，特殊字符包括+ - &amp;&amp; || ! ( ) { } [ ] ^ “ ~ * ? : “","link":"/dev/solr_searcher/"},{"title":"supervisor-python进程管理","text":"supervisor官网http://supervisord.org文档 http://supervisord.org/installing.html 安装12# 使用 apt-get 会自动的在 /etc/supervisor 加入配置文件，推荐使用apt-get install supervisor #pip install supervisor 安装好后有两个可执行文件和一些配置文件123/usr/bin/supervisord -- supervisor服务守护进程/usr/bin/supervisorctl -- supervisor服务控制程序，比如：status/start/stop/restart xx 等/etc/supervisor/supervisord.conf -- 配置文件，定义服务名称以及接口等等 配置示例，在/etc/supervisor/supervisord.conf 指定一个 getfile 的程序123456[program:getfile]command=python /home/zhangwei/ptest_service/app_getfile.pydirectory=/home/zhangwei/ptest_serviceautorstart=truestdout_logfile=/home/zhangwei/ptest_service/log/getfile.logstderr_logfile=/home/zhangwei/ptest_service/log/getfile_err.log 管理12supervisorctl status # 查看进程状态supervisorctl status|stop|start getfile #查看 停止 启动进程 supervisord.conf文件里会发现有一段[unix_http_server]的配置，默认是9001端口，配置后，重启supervisor服务，打开浏览器输入：http://localhost:9001 可以查看相关的内容。","link":"/dev/supervisor-python进程管理/"},{"title":"tesseract配置","text":"Tesseract开源的OCR引擎,使用 Apache 2.0 license授权协议,可以直接使用或者使用API开发.并且支持多语言. 安装install lib12345678sudo apt-get install autoconf automake libtoolsudo apt-get install libpng12-devsudo apt-get install libjpeg62-devsudo apt-get install libtiff4-devsudo apt-get install zlib1g-devsudo apt-get install libicu-dev # (if you plan to make the training tools)sudo apt-get install libpango1.0-dev # (if you plan to make the training tools)sudo apt-get install libcairo2-dev # (if you plan to make the training tools) install leptonicahttp://www.leptonica.org/12345./configure --prefix=/usr/local/leptonicamakemake installsudo ldconfig install tesseract1234./autogen.sh./configure --prefix=/usr/local/tesseract --with-extra-libraries=/usr/local/leptonica/libmakemake install chinese language data将语言包解压后复制到share/tessdata12345678vagrant@aegir:~/tesseract$ sudo tar zxvf chi-tesseract-ocr.tar.gztesseract-ocr/tessdata/chi_sim.traineddatacd tesseract-ocr/tessdatasudo mv chi_sim.traineddata /usr/local/tesseract/share/tessdata/export TESSDATA_PREFIX=/usr/local/tesseract/share# if your tessdata path is &apos;/usr/local/share/tessdata&apos; you have to use &apos;export TESSDATA_PREFIX=&apos;/usr/local/share/ 命令行1./tesseract imagename outputbase [-l lang] [-psm pagesegmode] [configfile...] 1./tesseract /vagrant/image/7.png 1.txt -l chi_sim https://code.google.com/p/tesseract-ocr/wiki/ReadMe https://code.google.com/p/tesseract-ocr/wiki/Compiling","link":"/dev/tesseract配置/"},{"title":"thrift基础以及在c#中的使用","text":"###thrift是什么Apache Thrift 是 Facebook 实现的一种高效的、支持多种编程语言的远程服务调用的框架.它采用接口描述语言定义并创建服务，支持可扩展的跨语言服务开发，所包含的代码生成引擎可以在多种语言中，如 C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk 等创建高效的、无缝的服务，其传输数据采用二进制格式，相对 XML 和 JSON 体积更小，对于高并发、大数据量和多语言的环境更有优势. 下载wget http://apache.fayea.com/thrift/0.9.2/thrift-0.9.2.tar.gz tar -zxvf thrift-0.9.2.tar.gz 编译1234./configuremakemake install http://thrift.apache.org/docs/BuildingFromSource 基础类型 bool 布尔型 byte 比特 i16 16位整形 i32 32位整形 i64 64位整形 double 64位浮点数 binary 二进制 struct 结构体 list / set /map 容器 exception 异常 service 服务类 ###协议hrift 可以让用户选择客户端与服务端之间传输通信协议的类别，在传输协议上总体划分为文本 (text) 和二进制 (binary) 传输协议，为节约带宽，提高传输效率，一般情况下使用二进制类型的传输协议为多数，有时还会使用基于文本类型的协议，这需要根据项目 / 产品中的实际需求。常用协议有以下几种 TBinaryProtocol —— 二进制编码格式进行数据传输 TCompactProtocol —— 高效率的、密集的二进制编码格式进行数据传输 TJSONProtocol —— 使用 JSON 的数据编码协议进行数据传输 TSimpleJSONProtocol —— 只提供 JSON 只写的协议，适用于通过脚本语言解析 ###传输层 TSocket —— 使用阻塞式 I/O 进行传输，是最常见的模式 TFramedTransport —— 使用非阻塞方式，按块的大小进行传输 TNonblockingTransport —— 使用非阻塞方式，用于构建异步客户端 ###服务端类型 TSimpleServer —— 单线程服务器端使用标准的阻塞式 I/O TThreadPoolServer —— 多线程服务器端使用标准的阻塞式 I/O TNonblockingServer —— 多线程服务器端使用非阻塞式 I/O 生成thrift -r –gen csharp tutorial.thrift 基础教程http://thrift.apache.org/tutorial/ ###.net示例 ####创建.thrift清单文件 当thrift安装好后,创建一个test.thrift文件,内容如下 12345678struct SharedStruct { 1: i32 key 2: string value}service SharedService { SharedStruct getStruct(1: i32 key)} ####生成文件 运行下面的命令生成.net的库,将得到两个文件SharedService.cs ShardStruct.cs.1thrift -r --gen csharp test.thrift ####服务端 创建一个测试的工程,并引入Thrift.dll.位置thrift-0.9.2/lib/csharp.并将生成的两个文件添加到工程中. 首先需要实现一个SharedService.Iface的接口,里面实现了一个getStruct的方法,用来给客户端返回数据.123456789101112131415161718192021public class SharedServiceTest : SharedService.Iface { public SharedStruct getStruct(int key) { if (key &lt; 10) { return new SharedStruct { Key = key, Value = &quot;小于10&quot; }; } if (key &gt;= 10 &amp;&amp; key &lt; 100) { return new SharedStruct { Key = key, Value = &quot;10到100之间&quot; }; } if (key &gt;= 100) { return new SharedStruct { Key = key, Value = &quot;100及以上&quot; }; } return null; } } 服务端启动代码,监听9090的端口123456789 SharedServiceTest service = new SharedServiceTest();SharedService.Processor processsor = new SharedService.Processor(service);TServerTransport serverTransport = new TServerSocket(9090);TServer server = new TSimpleServer(processsor, serverTransport);server.Serve(); ####客户端创建一个客户端工程,同服务端一样引入dll和两个生成的文件.其中test为一个输入值.1234567891011121314TTransport transport = new TSocket(&quot;10.8.64.109&quot;, 9090);TProtocol protocal = new TBinaryProtocol(transport);SharedService.Client client = new SharedService.Client(protocal);transport.Open();try{ SharedStruct ss = client.getStruct(test); Console.WriteLine(ss.Key + &quot;|&quot; + ss.Value);}finally{ transport.Close();}","link":"/dev/the-thrift-and-csharp/"},{"title":"mysql安装配置","text":"命令行安装1234567891011121314151617181920# Install the database packagessudo apt-get install -y mysql-server mysql-client libmysqlclient-dev# Ensure you have MySQL version 5.5.14 or latermysql --version&lt;!--more--&gt;# Pick a MySQL root password (can be anything), type it and press enter# Retype the MySQL root password and press enter# Secure your installationsudo mysql_secure_installation# Login to MySQLmysql -u root -p# Ensure you can use the InnoDB engine which is necessary to support long indexes# If this fails, check your MySQL config files (e.g. `/etc/mysql/*.cnf`, `/etc/mysql/conf.d/*`) for the setting &quot;innodb = off&quot;mysql&gt; SET storage_engine=INNODB; 添加用户设置权限123mysql&gt; create user username identified by ‘password&apos;; #创建用户usernamemysql&gt; show grants for username; #显示username权限mysql&gt; GRANT select,insert,update,delete,execute,create,drop ON *.* TO &apos;username&apos;@&apos;%&apos; IDENTIFIED BY PASSWORD &apos;*FF53393A7CDB3A4C6657A4E25092DA7DA41120BB’; #重新执行设置权限 mysql 权限详细分类1234567891011121314151617FILE: 在MySQL服务器上读写文件。PROCESS: 显示或杀死属于其它用户的服务线程。RELOAD: 重载访问控制表，刷新日志等。SHUTDOWN: 关闭MySQL服务。数据库/数据表/数据列权限：ALTER: 修改已存在的数据表(例如增加/删除列)和索引。CREATE: 建立新的数据库或数据表。DELETE: 删除表的记录。DROP: 删除数据表或数据库。INDEX: 建立或删除索引。INSERT: 增加表的记录。SELECT: 显示/搜索表的记录。UPDATE: 修改表中已存在的记录。特别的权限：ALL: 允许做任何事(和root一样)。USAGE: 只允许登录--其它什么也不允许做。 允许其他机器访问1vim /etc/mysql/my.cnf 修改 bind-address = 127.0.0.1 =&gt; 主机ip或者注释掉 然后在用户权限设置里修改为 ‘username’@‘%’","link":"/dev/ubuntu-aptget-install-mysql/"},{"title":"vagrant_init","text":"vagrant 基础操作下载安装 VirtualBox ：https://www.virtualbox.org/ 下载安装 Vagrant ：http://www.vagrantup.com/ 假设你的电脑上已经安装了 VirtualBox 和 Vagrant 。请务必先安装 VirtualBox，因为 Vagrant 是依赖它的。 添加 box如果你已经将 wifi.box 这个 vagrant 的包下载到了你的电脑上，我们需要将这个包导入到 Vagrant.这个包大小有 5.4 G。请保证你的硬盘空间足够大。为它保留出25G的空间（5.4G 缓存包+ 17G 左右的虚拟机） 1$vagrant box add openwrt wifi.box # vagrant box add &lt;name&gt; &lt;path&gt; 执行了以上的命令后，你的 vagrant 里面已经包括了这个包 12$ vagrant box listopenwrt (virtualbox, 0) # 这个就是我们刚才导入的这个包 开发环境进入到自己的开发目录,假设是本地硬盘上的 ~/openwrt_build 或者 D:\\openwrt_build1234567cd ~/openwrt_build # d:# cd openwrt_build # 进入指定目录vagrant init openwrt # 执行后会在当前目录创建一个 Vagrantfile 的文件vagrant up # 启动，第一次创建目录 。等待命令完成后可以看到 VirtualBox 中虚拟机的状态是启动状态vagrant ssh # 使用 ssh 登录到 虚拟机 网络配置Vagrant的网络有三种模式 1、较为常用是端口映射，就是将虚拟机中的端口映射到宿主机对应的端口直接使用 ，在Vagrantfile中配置：12config.vm.network :forwarded_port, guest: 80, host: 8080 guest: 80 表示虚拟机中的80端口， host: 8080 表示映射到宿主机的8080端口。 开启这个后，如果vagrant已经启动了，在命令行输入 vagrant reload 重启机器，就可以再宿主机伤使用 localhost:8080来访问虚拟机的localhost:80 。 2、如果需要自己自由的访问虚拟机，但是别人不需要访问虚拟机，可以使用private_network，并为虚拟机设置IP ，在Vagrantfile中配置： 1config.vm.network :private_network, ip: &quot;192.168.1.104&quot; 192.168.1.104 表示虚拟机的IP，多台虚拟机的话需要互相访问的话，设置在相同网段即可3、如果需要将虚拟机作为当前局域网中的一台计算机，由局域网进行DHCP，那么在Vagrantfile中配置：1config.vm.network :public_network 目录映射既然是开发环境，那么开发工作肯定还是需要在本地完成，而不是都要进到虚拟机中去完成，虚拟机就好好在后台运行服务就好了，不然就本末倒置了，所以这里就需要使用目录映射功能，将本地的目录映射到虚拟机的对应目录。 默认情况下，当前的工作目录，会被映射到虚拟机的 /vagrant 目录，当前目录下的文件可以直接在 /vagrant 下进行访问，当然也可以在通过 ln 创建软连接，如 ln -fs /vagrant/wwwroot /var/www 来进行目录映射，当然，从自动化配置的角度，能不进系统就不需要进系统，所以在Vagrant也可以进行目录映射的操作： config.vm.synced_folder “wwwroot/“, “/var/www” 前面的参数 “wwwroot/” 表示的是本地的路径，这里使用对于工作目录的相对路径，这里也可以使用绝对路径，比如： “d:/www/” 后面的参数 “/var/www” 表示虚拟机中对应映射的目录。 一些命令12345678vagrant up （启动虚拟机）vagrant halt （关闭虚拟机——对应就是关机）vagrant suspend （暂停虚拟机——只是暂停，虚拟机内存等信息将以状态文件的方式保存在本地，可以执行恢复操作后继续使用）vagrant resume （恢复虚拟机 —— 与前面的暂停相对应）vagrant destroy （删除虚拟机，删除后在当前虚拟机所做进行的除开Vagrantfile中的配置都不会保留）vagrant reload (重启)` 一些问题","link":"/dev/vagrant-init/"},{"title":"walle 安装体验记录","text":"githubhttps://github.com/meolu/walle-web walle 是一个代码部署工具，本来是最佳想用 python 写一个带 web 界面的代码部署工具。原因是前面用 shell 写的一个简单的部署有很多的问题。今天早上看到了这个，思路和我想要做的差不多，处理上线那块作者直接用 ssh。 由于我用的 saltstack在做管理,我是想用 saltstack 的接口来做部署. 依赖 bash 系统自带,迁出需要用到git，拷贝文件需要用到ssh php环境 ，我这里是 LNMP composer 1curl -sS http://install.phpcomposer.com/installer | sudo php -- --install-dir=/usr/local/bin --filename=composer 设置数据库连接 两个文件 config/web.php config/local.php12345678vi config/web.php +12&apos;db&apos; =&gt; [ &apos;class&apos; =&gt; &apos;yii\\db\\Connection&apos;, &apos;dsn&apos; =&gt; &apos;mysql:host=127.0.0.1;dbname=walle&apos;, # 新建数据库walle &apos;username&apos; =&gt; &apos;username&apos;, # 连接的用户名 &apos;password&apos; =&gt; &apos;password&apos;, # 连接的密码 &apos;charset&apos; =&gt; &apos;utf8&apos;,], 123456vi config/local.php +17&apos;db&apos; =&gt; [ &apos;dsn&apos; =&gt; &apos;mysql:host=127.0.0.1;dbname=walle&apos;, &apos;username&apos; =&gt; &apos;root&apos;, &apos;password&apos; =&gt; &apos;123456&apos;,#此处需要设置连接mysql密码,默认为空], 安装 vendor 这里用到了 composer12cd wallecomposer install --prefer-dist --no-dev --optimize-autoloader -vvvv 我这里这里生成了一个 bower-asset 的文件夹,修改成bower. 初始化 12cd walle./yii walle/setup # 需要你的yes 配置 nginx 123456789101112131415161718192021server { listen 80; server_name deploy.net; # 改你的host root /usr/local/www/walle/web; # 根目录为web index index.php; # 注意！！测试通过之后一定取消下面注释，设置访问内网 # allow 192.168.0.0/24; # deny all; location / { try_files $uri $uri/ /index.php$is_args$args; } location ~ \\.php$ { try_files $uri = 404; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; }} 访问 重启 php-fpm nginx12sudo service php5-fpm restartsudo service nginx restart 访问配置好的路径 安装错误1Error: The file or directory to be published does not exist: /usr/local/www/walle/vendor/bower/jquery/dist 真实路径是 bower-asset,修改下 1mv bower-asset bower 配置 为 php 运行账户生成一个key，在访问 gitlab 和上线都需要。我的 php-fpm 为www-data,如果www-data没有文件夹使用usermod或直接修改/etc/sudoer 1$ ssh-keygen -t rsa 在 gitlab 的 admin/deploy keys 里添加进去 总体来说，安装还是非常简单的，文档也比较全。","link":"/dev/walle-install/"},{"title":"zabbix监控pm2.5","text":"文件 pm2.5.py 用来获取pm值 zabbix-pm25.conf 配置 zabbix 的 UserParameter github: https://github.com/xxiu/pm2.5_zabbix pm2.5没有找到很好的api，于是找了个pm2.5的网站，用解析html的方式来获取当前的pm2.5的值 配置我的 zabbix 配置文件目录在 /usr/local/etc将 pm2.5.py 拷贝到/usr/local/etc/zabbix_agentd.conf.d将 zabbix-pm25.conf 拷贝到 /usr/local/etc/script重启 zabbix-agentd 可以在zabbix-server上测试一下配置是否成功1zabbix_get -s IP -p 10050 -k &apos;pm25[beijing]&apos; 触发器添加一个项目 pm25_beijing 键值为 pm25[beijing]添加一个图像来监控这个项目的走势添加3个触发器，表达式如下，用来监控pm2.5超过100，200，350的变化，过了350的感觉就生无可恋了。{pm25:pm25[beijing].last()}&gt;100{pm25:pm25[beijing].last()}&gt;200{pm25:pm25[beijing].last()}&gt;350 告警action 那里已经配置了全局的告警，就不需要配置了。","link":"/dev/zabbix-pm2-5/"},{"title":"在osx el capitan 中安装 wxpython","text":"昨天在 windows 上安装了wxpython,然后写了个 hello world 出来，想着在 mac 上看看效果，结果安装一直都不成功，用安装包安装提示找不到安装目标，用 brew 安装引用不成功。找到下面的解决方法 Since OS X 10.11 (El Capitan), the lastest version of the available wxPython dmg (3.0.2.0) have an unsupported pkg structure. You can see an open bug at http://trac.wxwidgets.org/ticket/17203 Thanks to memoselyk@stackoverflow we can convert manually the package structure to be able to install it : http://stackoverflow.com/questions/34402303/install-wxpython-in-osx-10-11/34622956#34622956 ; you can find an updated version of the method below : Installing wxPython-3.0.2.0 on OS X El Capitan :123456789101112131415161718192021222324252627282930313233343536373839404142434445# base workdirmkdir ~/wxpython_elcapitancd ~/wxpython_elcapitan# download the wxPython dmgcurl -L &quot;http://downloads.sourceforge.net/project/wxpython/wxPython/3.0.2.0/wxPython3.0-osx-3.0.2.0-cocoa-py2.7.dmg?r=http%3A%2F%2Fwww.wxpython.org%2Fdownload.php&amp;ts=1453708927&amp;use_mirror=netix&quot; -o wxPython3.0-osx-3.0.2.0-cocoa-py2.7.dmg# mount the dmghdiutil attach wxPython3.0-osx-3.0.2.0-cocoa-py2.7.dmg# copy the dmg package to the local diskmkdir ~/wxpython_elcapitan/repack_wxpythoncd ~/wxpython_elcapitan/repack_wxpythoncp -r /Volumes/wxPython3.0-osx-3.0.2.0-cocoa-py2.7/wxPython3.0-osx-cocoa-py2.7.pkg .# unmount the dmgdmgdisk=&quot;$(hdiutil info | grep &apos;/Volumes/wxPython3.0-osx-3.0.2.0-cocoa-py2.7&apos; | awk &apos;{ print $1; }&apos;)&quot;hdiutil detach ${dmgdisk}# prepare the new package contentsmkdir ~/wxpython_elcapitan/repack_wxpython/pkg_rootcd ~/wxpython_elcapitan/repack_wxpython/pkg_rootpax -f ../wxPython3.0-osx-cocoa-py2.7.pkg/Contents/Resources/wxPython3.0-osx-cocoa-py2.7.pax.gz -z -rcd ~/wxpython_elcapitan/repack_wxpython# prepare the new package scriptsmkdir ~/wxpython_elcapitan/repack_wxpython/scriptscp wxPython3.0-osx-cocoa-py2.7.pkg/Contents/Resources/preflight scripts/preinstallcp wxPython3.0-osx-cocoa-py2.7.pkg/Contents/Resources/postflight scripts/postinstall# delete the old packagerm -rf ~/wxpython_elcapitan/repack_wxpython/wxPython3.0-osx-cocoa-py2.7.pkg# build the new one :pkgbuild --root ./pkg_root --scripts ./scripts --identifier com.wxwidgets.wxpython wxPython3.0-osx-cocoa-py2.7.pkg# put the package on Desktop, and clean workdirmv ~/wxpython_elcapitan/repack_wxpython/wxPython3.0-osx-cocoa-py2.7.pkg ~/Desktop/cd ~rm -rf ~/wxpython_elcapitan# install it ! it will ask for your password (to become superuser/root)sudo installer -pkg ~/Desktop/wxPython3.0-osx-cocoa-py2.7.pkg -target /# EOF wxPython is now available on your OS X El Capitan :12$ python -c &apos;import wx;print wx.version()&apos;3.0.2.0 osx-cocoa (classic) 来源http://davixx.fr/blog/2016/01/25/wxpython-on-os-x-el-capitan/","link":"/dev/wxpython-on-os-x-el-capitan-install/"},{"title":"使用acme.sh配置letsencrypt证书","text":"越来越多的人开始意识到 https 的重要性了，letsencrypt 证书免费而且兼容性好。在一般情况下都足够用。如果需要更高级别的证书可以考虑付费的。 首先，我这里的需求是前端有两台 web 服务器，前端有一台作为均衡负载。所以常规的使用域名作为验证的方法是不可行的，在看 acme.sh 时发现了它支持使用 DNS 作为验证。满足了我的要求。 安装1curl https://get.acme.sh | sh 安装完后 acme.sh 会被安装在~/.acme.sh 的目录下面. dns认证在安装目录的 dnsapi 目录下面 README.md 有认证方式,我使用了 dnspod 作为 dns 服务。在 dnspod 控制台申请到 id 和key后，在当前目录执行： 12345export DP_Id=&quot;1111&quot;export DP_Key=&quot;111111111111111&quot;acme.sh --issue --dns dns_dp -d aa.com -d www.aa.comacme.sh --issue --dns dns_dp -d aaa.net -d *.aaa.net #泛域名支持 会在 .acme.sh 目录生成一个以域名为目录的证书目录 1234[Tue Mar 21 15:51:00 CST 2017] Your cert is in /home/pprt/.acme.sh/&lt;domain&gt;/&lt;domain&gt;.cer[Tue Mar 21 15:51:00 CST 2017] Your cert key is in /home/pprt/.acme.sh/&lt;domain&gt;/&lt;domain&gt;.key[Tue Mar 21 15:51:01 CST 2017] The intermediate CA cert is in /home/pprt/.acme.sh/&lt;domain&gt;/ca.cer[Tue Mar 21 15:51:01 CST 2017] And the full chain certs is there: /home/pprt/.acme.sh/&lt;domain&gt;/fullchain.cer 部署acme.sh 提供了一个接口用来部署证书，也就是我们需要将生成的证书拷贝到指定的服务器,实例如下1acme.sh --deploy -d example.com --deploy-hook cpanel 服务器上使用了 ansible 作为管理 所以需要将文件使用 ansible 复制到其他机器上指定的文件夹 在 deploy 文件夹下面 创建一个 ansible.sh 的文件,里面添加如下代码，将文件拷贝到指定的服务器。其中 _cert_dir 为服务器上保存证书的目录 1234567891011121314151617181920212223ansible_deploy() { _cdomain=&quot;$1&quot; _ckey=&quot;$2&quot; _ccert=&quot;$3&quot; _cca=&quot;$4&quot; _cfullchain=&quot;$5&quot; _debug _cdomain &quot;$_cdomain&quot; _debug _ckey &quot;$_ckey&quot; _debug _ccert &quot;$_ccert&quot; _debug _cca &quot;$_cca&quot; _debug _cfullchain &quot;$_cfullchain&quot; _cert_dir=/Code/sslCert/ ansible olweb -m file -a &quot;path=$_cert_dir/$c_domain state=directory &quot; --become ansible olweb -m copy -a &quot;src=$_cfullchain dest=$_cert_dir/$_cdomain/cert.cer&quot; ansible olweb -m copy -a &quot;src=$_ckey dest=$_cert_dir/$_cdomain/cert.key&quot; ansible olweb -m command -a &quot;service openresty force-reload&quot; --become # _err &quot;deploy cert to nginx server, Not implemented yet&quot; return 0} 在执行deploy后，会制动添加一个 crontab133 0 * * * &quot;/home/pprt/.acme.sh&quot;/acme.sh --cron --home &quot;/home/pprt/.acme.sh&quot; &gt; /dev/null 如果设置有多个域名，只会部署第一个，需要在crontab添加一个-f的参数 参考acme.sh github acme.sh wiki acme.sh dnsapi acme.sh deploy","link":"/dev/使用acme-sh配置letsencrypt证书/"},{"title":"使用语雀写hexo","text":"首先要有一个hexo blog 安装 yuque-hexo1npm i -g yuque-hexo 注册语雀，创建知识库，获得你的个人路径和知识库的名字，比如 https://www.yuque.com// 在 Hexo 博客的目录下面的 package.json 中，进行下面的配置 12345678910{ \"name\": \"your hexo project\", //如果原来已经有这个不用再加了，直接加下面的就可以 \"yuqueConfig\": { \"baseUrl\": \"https://www.yuque.com/api/v2\", \"login\": \"&lt;login&gt;\", \"repo\": \"&lt;repo&gt;\", \"mdNameFormat\": \"slug\", \"postPath\": \"source/_posts/yuque\" }} 如果不是 Hexo 博客，则需要按照上面的文件保存一个 package.json 到博客目录，并且配置 postPath 为正确的文章目录 同步文章1yuque-hexo sync PS: 插件支持 Front-matter，在语雀写文章的时候直接写在前面，然后插入一条分割线即可，不写也没问题：123tags: [Hexo]categories: 教程date: 2018-10-05 10:43:50","link":"/dev/使用语雀写hexo/"},{"title":"安装Docker","text":"docker docs : https://docs.docker.com windows下载地址：https://store.docker.com/editions/community/docker-ce-desktop-windows 需要开启 Hyper-V 直接安装 mac下载地址 https://store.docker.com/editions/community/docker-ce-desktop-mac 直接安装 Docker.dmg . ubuntu如果是新安装的建议将/etc/apt/source.list内容替换为阿里源 12345678910deb https://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse 用脚本安装12$ curl -fsSL https://get.docker.com -o get-docker.sh$ sudo sh get-docker.sh 按步骤安装安装 Docker CE 需要 64 位的 Ubuntu 删除老版本的 Docker1$ sudo apt-get remove docker docker-engine docker.io Docker 支持 aufsubunut14.04 需要，16.04 不需要 12345678$ sudo apt-get update$ sudo apt-get install \\ linux-image-extra-$(uname -r) \\ linux-image-extra-virtual #问题 安装后机器起Docker会死机 升级内核到4.2后问题解决 sudo apt-get install linux-generic-lts-wily 安装update apt-get1$ sudo apt-get update 添加apt https 支持12345$ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common 添加 Docker GPG key1234567891011$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo apt-key fingerprint 0EBFCD88 #验证pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) &lt;docker@docker.com&gt;sub 4096R/F273FCD8 2017-02-22$ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 安装Docker-CE123$ sudo apt-get update$ sudo apt-get install docker-ce #写文档时最新版本docker version 18.06.1-ce# $ sudo apt-get install docker-ce=&lt;VERSION&gt; # 安装指定版本的docker","link":"/dev/安装Docker/"},{"title":"将数据库表导入到solr索引","text":"编辑solrcofnig.xml添加处理器 12345&lt;requestHandler name=&quot;/dataimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 创建一个名为data-config.xml的文件并保存如下内容到conf目录(也就是solrconfig.xml的目录) 12345678910111213&lt;dataConfig&gt; &lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost/dbname&quot; user=&quot;user-name&quot; password=&quot;password&quot;/&gt; &lt;document&gt; &lt;entity name=&quot;id&quot; query=&quot;select id,name,desc from mytable&quot;&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 编辑schema.xml文件，保证文件中有’id’,’name’,’desc’等fields。并更改data-config.xml的详细信息。 将JDBC的jar驱动文件放到/lib文件夹中（tomcat/webapps/solr/WEB-INF/lib） 在solr-4.10.1/dist目录拷贝solr-dataimporthandler开头的jar包到solr/WEB-INF/lib/目录. 1cp solr-dataimporthandler-* /usr/local/tomcat/webapps/solr/WEB-INF/lib/ 运行命令http://solr-host:port/solr/dataimport?command=full-import进行全量索引，每次进行全量索引时，会将数据清空，如果不想清空需要添加clean=false。例如http://solr-host:port/solr/dataimport?command=full-import&amp;clean=false ####在字段名和field明不同的时候添加索引 修改data-config.xml,如下所示 123456789101112131415&lt;dataConfig&gt; &lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost/dbname&quot; user=&quot;user-name&quot; password=&quot;password&quot;/&gt; &lt;document&gt; &lt;entity name=&quot;id&quot; query=&quot;select id,name,desc from mytable&quot;&gt; &lt;field column=&quot;id&quot; name=&quot;solr_id&quot;/&gt; &lt;field column=&quot;name&quot; name=&quot;solr_name&quot;/&gt; &lt;field column=&quot;desc&quot; name=&quot;solr_desc&quot;/&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 写入solr的字段为’solr_id’, ‘solr_name’, solr_desc’。所以schema.xml中必须要要这几个field。 运行 http://solr-host:port/dataimpor?command=full-import 建立索引 配置多个表建立索引 修改data-config如下： 12345678910111213141516171819&lt;dataConfig&gt; &lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost/dbname&quot; user=&quot;user-name&quot; password=&quot;password&quot;/&gt; &lt;document&gt; &lt;entity name=&quot;outer&quot; query=&quot;select id,name,desc from mytable&quot;&gt; &lt;field column=&quot;id&quot; name=&quot;solr_id&quot;/&gt; &lt;field column=&quot;name&quot; name=&quot;solr_name&quot;/&gt; &lt;field column=&quot;desc&quot; name=&quot;solr_desc&quot;/&gt; &lt;entity name=&quot;inner&quot; query=&quot;select details from another_table where id =&apos;${outer.id}&apos;&quot;&gt; &lt;field column=&quot;details&quot; name=&quot;solr_details&quot;/&gt; &lt;/entity&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; schema.xml应该包含solr_details的字段 运行full-import mysql配置 下载mysql的JDBC的jar，并拷贝到/lib的文件夹 修改data-config为如下 1234567891011&lt;dataConfig&gt;&lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://ip:3306/dbname&quot; user=&quot;username&quot; password=&quot;password&quot;/&gt; &lt;document name=&quot;products&quot;&gt; &lt;entity ...... &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt;","link":"/dev/将数据库表导入到solr索引/"},{"title":"理解 zabbix 配置","text":"zabbix 配置看上去很复杂。 实际上是由于界面布局不合理，功能都是以展示为主，平铺直列。在直观状态下实际上是误导。 作为一个运维监控的工具。把我们自己需要的功能对应到 zabbix 上，就会变的很好理解。 基本需求，就是需要监控一群机器，并给出相应的可视化的表或者提示。 实际上可以按照层级关系列出，而不是按照 zabbix 界面的平铺。 用户管理用户作为管理者或者使用者，很好理解，但是报警媒介放在了用户管理界面是很突兀的。实际上这个应该放到配置里面。 也许是作者觉得应该和用户绑定，但是这是一个功能，而不是一个设置。对用户来说是个干扰。 配置配置是最重要的功能。研究完配置基本就能够理解 zabbix 的逻辑，但是对于 zabbix 的 UI 却是没有逻辑的。 如果我们要监控一群机器。需要将机器都添加到系统里面，这就是 配置-主机 我们可以为主机创建 监控项 实际上到这里，我们需要的功能完了，添加主机，给主机添加监控项。 这就是我们所需要的功能。 对于监控项我们需要做什么呢 添加触发器比如我们监控CPU的使用率，当 CPU 使用率到 90% 时产生一个事件。 添加图型让CPU的状态能够可视化 如果监控项太多，需要分组。所以我们把监控项添加到应用集 ，当我们在做配置的时候不可能每一台机器都去一项一项的配置。于是把主机的概念虚拟出来，这里叫做模板 ，然后在创建主机时将模板绑定到主机就行了。 所以这分成了实际的物理机器，和虚拟的模板。创建一个主机群组 ，创建的主机可以添加到主机群主，创建的模板也可以添加到群组。实际是模板和机器的绑定。 所以逻辑就变成了： 创建主机群组 将机器添加到群组 为群组添加模板 +. 为模板创建应用集 +. 为应用集添加监控项 +. 为监控项添加触发器 +. 为触发器添加报警 +. 为监控项添加图行 以上就是 zabbix 的配置逻辑。而所有其他的功能大多都是为这一套逻辑来服务，或是对这一套逻辑的功能扩展。 涉及到具体的配置逻辑只需要参考 zabbix 的官方文档即可 。","link":"/dev/理解zabbix配置/"},{"title":"生成分布式id","text":"分布式系统中经常需要生成全局唯一 id ，要求： 全局唯一： 不能出现重复ID 高可用： 作为基础系统，被许多关键系统调用，一旦宕机，影响严重 在分布式系统中有很多的案例。 UUIDUUID 由算法生成，通常为36字节字符串表示。比如: 3A2504D0-4M89-11D3-9A0C-0305E82C3301 保证唯一性，根据不同的规范，定义不同位的值。可能包括MAC地址、时间戳、命名空间、随机数、时序等。 定义好规则后，可以直接由机器本地生成，相比其他来说不需要远程调用，使用方便。缺点是无法保证递增。 mysql 自增长这个最早是 Flicker 采用的，基本所有聊到分布式 ID，都会说到这里。 思路是采取了mysql 的自增长 ID 的机制 ，(auto_increment + replace into)。 replace into 首先会尝试插入，如果存在会删除旧数据，插入新数据。 获取 ID ： 1REPLACE INTO Tickets64(stub) values(&apos;a&apos;); select last_insert_id(); 避免单点故障，多个mysql 实例通过区分 auto_increment 的起始值和步长来生产 ID 。 优点：充分利用 mysql 的自增 id ，高可靠，有序 缺点：依赖数据库，性能可靠性取决于数据库。不好扩展。 snowflakesnowflake - 64bit Twitter的方案 41 位时间序列，精确到毫秒（可以使用69年） 10 位机器标示，（1024个节点） 12 位计数号 （每秒4096个ID） 优点： 整体递增，时间有序，高性能，可以根据业务灵活调整bit位划分 缺点： 依赖时钟，时间回拨可能导致重复 ID ，在分布式环境中非时间不一致导致非全局递增。 序列生成方式 使用数据库同步ID 每次取一定数量的可用 ID 在内存中，使用完后重新获取 每个业务可以定义自己的序列名，隔离业务 。 优点： 大大降低数据库压力，生产 ID 数量大大提高，不同业务不影响 缺点： 依赖数据库 阿里的 tddl，美团的Leaf 都是这种思路 。 对于有些对 id 连续有要求的，可以根据每秒生成数做限制。比如4096个只使用其中的一部分，每次取前对id进行随机的跳跃。可以得到不连续的id。 思考uuid 和 snowflake 的方案思路是一样的，只是一个是依赖机器，一个是依赖时间。 mysql 的方法是在没有解决方法的时候简单又行之有效的方法。 序列生成已经是很标准成熟的解决方案 依赖数据库，又不完全依赖数据库，数据库只是用来做持久化，用来做数据段的分配。 取出一段 id 后，不管业务怎么用，保证了 id 生成的效率 保证 id 是整体递增的","link":"/dev/生成分布式id-md/"},{"title":"用单例和动态类在sqlalchemy中做动态表绑定","text":"当我们的数据表需要分库的时候，如果使用了 sqlalchemy 来做 ORM，在做数据表和类关联的时候就很痛苦了，一个类只能和一个表关联。当然还有一种方法就是修改 model.tablename.name 来解决，但是这个有个问题是只能绑定一次，实例化后修改就无效了。所以我们就需要使用 type 来动态的创建一个类，到这里问题又来了，如果在不同的时候创建了两个相同的类名的时候，程序就会抛出异常。所以类名必须是单例的。整个的实现如下： 1234567891011121314151617181920212223Base = declarative_base()class m_model(object): _mapper = {} @staticmethod def model(tablename): class_name = tablename modelClass = m_model._mapper.get(class_name, None) if modelClass is None: modelClass = type(class_name, (Base,),{ &apos;__module__&apos;: __name__, &apos;__name__&apos;: class_name, &apos;__tablename__&apos;: class_name, &apos;id&apos;: Column(Integer, primary_key=True), &apos;phone&apos;: Column(String), # ...... }) m_model._mapper[class_name] = modelClass return modelClass()","link":"/dev/用单例和动态类在sqlalchemy中做动态表绑定/"},{"title":"简单的坐标转省市区的算法","text":"在很早以前，就通过地图 api 抓取的相关的省市区信息，其中有一个字段是 polylines ，里面是一组 gps 的坐标。为了离线来根据 gps 坐标获取省市区的信息，最开始想到的是使用 geohash 来计算出每个区域所包含的hash值，把所有的hash值存下来，这样查询变成了键值查询，如果 geohash精确到第8位，值偏差大概是19米，这个精度已经在我所能够承受的范围内了。 于是，写了点代码跑了一下东城区围栏的 geohash，然后把区域在一起的32个值去掉一位，向前递进。最后得到了 5000 多个值，也就是用 5000 多个 geohash 的值可以表示出东城区整个区域，整个程序执行了 5 分钟左右，用笔在纸上算了一下全国的区县，按照这个规模，hash 值数量大概在亿的基本，这个也是存储能够接受的。 于是拿海淀区的围栏试了试，计算时间成倍增长，这个计算速度，估计今年底能算完就不错了，然后想了想改进的算法，从 geohash 5位开始算起，理论上能快一个数量级，实际上呢，地图区域不是规则的，计算速度依然不能够满足需要。 需要重新考虑计算方法，看着这一堆的数据表，数据表中有个 center 的字段，突然发现计算一个点是否在围栏里面的算法是线性的，也就是算法负责度为O(n).于是马上改变了思路。 计算方法，首先找到给坐标（x,y)点最近的4个区域，然后计算点在哪个区域里。 那么整么才能找到给定坐标的最近的点呢，这里就要请出 mongodb ，利用 mongodb 的 2d 索引，可以非常方便的找到临近的点。 1collection.area.find({&apos;gps&apos;: {&apos;$near&apos;: {&apos;lat&apos;: lat, &apos;lng&apos;: lng}}, &apos;leaf_note&apos;: 1}).limit(4) 找到临近点后，只需要循环算一下点在哪个区域内，这个算法是在入坑前就找到的，原理就是从目录点随便引出一条射线，如果射线与围栏区域的交点是奇数个，那么这个点就在区域内，算法代码如下： 1234567891011121314151617181920def boundary(p, points): &apos;&apos;&apos; 计算方法：射线法 从内部发出一条射线，判断与边的交点个数 计算点是否在区域内 算法来源 http://stackoverflow.com/questions/8721406/how-to-determine-if-a-point-is-inside-a-2d-convex-polygon &apos;&apos;&apos; result = False i = 0 j = len(points) - 1 while i &lt; len(points): if (points[i].y &gt; p.y) != (points[j].y &gt; p.y) and (p.x &lt; (points[j].x - points[i].x) * (p.y - points[i].y) / (points[j].y - points[i].y) + points[i].x): result = not result j = i i = i + 1 return result 然后这个没几行代码、简单、快速、不占用什么存储、方便移植到其他语言的计算方法就算完成了。当然中间还有些细节的考虑","link":"/dev/简单的坐标转省市区的算法/"},{"title":"在github上配置hexo写blog","text":"####安装nodejs http://nodejs.org/ 安装hexo1npm install -g hexo ###创建bolg文件夹 安装完成后在自己的工作目录创建一个文件夹 12345678910111213141516171819202122D:\\work&gt;mkdir blogD:\\work&gt;cd blogD:\\work\\blog&gt;hexo init #初始化[info] Copying data[info] You are almost done! Don&apos;t forget to run `npm install` before you start blogging with Hexo!D:\\work\\blog&gt;npm install #安装依赖hexo-renderer-marked@0.1.0 node_modules\\hexo-renderer-marked├── marked@0.3.2└── lodash@2.4.1hexo-renderer-ejs@0.1.0 node_modules\\hexo-renderer-ejs├── ejs@1.0.0└── lodash@2.4.1hexo-renderer-stylus@0.1.0 node_modules\\hexo-renderer-stylus├── stylus@0.44.0 (css-parse@1.7.0, mkdirp@0.3.5, sax@0.5.8, debug@2.1.0, glob@3.2.11)└── nib@1.0.4 (stylus@0.45.1) 执行完上面的命令，就已经搭了一个本地的hexo环境，在当前目录输入命令12hexo generatehexo server 访问 http://localhost:4000/ ，这时候本地的bolg就搭建好了。 _config.yml设置 参考http://zipperary.com/2013/05/29/hexo-guide-3/ 设置主题在 https://github.com/hexojs/hexo/wiki/Themes 可以找到喜欢的主题使用git命令下载比如pacman这个主题 $ git clone https://github.com/A-limon/pacman.git themes/pacman 上传到github1hexo d -g 绑定域名 需要在git的master分支下创建一个名称为CNAME的文件。 ping name.github.io 的到一个IP地址。 在dns里面将A记录解析到得到的IP地址。 等待解析。","link":"/tool/hexo-and-github-blog/"},{"title":"keepass","text":"keepass 作为一款开源的密码管理工具，在有 chrome 扩展后，个人认为已经可以完全替代 lasspass。 现在市面上的密码管理工具已经很多了，有在线的lastpass，有1password高颜值等等。但是它们都是收费的。 而今天要安利给大家的是全平台的，免费的，开源的密码管理工具keepass。 KeePassPassword Safe（简称KeePass），是一款免费开源的密码管理软件，通过它，你只要记住一个主密码就可以管理你所有的网络帐号和密码了，并且KeePass会生成一个数据库文件，保存好这个数据库文件就等于保存好了你所有的密码资料。有了KeePass，你会发现我们的网络世界方便了不只一点点。 KeePass支持Windows、Linux、Mac OS、BlackBerry、iPhone、Andriod……几乎所有平台，几乎所有语言，并且有丰富的插件可以与Chrome、黑莓桌面管理器等配合使用。你用了才会发现他居然是如此的强大！ 北嗅这里主要介绍windows、mac 、android 、ios 四个平台的客户端。 windows首先windows就不用说了，官方客户端就够用了。直接去官网http://keepass.info/download.html下载就好了， macmac 平台也有一些客户端，比如直接在AppStore上搜索可用的 Keepass Desktop,但是呢它缺少了一个功能，就是浏览器自动输入。所以这里给大家推荐的是一个叫做 MacPass 的客户端,下载地址 https://github.com/mstarke/MacPass/releases 这款客户端同样也是开源的，代码托管在了 GitHub 上。 androidandroid 平台没什么好说的，直接到各大应用市场搜索 KeePass2Android 下载，支持自动填充，键盘填充。 iosios 平台推荐 ‘KeePass Touch’ 这个客户端，相对来说功能比较完整，也支持直接dropbox同步。minikeepass支持dropbox导入导出，同步似乎有点问题。 使用keepass要解决两个问题 密码库同步 浏览器支持 密码库同步由于我的平时使用的环境为 windows + mac + android 。所以对这三个平台比较熟悉。由于 KeePass2Android 是支持 dropbox、onedrive、googledrive 等等常用的网盘支持，由于国内网络问题，onedrive 是目前唯一没有屏蔽的网盘，所以我在 windows 和 mac 上使用 onedrive 来同步密码库。同样，如果你是在 ios 上可以检查客户端是否支持 iCloud，来选择使用 iCloud 同步密码。 浏览器支持浏览器支持主要是在 windows 和 mac 两个平台上，北嗅一般使用Chrome+KeePass+ChromeIPass+KeePassHttp使用方法就不在这里描述了，windows 平台网络上有一大把的教材，MacPass也支持ChromeIPass+KeePassHttp，可以下载 https://github.com/MacPass/MacPassHTTP/releases 放到MacPass目录。 其实呢KeePass 和 MacPass 本身都自带了自动输入功能，并且与浏览器无关。所以ChromeIPass就显得有点多余了。 这里呢没有教大家具体的使用方法，也没有具体的教程，只是推荐的几个客户端和相关使用时要注意的东西。当然我相信大家的动手能力，不会去搜索嘛。","link":"/tool/keepass/"},{"title":"sublime3配置","text":"安装插件管理器https://sublime.wbond.net/installation 菜单view &gt; Show Console 调出命令行工具粘贴命令，之后重启 1import urllib.request,os,hashlib; h = &apos;2915d1851351e5ee549c20394736b442&apos; + &apos;8bc59f460fa1548d1514676163dafc88&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) 执行不成功,可能需要代理 （我用的 ss +Privoxy）菜单PreferencesSettings &gt;&gt; Settings –User123{ &quot;http_proxy&quot;: &quot;http://127.0.0.1:8118&quot;} 安装插件shift+ctrl+p输入pci （Package Control: Install Package） Emmet 提高HTML &amp; CSS3编写速度。 DocBlockr 快速注释 ConvertToUTF8 UTF8转换 Markdown Preview Markdown 文件预览 SideBarEnhancements 扩展侧边栏，delete 是到回收站 Anaconda 终极 Python 插件。它为 ST3 增添了多项 IDE 类似的功能，例如： Autocompletion 自动完成，该选项默认开启，同时提供多种配置选项。 Code linting 使用支持 pep8 标准的 PyLint 或者 PyFlakes McCabe code complexity checker 让你可以在特定的文件中使用 McCabe complexity checker. 如果你对软件复杂度检查工具不太熟悉的话，请务必先浏览上边的链接。 Goto Definitions 能够在你的整个工程中查找并且显示任意一个变量，函数，或者类的定义。 Find Usage 能够快速的查找某个变量，函数或者类在某个特定文件中的什么地方被使用了。 Show Documentation： 能够显示一个函数或者类的说明性字符串(当然，是在定义了字符串的情况下) Sublime &gt; Preferences &gt; Package Settings &gt; Anaconda &gt; Settings – User： 123{ &quot;auto_formatting&quot;: true,} 配置Preferences-&gt;Settings-User 增加： // 显示空格 &quot;draw_white_space&quot;: &quot;all&quot;, // tab 为4个空格 &quot;tab_size&quot;: 4, // tab 转换为空格 &quot;translate_tabs_to_spaces&quot;: true // 保存时自动去除行末空白 &quot;trim_trailing_white_space_on_save&quot;: true, // 保存时自动增加文件末尾换行 &quot;ensure_newline_at_eof_on_save&quot;: true, // 默认编码格式 &quot;default_encoding&quot;: &quot;UTF-8&quot;","link":"/tool/sublime3/"},{"title":"Mac 上的 SQLServer 客户端 Oracle SQL Development","text":"最近需要写一个小的业务，业务部门提供的数据库是 SQLServer 。需要找一个在 Mac 上连接 SQLServer 的客户端。 Oracle 开发了一个很好用的免费 SQL Developer 工具。需要稍微的配置一下。 下载 sql developer下载SQL开发。 甲骨文公司有一个很好的免费的SQL开发工具, SQL Developer 这将允许您连接到该数据库服务器。 1http://www.oracle.com/technetwork/developer-tools/sql-developer/overview/index-097090.html 下载驱动下载该软件的 JDBC 驱动 1http://sourceforge.net/projects/jtds/files/ 将下载的文件解压后，得到一个 jtds-x.x.x.jar 的文件，将这个文件放到一个目录，比如 /Library/Java/Extensions 设置—F运行 Oracle SQL Developer ，主菜单 -&gt; Preferences -&gt; Database -&gt; Third Party JDBC Drivers-&gt;Add Entry 将 jtds-x.x.x.jar 添加进来。 完成这个步骤后就可以创建到 SQLServer 和 Sybase 的新链接了。你会在以前 Oracle 的选项卡中看到 SQLServer 和 SyBase 的选项卡。","link":"/tool/mac-ms-sqlserver-client/"},{"title":"tmux","text":"会话创建新会话tmux new[-session] -s &lt;会话名称&gt; [初始命令] 结束会话Ctrl+b 或 输入 exit 连接会话tmux attach-session -t 目标会话名简写tmux attach -t 会话名tmux a -t 会话名 或者直接tmux a 列出会话tmux ls 操作类似各种平铺式窗口管理器，tmux使用键盘操作，常用快捷键包括： Ctrl+b 激活控制台；此时以下按键生效 系统操作123456789? 列出所有快捷键；按q返回d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话D 选择要脱离的会话；在同时开启了多个会话时使用Ctrl+z 挂起当前会话r 强制重绘未脱离的会话s 选择并切换会话；在同时开启了多个会话时使用: 进入命令行模式；此时可以输入支持的命令，例如kill-server可以关闭服务器[ 进入复制模式；此时的操作与vi/emacs相同，按q/Esc退出~ 列出提示信息缓存；其中包含了之前tmux返回的各种提示信息 窗口操作123456789101112131415161718192021222324c 创建新窗口&amp; 关闭当前窗口数字键 切换至指定窗口p 切换至上一窗口n 切换至下一窗口l 在前后两个窗口间互相切换w 通过窗口列表切换窗口, 重命名当前窗口；这样便于识别. 修改当前窗口编号；相当于窗口重新排序f 在所有窗口中查找指定文本面板操作 “ 将当前面板平分为上下两块% 将当前面板平分为左右两块x 关闭当前面板! 将当前面板置于新窗口；即新建一个窗口，其中仅包含当前面板Ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小Alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小Space 在预置的面板布局中循环切换；依次包括even-horizontal、even-vertical、main-horizontal、main-vertical、tiledq 显示面板编号o 在当前窗口中选择下一面板方向键 移动光标以选择面板{ 向前置换当前面板} 向后置换当前面板Alt+o 逆时针旋转当前窗口的面板Ctrl+o 顺时针旋转当前窗口的面板","link":"/tool/tmux/"},{"title":"5G会带来什么","text":"5G是什么5G 是第五代移动通信技术是最新一代蜂窝移动通信技术，是4G（LTE-A、 WiMAX-A）、3G（UMTS）和2G（GSM）系统后的延伸。5G的性能目标是高数据速率、减少延迟、节省能源、降低成本、提高系统容量和大规模设备连接。 1G第一代的移动通信技术，为模拟式移动电话系统。 和 2G 的区别是1G使用模拟调制，而2G则是数字调制。在模拟时代，会有很多电话盗打，原理就是模拟电话信号。当变成数字信号后，模拟变成了不可能。 2G2G 是我们通常所知道的 移动的 GSM 或者 联通的 CDMA .2G 移动网络对通信系统以数字化传输，并加入了短信的功能。 数字化传输让基站和手机变得廉价，让人人都用得起手机 ，而短信的价值就不用说了。使用诺基亚手机来盲发短信，每隔一段时间都要纠结短信容量满了，和现在手机内容小了要删照片的感觉是一样一样的。 3G3G 之前还有2.5G 2.75G的技术规格。3G 开始支持高速数据传输，使用环境的不同约有300k-2Mbps左右的水准。3G 是手机开始成为一种生活方式的开始，我们不在是用手机，而是刷手机，它改变了手机的功能，让手机变成了个人生活平台，手机的电话和短信功能变成了次要功能，微博，微信开启了他们的时代。 4G4G 实际上是 3G 的延伸，从技术标准的角度看，，静态传输速率达到1Gbps，用户在高速移动状态下可以达到100Mbps，就可以作为4G的技术之一。4G要有更高的数据吞吐量、更低时延。随着 4G 出现是直播，短视频，斗鱼、抖音、快手……。 在我们拿起手机的时候，1G 我们在打大哥大，2G 发短信，3G 刷微博 ,4G 刷抖音。但是在这个得背后，每一次都是一次大的产业升级。 产业是一片大海，而我们所看到，所使用的只是大海上的一叶扁舟。 5G 的来到会给我们带来什么变化呢。 更高的速度，最高可达10 Gbit/s (1250 MB/s)，是 4G 的100倍 11. 机械硬盘基本上都低于300MB/s 固态硬盘在 500MB/s 到几个GB 不等。 内存的读写可以轻松上20GB/s 也就是说 5G 的速度已经超过了机械硬盘，下载速度超过了sata接口的ssd，甚至是nvme的固态盘。 既然速度差不多，那么存储云端化是不是一个必然发生的事情呢。假设未来运营商的骨干网的带宽价格无限的降低，那么数据存储在云端的成本可能会比存储在本地更加的便宜。 如果我们可以随时的连接到网络，那么挂载一个云端硬盘和本地硬盘有什么区别呢。区别可能是云端硬盘更加便宜和数据更加的安全。 那么我们的办公环境会发生什么样的变化呢。我们的笔记本、手机、相机等待各种需要存储的设备会发生什么变化呢。 网络的数据存取和本地一致后，操作系统云化将变为现实，在网络能够保证的情况下，使用云化的系统，和使用本地系统体验不会有任何的差别，那么随之而来的是应用程序的开发也会云化，所有的存储数据在云端，所有的应用数据在内存，本地硬盘变成了缓存空间。网络变成了和水和电一样的基础设施，云也变成了和水和电一样的基础设施。 更高的速度，代表我们可以看 4K 、8K 甚至更加高清的视频，除了三大运营商在，广电也得到了 5G 网络的牌照，很多人都在疑惑广电凑什么热闹。5G 网络能够低延时高可靠高容量的传输，那利用 5G 来传输电视信号会成为新的选择，那么电视直接接到 5G 网络，那么电视作为客厅的一块屏，会以怎样的一种形态来出现呢。可能电视就一块壁纸，贴在了墙上，你的房顶有连接着 5G 网络的设备，设备中有各种的传感器。可以给你放高清电视，可以提供体感娱乐，可以提供 3D 的远程视频。电视会变成真正的家庭娱乐终端。而不只是看视频。 电视台的节目分发不在通过信号塔，而是 5G 网络来分发。那么电视台会带来什么改变呢，一个 UP 主？ 在游戏上，打个比方，现在的王者荣耀的安装包已经超过5个G的容量，如果成为一个游戏，游戏里只带有游戏的逻辑，而所有的游戏资源将在云端，可能你根本不需要下载游戏，在微信里打开游戏的连接，只需要加载几秒的时间，就可以直接打开游戏。 更低的延时，低延时对于我们的最直观的感受就是看视频不卡，或者是玩王者荣耀不卡。但是这里更低的延时，4G 的延时是15ms，对于绝大多数情况，这个延时已经很够用了，但是对于一些对延时要求更低的，比如当前很火的汽车自动驾驶，其中有一项很关键的点就是延时，当汽车出现问题的时候，要更快的反应，可能几毫秒就是生与死的差别。而 5G 的出现解决了这个痛点。更低的网络延时，代表了更精确的控制，相信这会在很多的行业的自动化领域发挥出巨大的威力。 更大的连接数，更大的连接数表示了可以让更多的设备连接到网络中，这就是我们所说的万物互联。现在的很多物联网设备还是依赖于网关，设备接入网关，网关接入互联网，所有的指令都是通过网关来下发，那么在 5G 时代， 配合 IPv6, 每一个设备都有自己的地址，我们可以随时随地的远程访问我们的物联网设备。而且我们的手机与物联网设备是直联的。那么需要如何去控制管理物联网设备，从当前的物联网来看，包括苹果、谷歌、三大运营商、小米、阿里、腾讯等等都有自己的一套智能设备的管理协议。在所有的设备都能够接入到5G网络后，我们应该定义一套通用的物联网设备在应用层的协议，让所有的设备都可以相互兼容的接入到一个网络，连接、授权、认证、管理等。 那么 5G 如果覆盖到后，所有的设备都可以直接连接到 5G 网络，可以预料的是 5G 的网络资费和现在的光纤到家资费差不多，或许更低。 那么我们还需要路由器这种设备吗？也许到那个时候我们对路由器的需求和我们现在对交换机的需求差不多，只是某些特殊的场景才会想起它吧。 5G 的运用场景，更多的是对产业的改变，如下图是华为的一篇科普文章中截图出来的。 里面的 4G 里包括的语音、社交、网页、视频。而 5G 里包括了 VR、车联网、视频监控、智能电网、智能工程、智能物流……。 这里 4G 的应用和我们普通人的生活息息相关，而 5G 除了 VR 带有部分娱乐功能外，其他的几乎全都是产业互联网。所以 5G 会给人类社会带来更大的变化，这个变化会渗透的每一个行业的背后。 而这波浪潮的会是哪些公司站在浪潮之巅呢，相信每个人都会有自己的答案。 2019 年是 5G 的元年，5G 的到来会比 4G 来的更快，改变也会更快更彻底。到底会怎么变化，没有人能猜到，但是它变化的方向，隐约是知道的。 这是一个全球一体化的社会，你中有我，我中有你，中国人有自己特有的韧性，对我们的攻击只会让我们更加的强大。 最后 5G 新空口声明标准专利统计 5G 新核心网声明标准专利统计 5G的标准立项共50项中，主要分为三大阵营，中国、欧洲、美国，其中中国立项21项、欧洲14项、美国9项、日本4项、韩国2项。","link":"/note/5G会带来什么/"}],"tags":[{"name":"Google","slug":"Google","link":"/tags/Google/"},{"name":"socket","slug":"socket","link":"/tags/socket/"},{"name":"android","slug":"android","link":"/tags/android/"},{"name":"摇号","slug":"摇号","link":"/tags/摇号/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Portainer","slug":"Portainer","link":"/tags/Portainer/"},{"name":"harbor","slug":"harbor","link":"/tags/harbor/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"ansible","slug":"ansible","link":"/tags/ansible/"},{"name":"solr","slug":"solr","link":"/tags/solr/"},{"name":"kong","slug":"kong","link":"/tags/kong/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"php","slug":"php","link":"/tags/php/"},{"name":"smarty","slug":"smarty","link":"/tags/smarty/"},{"name":"fastdfs","slug":"fastdfs","link":"/tags/fastdfs/"},{"name":"frp","slug":"frp","link":"/tags/frp/"},{"name":"coap","slug":"coap","link":"/tags/coap/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"gitolite","slug":"gitolite","link":"/tags/gitolite/"},{"name":"brew","slug":"brew","link":"/tags/brew/"},{"name":"hugo","slug":"hugo","link":"/tags/hugo/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"ikvm","slug":"ikvm","link":"/tags/ikvm/"},{"name":"devops","slug":"devops","link":"/tags/devops/"},{"name":"keettle","slug":"keettle","link":"/tags/keettle/"},{"name":"kettle","slug":"kettle","link":"/tags/kettle/"},{"name":"letsencrypt","slug":"letsencrypt","link":"/tags/letsencrypt/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":".net","slug":"net","link":"/tags/net/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"minikube","slug":"minikube","link":"/tags/minikube/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"memcacheq","slug":"memcacheq","link":"/tags/memcacheq/"},{"name":"gzip","slug":"gzip","link":"/tags/gzip/"},{"name":"cache","slug":"cache","link":"/tags/cache/"},{"name":"nsq","slug":"nsq","link":"/tags/nsq/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"magick","slug":"magick","link":"/tags/magick/"},{"name":"pdf","slug":"pdf","link":"/tags/pdf/"},{"name":"openwrt","slug":"openwrt","link":"/tags/openwrt/"},{"name":"petl","slug":"petl","link":"/tags/petl/"},{"name":"etl","slug":"etl","link":"/tags/etl/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"RocketMQ","slug":"RocketMQ","link":"/tags/RocketMQ/"},{"name":"saltstack","slug":"saltstack","link":"/tags/saltstack/"},{"name":"词库","slug":"词库","link":"/tags/词库/"},{"name":"tesseract-ocr","slug":"tesseract-ocr","link":"/tags/tesseract-ocr/"},{"name":"thrift","slug":"thrift","link":"/tags/thrift/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"vagrant","slug":"vagrant","link":"/tags/vagrant/"},{"name":"zabbix","slug":"zabbix","link":"/tags/zabbix/"},{"name":"分布式","slug":"分布式","link":"/tags/分布式/"},{"name":"snowflake","slug":"snowflake","link":"/tags/snowflake/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"keepass","slug":"keepass","link":"/tags/keepass/"},{"name":"sublime","slug":"sublime","link":"/tags/sublime/"},{"name":"sqlserver","slug":"sqlserver","link":"/tags/sqlserver/"},{"name":"tmux","slug":"tmux","link":"/tags/tmux/"},{"name":"5g","slug":"5g","link":"/tags/5g/"}],"categories":[{"name":"note","slug":"note","link":"/./note/"},{"name":"dev","slug":"dev","link":"/./dev/"},{"name":"tool","slug":"tool","link":"/./tool/"}]}